{
    "a-ldav": {
        "event": "LDAV",
        "long_name": "IEEE Symposium on Large Data Analysis and Visualization",
        "event_type": "Symposium",
        "event_description": "Modern large-scale scientific simulations, sensor networks, and experiments are generating enormous datasets, with some projects approaching the multiple exabyte range in the near term. Managing and analyzing large datasets in order to transform it into insight is critical for a variety of disciplines including climate science, nuclear physics, security, materials design, transportation, and urban planning. The tools and approaches needed to mine, analyze, and visualize data at extreme scales can be fully realized only if there are end-to-end solutions, which demands collective, interdisciplinary efforts.\n\nThe 10th IEEE Large Scale Data Analysis and Visualization (LDAV) symposium, to be held in conjunction with IEEE VIS 2020, is specifically targeting methodological innovation, algorithmic foundations, and possible end-to-end solutions. The LDAV symposium will bring together domain scientists, data analysts, visualization researchers, and users to foster common ground for solving both near- and long-term problems. Paper submissions are solicited for a long and short paper tracks. Topic emphasis is on algorithms, languages, systems, and/or hardware solutions that support the collection, analysis, manipulation, or visualization of large-scale data.",
        "event_url": "https://ldav.org/",
        "sessions": [
            {
                "title": "Opening, Keynote and Best Paper",
                "session_id": "a-ldav-1",
                "chair": [
                    "Hongfeng Yu"
                ],
                "organizers": [
                    "Julien Tierny",
                    "Hongfeng Yu"
                ],
                "display_start": "2020-10-25T14:00:00Z",
                "time_start": "2020-10-25T14:00:00Z",
                "time_end": "2020-10-25T15:30:00Z",
                "discord_category": "ldav",
                "discord_channel": "opening-keynote-and-best-paper",
                "discord_channel_id": "767932161583087626",
                "youtube_url": "https://youtu.be/egXjNGyYHlQ",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Opening Remarks",
                        "contributors": [
                            "Hongfeng Yu"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:00:00Z",
                        "time_end": "2020-10-25T14:10:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Keynote: Large-Scale Visual Analytics: A Reflection on Methodology and Practice",
                        "contributors": [
                            "Huamin Qu"
                        ],
                        "abstract": "In the past decade, the VisLab at the Hong Kong University\u00a0of Science and Technology has closely worked with industry to develop\u00a0large-scale visual analytics\u00a0systems for different application domains,\u00a0especially E-Learning, Urban Computing, Smart Manufacturing, and FinTech. These\u00a0systems have been deployed to\u00a0more than 10 companies and about 20 papers\u00a0related to these systems have been published in IEEE VAST conferences. During\u00a0this process, we have\u00a0accumulated experience and learned lessons. In this talk, I will introduce some representative works and share my\u00a0reflection on the methodology and practice\u00a0of developing large-scale visual\u00a0analytics systems. I will especially talk about the limitations of the current\u00a0approaches including the ad-hoc nature of the\u00a0methodology and the overly\u00a0complicated visualization designs and systems, and provide my thoughts on how\u00a0these can be improved with the help of machine\u00a0learning and also technology\u00a0from software engineering.\u00a0",
                        "time_start": "2020-10-25T14:10:00Z",
                        "time_end": "2020-10-25T15:00:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Best Paper: Foveated Encoding for Large High-Resolution Displays",
                        "contributors": [
                            "Florian Frie\u00df"
                        ],
                        "abstract": "Collaborative exploration of scientific data sets across large high-resolution displays requires both high visual detail as well as low-latency transfer of image data (oftentimes inducing the need to trade one for the other). In this work, we present a system that dynamically adapts the encoding quality in such systems in a way that reduces the required bandwidth without impacting the details perceived by one or more observers. Humans perceive sharp, colourful details, in the small foveal region around the centre of the field of view, while information in the periphery is perceived blurred and colourless. We account for this by tracking the gaze of observers, and respectively adapting the quality parameter of each macroblock used by the H.264 encoder, considering the so-called visual acuity fall-off. This allows to substantially reduce the required bandwidth with barely noticeable changes in visual quality, which is crucial for collaborative analysis across display walls at different locations. We demonstrate the reduced overall required bandwidth and the high quality inside the foveated regions using particle rendering and parallel coordinates.",
                        "time_start": "2020-10-25T15:00:00Z",
                        "time_end": "2020-10-25T15:30:00Z",
                        "uid": "a-ldav-1026"
                    }
                ]
            },
            {
                "title": "In Situ",
                "session_id": "a-ldav-2",
                "chair": [
                    "Ming Jiang"
                ],
                "organizers": [
                    "Julien Tierny",
                    "Hongfeng Yu"
                ],
                "display_start": "2020-10-25T16:00:00Z",
                "time_start": "2020-10-25T16:00:00Z",
                "time_end": "2020-10-25T17:30:00Z",
                "discord_category": "ldav",
                "discord_channel": "in-situ",
                "discord_channel_id": "767932222744297482",
                "youtube_url": "https://youtu.be/skM-gluIXzU",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "DIVA: A Declarative and Reactive Language for In Situ Visualization",
                        "contributors": [
                            "Qi Wu"
                        ],
                        "abstract": "The use of adaptive workflow management for in situ visualization and analysis has been a growing trend in large-scale scientific simulations. However, coordinating adaptive workflows with traditional procedural programming languages can be difficult because systemflow is determined by unpredictable scientific phenomena, which often appear in an unknown order and can evade event handling. This makes the implementation of adaptive workflows tedious and error-prone. Recently, reactive and declarative programming paradigms have been recognized as well-suited solutions to similar problems in other domains. However, there is a dearth of research on adapting these approaches to in situ visualization and analysis. With this paper, we present a language design and runtime system for developing adaptive systems through a declarative and reactive programming paradigm. We illustrate how an adaptive workflow programming system is implemented using our approach and demonstrate it with a use case from a combustion simulation.",
                        "time_start": "2020-10-25T16:00:00Z",
                        "time_end": "2020-10-25T16:30:00Z",
                        "uid": "a-ldav-1016"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Data Parallel Hypersweeps for In Situ Topological Analysis",
                        "contributors": [
                            "Petar Hristov"
                        ],
                        "abstract": "The contour tree is a tool for understanding the topological structure of a scalar field. Recent work has built efficient contour tree algorithms for shared memory parallel computation, driven by the need to analyze large data sets in situ while the simulation is running. Unfortunately, methods for using the contour tree for practical data analysis are still primarily serial, including single isocontour extraction, branch decomposition and simplification. We report data parallel methods for these tasks using a data structure called the hyperstructure and a general purpose approach called a hypersweep. We implement and integrate these methods with a Cinema database that stores features as depth images and with a web server that reconstructs the features for direct visualization.",
                        "time_start": "2020-10-25T16:30:00Z",
                        "time_end": "2020-10-25T17:00:00Z",
                        "uid": "a-ldav-1022"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Comparing Time-to-Solution for In Situ Visualization Paradigms at Scale",
                        "contributors": [
                            "James Kress"
                        ],
                        "abstract": "This short paper compares the time-to-solution for in-line and in-transit in situ visualization, analyzing when one paradigm is faster than another. To perform this comparison, we create a corpus of data by running a scaling study using two common visualization algorithms (isosurfacing and volume rendering), with in-line and in- transit. Our experiments vary an array of parameters, including the size of the in-transit in situ allocation, the simulation cycle time, and the scale of the simulation (up to 32,768 cores and 64 billion total cells). We then analyze this corpus of data and draw conclusions about when each paradigm is the fastest. Our findings show that in-transit is faster than in-line when inter-process communication was high (up to 35% more efficient) and when our computation-bound algorithm was run at large scale (up to 47% more efficient). On the other hand, in-line was faster with our computation-bound algorithm and a short simulation cycle time (up to 42% more efficient). Finally, this work informs future directions for understanding other classes of in situ visualization algorithms.",
                        "time_start": "2020-10-25T17:00:00Z",
                        "time_end": "2020-10-25T17:30:00Z",
                        "uid": "a-ldav-1013"
                    }
                ]
            },
            {
                "title": "Rendering and Displays",
                "session_id": "a-ldav-3",
                "chair": [
                    "Markus Hadwiger"
                ],
                "organizers": [
                    "Julien Tierny",
                    "Hongfeng Yu"
                ],
                "display_start": "2020-10-25T18:00:00Z",
                "time_start": "2020-10-25T18:00:00Z",
                "time_end": "2020-10-25T19:30:00Z",
                "discord_category": "ldav",
                "discord_channel": "rendering-and-displays",
                "discord_channel_id": "767932244982759445",
                "youtube_url": "https://youtu.be/piBNlAZKHAM",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Interactive Visualization of Terascale Data in the Browser: Fact or Fiction?",
                        "contributors": [
                            "Will Usher"
                        ],
                        "abstract": "Information visualization applications have become ubiquitous, in no small part thanks to the ease of wide distribution and deployment to users enabled by the web browser. Scientific visualization applications, relying on native code libraries and parallel processing, have been less suited to such widespread distribution, as browsers do not provide the required libraries or compute capabilities. In this paper, we revisit this gap in visualization technologies and explore how new web technologies, WebAssembly and WebGPU, can be used to deploy powerful visualization solutions for large-scale scientific data in the browser. In particular, we evaluate the programming effort required to bring scientific visualization applications to the browser through these technologies and assess their competitiveness against classic native solutions. As a main example, we present a new GPU-driven isosurface extraction method for block-compressed data sets, that is suitable for interactive isosurface computation on large volumes in resource-constrained environments, such as the browser. We conclude that web browsers are on the verge of becoming a competitive platform for even the most demanding scientific visualization tasks, such as interactive visualization of isosurfaces from a 1TB DNS simulation. We call on researchers and developers to consider investing in a community software stack to ease use of these upcoming browser features to bring accessible scientific visualization to the browser.",
                        "time_start": "2020-10-25T18:00:00Z",
                        "time_end": "2020-10-25T18:30:00Z",
                        "uid": "a-ldav-1012"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Cinema Darkroom: A Deferred Rendering Framework for Large-Scale Datasets",
                        "contributors": [
                            "Jonas Lukasczyk"
                        ],
                        "abstract": "This paper presents a framework that fully leverages the advantages of a deferred rendering approach for the interactive visualization of large-scale datasets. Geometry buffers (G-Buffers) are generated and stored in situ, and shading is performed post hoc in an interactive image-based rendering front end. This decoupled framework has two major advantages.First, the G-Buffers only need to be computed and stored once---which corresponds to the most expensive part of the rendering pipeline.Second, the stored G-Buffers can later be consumed in an image-based rendering front end that enables users to interactively adjust various visualization parameters---such as the applied color map or the strength of ambient occlusion---where suitable choices are often not known a priori.This paper demonstrates the use of Cinema Darkroom on several real-world datasets, highlighting CD's ability to effectively decouple the complexity and size of the dataset from its visualization.",
                        "time_start": "2020-10-25T18:30:00Z",
                        "time_end": "2020-10-25T19:00:00Z",
                        "uid": "a-ldav-1024"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Spatial Partitioning Strategies for Memory-Efficient Ray Tracing of Particles",
                        "contributors": [
                            "Patrick Gralka"
                        ],
                        "abstract": "3D particle data is relevant for a wide range of scientific domains, from molecular dynamics to astrophysics. Simulations in these domains can produce datasets containing millions or billions of particles and rendering needs to be in high quality and interactive to support the scientists in exploring and understanding the structure of their data. One general baseline approach is to represent particles as spheres and employ ray tracing as a rendering technique. However, ray tracing requires the data to be organized in acceleration data structures like bounding volume hierarchies (BVH) to achieve interactive frame rates. Modern GPUs provide hardware acceleration for traversing such data structures but are more limited in memory than CPUs. In this paper, we evaluate different acceleration data structures for sphere-based datasets, including particle kD trees, with respect to their scalability regarding both memory size and speed, and we analyze how these data structures can benefit from hardware acceleration. We show that a bricking of data results in the most effective BVH, both fast to traverse utilizing hardware acceleration and with a reasonably small memory footprint. Additionally, we present a hybrid acceleration data structure that has negligible memory overhead and still ensures reasonable traversal speed. Based on our results, visualization tools and APIs for the ray tracing can provide overall better performance by adapting to the needs of particle-centric application scenarios.",
                        "time_start": "2020-10-25T19:00:00Z",
                        "time_end": "2020-10-25T19:30:00Z",
                        "uid": "a-ldav-1019"
                    }
                ]
            },
            {
                "title": "Panel and Closing",
                "session_id": "a-ldav-4",
                "chair": [
                    "Julien Tierny"
                ],
                "organizers": [
                    "Julien Tierny",
                    "Hongfeng Yu"
                ],
                "display_start": "2020-10-25T20:00:00Z",
                "time_start": "2020-10-25T20:00:00Z",
                "time_end": "2020-10-25T21:30:00Z",
                "discord_category": "ldav",
                "discord_channel": "panel-and-closing",
                "discord_channel_id": "767932296694202419",
                "youtube_url": "https://youtu.be/3tqyvXIRnyY",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Panel",
                        "title": "Panel: Technical Advances in the Era of Big Data Analysis and Visualization",
                        "contributors": [
                            "Jim Ahrens",
                            "Janine Bennett",
                            "Andreas Gerndt",
                            "Ken Moreland",
                            "Manish Parashar"
                        ],
                        "abstract": "The year 2020 marks the tenth edition of the IEEE Symposium on Large Data Analysis and Visualization. At the occasion of this milestone, we would like to take a step back to contemplate the main accomplishments of our community over the last decade and present the main ongoing efforts in the land of large data analysis and visualization. For this, we gathered a panel of leading experts in the field, who will present their feedback on progresses in the last ten years and ongoing efforts on the topic. Future directions to take as a community, and how to organize research efforts to address them, will also be an important aspect of the panel.",
                        "time_start": "2020-10-25T20:00:00Z",
                        "time_end": "2020-10-25T21:20:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Closing Remarks",
                        "contributors": [
                            "Julien Tierny"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T21:20:00Z",
                        "time_end": "2020-10-25T21:30:00Z"
                    }
                ]
            }
        ]
    },
    "a-beliv": {
        "event": "BELIV",
        "long_name": "BELIV: Evaluation and Beyond - Methodological Approaches for Visualization",
        "event_type": "Workshop",
        "event_description": "",
        "event_url": "https://beliv-workshop.github.io/",
        "sessions": [
            {
                "title": "Vis in the Time of COVID",
                "session_id": "a-beliv-1",
                "chair": [
                    "Kyle Hall"
                ],
                "organizers": [
                    "Anastasia Bezerianos",
                    "Kyle Hall",
                    "Samuel Huron",
                    "Matthew Kay",
                    "Miriah Meyer",
                    "Michael Sedlmair"
                ],
                "display_start": "2020-10-25T14:00:00Z",
                "time_start": "2020-10-25T14:00:00Z",
                "time_end": "2020-10-25T15:30:00Z",
                "discord_category": "beliv",
                "discord_channel": "vis-in-the-time-of-covid",
                "discord_channel_id": "767932168680374302",
                "youtube_url": "https://youtu.be/xlN_QUdT6os",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Zoom",
                        "title": "Introduction and Welcome Message",
                        "contributors": [
                            "Kyle Hall"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:00:00Z",
                        "time_end": "2020-10-25T14:10:00Z"
                    },
                    {
                        "type": "Live Zoom",
                        "title": "Keynote: Lessons Learned from Visualizing the Pandemic",
                        "contributors": [
                            "John Burn-Murdoch"
                        ],
                        "abstract": "For eight months the Financial Times has been producing daily visualisations on the coronavirus pandemic, attempting to distill the most important information into charts for a mass audience spread across the globe. In the process of creating and disseminating more than 1,000 graphics on a topic of huge public interest, we have learned many lessons about the challenges of creating data visualisations that can be easily understood and trusted by an audience that spans a huge range in terms of its data- and visual-literacy, and its pre-existing feelings about a subject matter that has become intensely politicised. This talk will detail several of the challenges we have come up against in this process, and how we have attempted to solve them both within the domain of data visualisation and more broadly through best practices in clear communication.",
                        "time_start": "2020-10-25T14:10:00Z",
                        "time_end": "2020-10-25T14:50:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Distributed Visualization Design: Challenges and Opportunities",
                        "contributors": [
                            "Tatiana Losev"
                        ],
                        "abstract": "We reflect on our experiences as designers of COVID-19 data visu- alizations working in a distributed synchronous design space during the pandemic. This is especially relevant as the pandemic posed new challenges to distributed collaboration amidst civic lock down mea- sures and an increased dependency on spatially distributed teamwork across sectors. As a response to the barriers of working from home being 'the new normal', our team used various synchronous remote design tools and methods with an aim to preserve the richness of co-located collaboration such as face-to-face physical presence with the ability to view real-time body gestures, facial expressions, and the making and sharing of physical artifacts. Since members of our cross-disciplinary team had different technological skills, we explored potential solutions for collaborating and prototyping re- motely using tools at home. Practicing real-time sketching on paper and digitally while meeting over Zoom, Miro, and Google Docs, we cultivated an awareness and spontaneity of collective design activi- ties and idea sharing based on our experiences of co-located design. Using an auto-ethnographic approach, we articulate our experiences through challenges and opportunities throughout the process that may provide useful insights about enabling a more fluid distributed collaboration.",
                        "time_start": "2020-10-25T14:50:00Z",
                        "time_end": "2020-10-25T15:10:00Z",
                        "uid": "a-beliv-8301"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Pipelines Bent, Pipelines Broken: Interdisciplinary Self-Reflection on the Impact of COVID-19 on Current and Future Research",
                        "contributors": [
                            "Priscilla Balestrucci"
                        ],
                        "abstract": "Among the many changes brought about by the COVID-19 pandemic, one of the most pressing for scientific research concerns user testing. For the researchers who conduct studies with human participants, the requirements for social distancing have created a need for reflecting on methodologies that previously seemed relatively straightforward. It has become clear from the emerging literature on the topic and from first-hand experiences of researchers that the restrictions due to the pandemic affect every aspect of the research pipeline. The current paper offers an initial reflection on user-based research, drawing on the authors' own experiences and on the results of a survey that was conducted among researchers in different disciplines, primarily psychology, human-computer interaction (HCI), and visualization communities. While this sampling of researchers is by no means comprehensive, the multi-disciplinary approach and the consideration of different aspects of the research pipeline allow us to examine current and future challenges for user-based research. Through an exploration of these issues, this paper also invites others in the VIS--as well as in the wider--research community, to reflect on and discuss the ways in which the current crisis might also present new and previously unexplored opportunities.",
                        "time_start": "2020-10-25T15:10:00Z",
                        "time_end": "2020-10-25T15:30:00Z",
                        "uid": "a-beliv-3678"
                    }
                ]
            },
            {
                "title": "Evaluation Methods & Extensions",
                "session_id": "a-beliv-2",
                "chair": [
                    "Matthew Kay"
                ],
                "organizers": [
                    "Anastasia Bezerianos",
                    "Kyle Hall",
                    "Samuel Huron",
                    "Matthew Kay",
                    "Miriah Meyer",
                    "Michael Sedlmair"
                ],
                "display_start": "2020-10-25T16:00:00Z",
                "time_start": "2020-10-25T16:00:00Z",
                "time_end": "2020-10-25T17:30:00Z",
                "discord_category": "beliv",
                "discord_channel": "evaluation-methods-extensions",
                "discord_channel_id": "767932230210813952",
                "youtube_url": "https://youtu.be/yoQoNCQoL_k",
                "zoom_meeting": "https://us02web.zoom.us/j/85317890243?pwd=WlJBc2VXMkk4OXEwK00vNkpsSDgxZz09",
                "zoom_password": "fnthKizR",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "How to evaluate data visualizations across different levels of understanding",
                        "contributors": [
                            "Alyxander Burns"
                        ],
                        "abstract": "Understanding a visualization is a multi-level process. A reader must extract and extrapolate from numeric facts, understand how those facts apply to both the context of the data and other potential contexts, and draw or evaluate conclusions from the data. A well-designed visualization should support each of these levels of understanding. We diagnose levels of understanding of visualized data by adapting Bloom's taxonomy, a common framework from the education literature. We describe each level of the framework and provide examples for how it can be applied to evaluate the efficacy of data visualizations along six levels of knowledge acquisition - knowledge, comprehension, application, analysis, synthesis, and evaluation. We present three case studies showing that this framework expands on existing methods to comprehensively measure how a visualization design facilitates a viewer's understanding of visualizations. Although Bloom's original taxonomy suggests a strong hierarchical structure for some domains, we found few examples of dependent relationships between performance at different levels for our three case studies. If this level-independence holds across new tested visualizations, the taxonomy could serve to inspire more targeted evaluations of levels of understanding that are relevant to a communication goal.",
                        "time_start": "2020-10-25T16:00:00Z",
                        "time_end": "2020-10-25T16:20:00Z",
                        "uid": "a-beliv-8820"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Using Close Reading as a Method for Evaluating Visualizations",
                        "contributors": [
                            "Annie Bares"
                        ],
                        "abstract": "Visualization research and practice that incorporates the arts make claims to being more effective in connecting with users on a human level. However, these claims are difficult to measure quantitatively. In this paper, we present a follow-on study to use close reading, a humanities method from literary studies, to evaluate visualizations created using artistic processes [Bares 2020]. Close reading is a method in literary studies that we've previously explored as a method for evaluating visualizations. To use close reading as an evaluation method, we guide participants through a series of steps designed to prompt them to interpret the visualization's formal, informational, and contextual features. Here we elaborate on our motivations for using close reading as a method to evaluate visualizations, and enumerate the procedures we used in the study to evaluate a 2D visualization, including modifications made because of the COVID-19 pandemic. Key findings of this study include that close reading is an effective formative method to elicit information related to interpretation and critique; user subject position; and suspicion or skepticism. Information gained through close reading is valuable in the visualization design and iteration processes, both related to designing features and other formal elements more effectively, as well as in considering larger questions of context and framing.",
                        "time_start": "2020-10-25T16:20:00Z",
                        "time_end": "2020-10-25T16:40:00Z",
                        "uid": "a-beliv-6687"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Micro-entries: Encouraging Deeper Evaluation of Mental Models Over Time for Interactive Data Systems",
                        "contributors": [
                            "Jeremy E Block"
                        ],
                        "abstract": "Many interactive data systems combine visual representations of data with embedded algorithmic support for automation and data exploration. To effectively support transparent and explainable data systems, it is important for researchers and designers to know how users understand the system. We discuss the evaluation of users' mental models of system logic. Mental models are challenging to capture and analyze. While common evaluation methods aim to approximate the user's final mental model after a period of system usage, user understanding continuously evolves as users interact with a system over time. In this paper, we review many common mental model measurement techniques, discuss tradeoffs, and recommend methods for deeper, more meaningful evaluation of mental models when using interactive data analysis and visualization systems. We present guidelines for evaluating mental models over time to allow for assessment of the evolution of specific model updates and mapping to particular use of interface features and data queries. By asking users to describe what they know and how they know it, researchers can collect structured, time-ordered insight into a user's conceptualization process while also helping guide users to their own discoveries.",
                        "time_start": "2020-10-25T16:40:00Z",
                        "time_end": "2020-10-25T17:00:00Z",
                        "uid": "a-beliv-5851"
                    },
                    {
                        "type": "Zoom Breakout",
                        "title": "Zoom Breakout Session",
                        "contributors": [
                            "All"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T17:00:00Z",
                        "time_end": "2020-10-25T17:30:00Z"
                    }
                ]
            },
            {
                "title": "Provocations",
                "session_id": "a-beliv-3",
                "chair": [
                    "Miriah Meyer"
                ],
                "organizers": [
                    "Anastasia Bezerianos",
                    "Kyle Hall",
                    "Samuel Huron",
                    "Matthew Kay",
                    "Miriah Meyer",
                    "Michael Sedlmair"
                ],
                "display_start": "2020-10-25T18:00:00Z",
                "time_start": "2020-10-25T18:00:00Z",
                "time_end": "2020-10-25T19:30:00Z",
                "discord_category": "beliv",
                "discord_channel": "provocations",
                "discord_channel_id": "767932251815280660",
                "youtube_url": "https://youtu.be/m7tR2tV-o3k",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Panel",
                        "title": "Fireside Chat: Thoughts on Design Studies",
                        "contributors": [
                            "Michael Correll",
                            "Tamara Munzner"
                        ],
                        "abstract": "We often point to the relative increase in the amount and sophistication of evaluations of visualization systems versus the earliest days of the field as evidence that we are maturing as a field. I am not so convinced. In particular, I feel that evaluations of visualizations, as they are ordinarily performed in the field or asked for by reviewers, fail to tell us very much that is useful or transferable about visualization systems, regardless of the statistical rigor or ecological validity of the evaluation. Through a series of thought experiments, I show how our current conceptions of visualization evaluations can be incomplete, capricious, or useless for the goal of furthering the field, more in line with the \"heroic age\" of medical science than the rigorous evidence-based field we might aspire to be. I conclude by suggesting that our models for designing evaluations, and our priorities as a field, should be revisited.",
                        "time_start": "2020-10-25T18:00:00Z",
                        "time_end": "2020-10-25T18:30:00Z",
                        "uid": "a-beliv-8994"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Towards Identification and Mitigation of Task-Based Challenges in Comparative Visualization Studies",
                        "contributors": [
                            "Aditeya Pandey"
                        ],
                        "abstract": "The effectiveness of a visualization technique is dependent on how well it supports the tasks or goals of an end-user. To measure the effectiveness of a visualization technique, researchers often use a comparative study design. In a comparative study, two or more visualization techniques are compared over a set of tasks and commonly measure human performance in terms of task accuracy and completion time. Despite the critical role of tasks in comparative studies, the current lack of guidance in existing literature on best practices for task selection and communication of research results in evaluation studies is problematic. In this work, we systematically identify and curate the task-based challenges of comparative studies by reviewing existing visualization literature on the topic. Furthermore, for each of the presented challenges we discuss the potential threats to validity for a comparative study. The challenges discussed in this paper are further backed by evidence identified in a detailed survey of comparative tree visualization studies. Finally, we recommend best practices from personal experience and the surveyed tree visualization studies to provide guidelines for other researchers to mitigate the challenges. The survey data and a free copy of the paper is available at https://osf.io/g3btk/",
                        "time_start": "2020-10-25T18:30:00Z",
                        "time_end": "2020-10-25T18:40:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Panel: Vis Evaluation Moving into the Next Decade",
                        "contributors": [
                            "Florian Alt",
                            "Petra Isenberg",
                            "Tamara Munzner",
                            "Aditeya Pandey",
                            "Danielle Szafir"
                        ],
                        "abstract": "The panelists will discuss how evaluation will evolve in the coming years, both in response to the on-going COVID-19 pandemic and beyond, as we come out the other side of the current crisis.",
                        "time_start": "2020-10-25T18:40:00Z",
                        "time_end": "2020-10-25T19:30:00Z",
                        "uid": "a-beliv-1854"
                    }
                ]
            },
            {
                "title": "Design Methods & Extensions",
                "session_id": "a-beliv-4",
                "chair": [
                    "Kyle Hall"
                ],
                "organizers": [
                    "Anastasia Bezerianos",
                    "Kyle Hall",
                    "Samuel Huron",
                    "Matthew Kay",
                    "Miriah Meyer",
                    "Michael Sedlmair"
                ],
                "display_start": "2020-10-25T20:00:00Z",
                "time_start": "2020-10-25T20:00:00Z",
                "time_end": "2020-10-25T21:30:00Z",
                "discord_category": "beliv",
                "discord_channel": "design-methods-extensions",
                "discord_channel_id": "767932304294674472",
                "youtube_url": "https://youtu.be/pwl9U0Dp9us",
                "zoom_meeting": "https://us02web.zoom.us/j/85976980717?pwd=ekx6MTI5Q2NKM0lUdWpna2pmTExXUT09",
                "zoom_password": "VKJSIWOz",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Understanding User Experience of COVID-19 Maps through Remote Elicitation Interviews",
                        "contributors": [
                            "Damla \u00c7ay"
                        ],
                        "abstract": "During the coronavirus pandemic, visualizations gained a new level of popularity and meaning for a wider audience. People were bombarded with a wide set of public health visualizations ranging from simple graphs to complex interactive dashboards. In a pandemic setting, where large amounts of the world population are socially distancing themselves, it becomes an urgent need to refine existing user experience evaluation methods for remote settings to understand how people make sense out of COVID-19 related visualizations. When evaluating visualizations aimed towards the general public with vastly different socio-demographic backgrounds and varying levels of technical savviness and data literacy, it is important to understand user feedback beyond aspects such as speed, task accuracy, or usability problems. As a part of this wider evaluation perspective, micro-phenomenology has been used to evaluate static and narrative visualizations to reveal the lived experience in a detailed way. Building upon these studies, we conducted a user study to understand how to employ Elicitation (aka Micro-phenomenological) interviews in remote settings. In a case study, we investigated what experiences the participants had with map-based interactive visualizations. Our findings reveal positive and negative aspects of conducting Elicitation interviews remotely. Our results can inform the process of planning and executing remote Elicitation interviews to evaluate interactive visualizations. In addition, we share recommendations regarding visualization techniques and interaction design about public health data.",
                        "time_start": "2020-10-25T20:00:00Z",
                        "time_end": "2020-10-25T20:20:00Z",
                        "uid": "a-beliv-2850"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Data-First Visualization Design Studies",
                        "contributors": [
                            "Michael Oppermann"
                        ],
                        "abstract": "We introduce the notion of a data-first design study which is triggered by the acquisition of real-world data instead of specific stakeholder analysis questions. We propose an adaptation of the design study methodology framework to provide practical guidance and to aid transferability to other data-first design processes. We discuss opportunities and risks by reflecting on two of our own data-first design studies. We review 64 previous design studies and identify 16 of them as edge cases with characteristics that may indicate a data-first design process in action.",
                        "time_start": "2020-10-25T20:20:00Z",
                        "time_end": "2020-10-25T20:40:00Z",
                        "uid": "a-beliv-4421"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Extending Recommendations for Creative Visualization-Opportunities Workshops",
                        "contributors": [
                            "Christian Knoll"
                        ],
                        "abstract": "Participatory design is an approach in human-computer interaction to involve all relevant stakeholders coequally in the design process. A recent participatory method for visualization design is the creative visualization-opportunities (CVO) workshop, which is used to efficiently develop visualization design requirements in the early stages of applied visualization work. In this paper we report on our experiences of running four CVO workshops in different domains with diverse participants to explore new methods and variations of workshop variables. Through reflection on our experiences we propose two contributions that extend existing guidance for planning, executing, and analyzing CVO workshops: a set of 12 pragmatic recommendations that extend and complement existing ones; and a recommended method for analyzing workshop results, called user stories. Additionally, we report on the outcomes of our successful workshops to provide evidence for the efficiacy of CVO workshops.",
                        "time_start": "2020-10-25T20:40:00Z",
                        "time_end": "2020-10-25T21:00:00Z",
                        "uid": "a-beliv-6130"
                    },
                    {
                        "type": "Zoom Breakout",
                        "title": "Zoom Breakout Session",
                        "contributors": [
                            "All"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T21:00:00Z",
                        "time_end": "2020-10-25T21:30:00Z"
                    }
                ]
            }
        ]
    },
    "a-vis4dh": {
        "event": "VIS4DH",
        "long_name": "VIS4DH: Workshop on Visualization for the Digital Humanities",
        "event_type": "Workshop",
        "event_description": "For 4 years now, the VIS4DH workshop has brought together researchers and practitioners from the fields of visualization and the humanities to discuss new research directions at the intersection of visualization and (digital) humanities research. The general aim is to foster productive collaborations that mutually advances all fields involved. Papers and invited talks from previous years can be found here.\n\nRe-Thinking\nRe-Defining\nRe-Imagining Data\n\nThis year, we invite a dialogue on the topic and terminology around \u201cdata\u201d, a contested term within and between a range of disciplines. There are varying perspectives on the meaning and use of data that impact how we engage and debate it. \u201cData\u201d seems core to visualization! Or is it? We invite both theoretical and applied works at the intersection of visualization and (digital) humanities around these and other questions related to the workshop theme.",
        "event_url": "http://www.vis4dh.org/",
        "sessions": [
            {
                "title": "Session 1",
                "session_id": "a-vis4dh-1",
                "chair": [
                    "Florian Windhager",
                    "Eric Alexander",
                    "Catherine DeRose"
                ],
                "organizers": [
                    "Catherine Derose",
                    "Uta Hinrichs",
                    "Mennatallah El-Assady"
                ],
                "display_start": "2020-10-25T14:00:00Z",
                "time_start": "2020-10-25T14:00:00Z",
                "time_end": "2020-10-25T15:30:00Z",
                "discord_category": "vis4dh",
                "discord_channel": "session-1",
                "discord_channel_id": "767932178272354334",
                "youtube_url": "https://youtu.be/UPXeSFDXOLI",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Opening",
                        "contributors": [
                            "Vis4DH OC"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:00:00Z",
                        "time_end": "2020-10-25T14:10:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Keynote: From the Flaneur to the Fold: Imbuing Data Visualization with Philosophy",
                        "contributors": [
                            "Marian D\u00f6rk"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:10:00Z",
                        "time_end": "2020-10-25T15:00:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Provocations I: Ground Truths for the Humanities",
                        "contributors": [
                            "Yvette Oortwijn",
                            "Hein van den Berg",
                            "Arianna Betti"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T15:00:00Z",
                        "time_end": "2020-10-25T15:10:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Provocations I: The Necessity of Re in Re-Thinking, Re-Defining, Re-Imagining Data",
                        "contributors": [
                            "Chris Weaver"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T15:10:00Z",
                        "time_end": "2020-10-25T15:20:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Provocations I: Data as material versus data as reason",
                        "contributors": [
                            "Paul Heinicker"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T15:20:00Z",
                        "time_end": "2020-10-25T15:30:00Z"
                    }
                ]
            },
            {
                "title": "Session 2",
                "session_id": "a-vis4dh-2",
                "chair": [
                    "Florian Windhager",
                    "Stefania Forlini"
                ],
                "organizers": [
                    "Catherine Derose",
                    "Uta Hinrichs",
                    "Mennatallah El-Assady"
                ],
                "display_start": "2020-10-25T16:00:00Z",
                "time_start": "2020-10-25T16:00:00Z",
                "time_end": "2020-10-25T17:30:00Z",
                "discord_category": "vis4dh",
                "discord_channel": "session-2",
                "discord_channel_id": "767932237081083925",
                "youtube_url": "https://youtu.be/08I2gvy0Swk",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Papers I: Externalizing Transformations of Historical Documents: Opportunities for Provenance-Driven Visualization",
                        "contributors": [
                            "Tomas Vancisin",
                            "Mary Orr",
                            "Uta Hinrichs"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:00:00Z",
                        "time_end": "2020-10-25T16:15:00Z",
                        "uid": "a-vis4dh-13"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Papers I: Supporting Expert Close Analysis of Historical Scientific Writings: A Case Study for Near-by Reading",
                        "contributors": [
                            "Andrew McNutt",
                            "Agatha Kim",
                            "Sergio Elahi",
                            "Kazutaka Takahashi"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:15:00Z",
                        "time_end": "2020-10-25T16:30:00Z",
                        "uid": "a-vis4dh-05"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Papers I: ViS-\u00c1-ViS : Detecting Similar Patterns in Annotated Literary Text",
                        "contributors": [
                            "Moshe Schorr",
                            "Oren Mishali",
                            "Benny Kimelfeld",
                            "Ophir M\u00fcnz-Manor"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:30:00Z",
                        "time_end": "2020-10-25T16:45:00Z",
                        "uid": "a-vis4dh-10"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Papers I: Literal Encoding: Text is a first-class data encoding",
                        "contributors": [
                            "Richard Brath"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:45:00Z",
                        "time_end": "2020-10-25T17:00:00Z",
                        "uid": "a-vis4dh-14"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Lab Talks I: Speculative Narratives and Networks Studio",
                        "contributors": [
                            "Andrew Burrell"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T17:00:00Z",
                        "time_end": "2020-10-25T17:04:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Lab Talks I: Modeling the Cosmos - Digital Research Environment for the Computational History of Astronomy",
                        "contributors": [
                            "Luca Beisel"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T17:04:00Z",
                        "time_end": "2020-10-25T17:08:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Lab Talks I: DensityDesign Lab",
                        "contributors": [
                            "Michele Mauri"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T17:08:00Z",
                        "time_end": "2020-10-25T17:12:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Lab Talks I: Project Cornelia",
                        "contributors": [
                            "Koenraad Brosens"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T17:12:00Z",
                        "time_end": "2020-10-25T17:16:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Lab Talks I: UCLAB",
                        "contributors": [
                            "Mark-Jan Bludau"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T17:16:00Z",
                        "time_end": "2020-10-25T17:20:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Lab Talks I: VisUsal",
                        "contributors": [
                            "Roberto Theron"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T17:20:00Z",
                        "time_end": "2020-10-25T17:24:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Discussion",
                        "contributors": [
                            "All"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T17:24:00Z",
                        "time_end": "2020-10-25T17:30:00Z"
                    }
                ]
            },
            {
                "title": "Session 3",
                "session_id": "a-vis4dh-3",
                "chair": [
                    "Houda Lamqaddam",
                    "Catherine DeRose"
                ],
                "organizers": [
                    "Catherine Derose",
                    "Uta Hinrichs",
                    "Mennatallah El-Assady"
                ],
                "display_start": "2020-10-25T18:00:00Z",
                "time_start": "2020-10-25T18:00:00Z",
                "time_end": "2020-10-25T19:30:00Z",
                "discord_category": "vis4dh",
                "discord_channel": "session-3",
                "discord_channel_id": "767932259360702464",
                "youtube_url": "https://youtu.be/6U5RUwXkuUQ",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Capstone: Data Feminism",
                        "contributors": [
                            "Lauren Klein",
                            "Catherine D'Ignazio"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T18:00:00Z",
                        "time_end": "2020-10-25T19:00:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Provocations II: Making Sense of the Sea of Dashboards",
                        "contributors": [
                            "Michael Correll",
                            "Heather Froehlich"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T19:00:00Z",
                        "time_end": "2020-10-25T19:10:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Provocations II: Using Visual Analytics for Preservation and Documentation of Marginalised Perspectives",
                        "contributors": [
                            "Audrey Reinert",
                            "David Ebert"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T19:10:00Z",
                        "time_end": "2020-10-25T19:20:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Provocations II: Data is never raw",
                        "contributors": [
                            "Micah Vandegrift",
                            "Ashley Evans Bandy and Scott Bailey"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T19:20:00Z",
                        "time_end": "2020-10-25T19:30:00Z"
                    }
                ]
            },
            {
                "title": "Session 4",
                "session_id": "a-vis4dh-4",
                "chair": [
                    "Chris Weaver",
                    "Menna El-Assady",
                    "Eric Alexander"
                ],
                "organizers": [
                    "Catherine Derose",
                    "Uta Hinrichs",
                    "Mennatallah El-Assady"
                ],
                "display_start": "2020-10-25T20:00:00Z",
                "time_start": "2020-10-25T20:00:00Z",
                "time_end": "2020-10-25T21:30:00Z",
                "discord_category": "vis4dh",
                "discord_channel": "session-4",
                "discord_channel_id": "767932312797052979",
                "youtube_url": "https://youtu.be/HA3oGQYuoKs",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Papers II: Bio-inspired Structure Identification in Language Embeddings",
                        "contributors": [
                            "Hongwei (Henry) Zhou",
                            "Oskar Elek",
                            "Pranav Anand",
                            "Angus G. Forbes"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:00:00Z",
                        "time_end": "2020-10-25T20:15:00Z",
                        "uid": "a-vis4dh-08"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Papers II: Augmenting Sheet Music with Rhythmic Fingerprints",
                        "contributors": [
                            "Daniel F\u00fcrst",
                            "Matthias Miller",
                            "Daniel Keim",
                            "Alexandra Bonnici",
                            "Hanna Sch\u00e4fer",
                            "Mennatallah El-Assady"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:15:00Z",
                        "time_end": "2020-10-25T20:30:00Z",
                        "uid": "a-vis4dh-09"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Papers II: Pilaster: A Collection of Citation Metadata Extracted From Publications on Visualization for the Digital Humanities",
                        "contributors": [
                            "Alejandro Benito-Santos",
                            "Roberto Ther\u00f3n"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:30:00Z",
                        "time_end": "2020-10-25T20:45:00Z",
                        "uid": "a-vis4dh-11"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Papers II: Visualizing a Large Spatiotemporal Collection of Historic Photography with a Generous Interface",
                        "contributors": [
                            "Taylor Arnold",
                            "Nathaniel Ayers",
                            "Justin Madron",
                            "Robert Nelson",
                            "Lauren Tilton"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:45:00Z",
                        "time_end": "2020-10-25T21:00:00Z",
                        "uid": "a-vis4dh-12"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Lab Talks II: Sussex Humanities Lab",
                        "contributors": [
                            "Tim Hitchcock"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T21:00:00Z",
                        "time_end": "2020-10-25T21:04:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Lab Talks II: British Library Digital Scholarship Department",
                        "contributors": [
                            "Olivia Vane"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T21:04:00Z",
                        "time_end": "2020-10-25T21:08:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Lab Talks II: Wired! Lab for Digital Art History & Visual Culture",
                        "contributors": [
                            "Hannah Jacobs"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T21:08:00Z",
                        "time_end": "2020-10-25T21:12:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Lab Talks II: Distant Viewing Lab",
                        "contributors": [
                            "Lauren Tilton"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T21:12:00Z",
                        "time_end": "2020-10-25T21:16:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Lab Talks II: The Electronic Literature Lab",
                        "contributors": [
                            "Dene Grigar"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T21:16:00Z",
                        "time_end": "2020-10-25T21:20:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Lab Talks II: Data Experience Lab",
                        "contributors": [
                            "Micah Vandegrift"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T21:20:00Z",
                        "time_end": "2020-10-25T21:24:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Closing",
                        "contributors": [
                            "Vis4DH OC"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T21:24:00Z",
                        "time_end": "2020-10-25T21:30:00Z"
                    }
                ]
            }
        ]
    },
    "t-analysisdesign": {
        "event": "Visualization Analysis and Design",
        "long_name": "Visualization Analysis and Design",
        "event_type": "Tutorial",
        "event_description": "This introductory tutorial will provide a broad foundation for thinking systematically about visualization systems, built around the idea that becoming familiar with analyzing existing systems is a good springboard for designing new ones. The major data types of concern in visual analytics, information visualization, and scientific visualization will all be covered: tables, networks, and sampled spatial data. This tutorial is focused on data and task abstractions, and the design choices for visual encoding and interaction; it will not cover algorithms. No background in computer science or visualization is assumed.",
        "event_url": "",
        "sessions": [
            {
                "title": "Visualization Analysis and Design",
                "session_id": "t-analysisdesign",
                "chair": [
                    "Tamara Munzner"
                ],
                "organizers": [
                    "Tamara Munzner"
                ],
                "display_start": "2020-10-25T14:00:00Z",
                "time_start": "2020-10-25T14:00:00Z",
                "time_end": "2020-10-25T17:30:00Z",
                "discord_category": "visualization analysis and design",
                "discord_channel": "general",
                "discord_channel_id": "767932184832245811",
                "youtube_url": "https://youtu.be/_sFzBmQeCLY",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Visualization Analysis Framework: Analysis: What, Why, How",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:00:00Z",
                        "time_end": "2020-10-25T14:30:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visualization Analysis Framework: Marks and Channels",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:30:00Z",
                        "time_end": "2020-10-25T14:49:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visualization Analysis Framework: Arrange Tables",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:49:00Z",
                        "time_end": "2020-10-25T15:18:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visualization Analysis Framework: Arrange Spatial Data",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T15:18:00Z",
                        "time_end": "2020-10-25T15:30:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Idiom Design Choices: Arrange Networks and Tree",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:00:00Z",
                        "time_end": "2020-10-25T16:16:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Idiom Design Choices: Map Color and Other Channels",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:16:00Z",
                        "time_end": "2020-10-25T16:31:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Idiom Design Choices: Manipulate: Change, Select, Navigate",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:31:00Z",
                        "time_end": "2020-10-25T16:43:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Idiom Design Choices: Facet: Juxtapose, Partition, Superimpose",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:43:00Z",
                        "time_end": "2020-10-25T17:03:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Idiom Design Choices: Reduce: Filter, Aggregate",
                        "contributors": [
                            "Tamara Munzner"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T17:03:00Z",
                        "time_end": "2020-10-25T17:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-mlui": {
        "event": "MLUI 2020: Machine Learning from User Interaction for Visualization and Analytics",
        "long_name": "MLUI 2020: Machine Learning from User Interaction for Visualization and Analytics",
        "event_type": "Workshop",
        "event_description": "The high-level goal of this workshop is to bring together researchers from across the VIS community to share their knowledge and build collaborations at the intersection of the Machine Learning and Visualization fields, with a focus on learning from user interaction. Our hope in this workshop is to pull expertise from across all fields of VIS in order to generate open discussion about how we currently learn from user interaction and where we can go with future research.\n\nTo achieve this goal, we propose a workshop that incorporates paper presentations, keynotes, and free-form discussion. Our goal is to allow for the presentation of cutting-edge research, while also providing participants and speakers with time to exchange ideas and to discuss new research directions.",
        "event_url": "https://learningfromusersworkshop.github.io/",
        "sessions": [
            {
                "title": "MLUI 2020: Machine Learning from User Interaction for Visualization and Analytics",
                "session_id": "w-mlui",
                "chair": [
                    "Michelle Dowling",
                    "John Wenskovitch"
                ],
                "organizers": [],
                "display_start": "2020-10-25T14:00:00Z",
                "time_start": "2020-10-25T14:00:00Z",
                "time_end": "2020-10-25T17:30:00Z",
                "discord_category": "mlui 2020 machine learning from user interaction for visualization and analytics",
                "discord_channel": "general",
                "discord_channel_id": "767932191975800872",
                "youtube_url": "https://youtu.be/Sj-YhKszTGw",
                "zoom_meeting": "https://us02web.zoom.us/j/84131685469?pwd=OWFySXJtSFFHVmxsMzBEcHV3L1ZHdz09",
                "zoom_password": "kkoSQHOx",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Introduction and Welcome",
                        "contributors": [
                            "John Wenskovitch"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:00:00Z",
                        "time_end": "2020-10-25T14:05:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Machine Learning meets Visualization",
                        "contributors": [
                            "Michael Sedlmair"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:05:00Z",
                        "time_end": "2020-10-25T14:50:00Z"
                    },
                    {
                        "type": "Live Presentation w/ Backup",
                        "title": "Bridging Cognitive Gaps Between User and Model in Interactive Dimension Reduction",
                        "contributors": [
                            "Ming Wang"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:50:00Z",
                        "time_end": "2020-10-25T15:05:00Z"
                    },
                    {
                        "type": "Zoom Breakout",
                        "title": "Discussion",
                        "contributors": [
                            "All"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T15:05:00Z",
                        "time_end": "2020-10-25T15:30:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Welcome Back",
                        "contributors": [
                            "Michelle Dowling"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:00:00Z",
                        "time_end": "2020-10-25T16:01:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Using Models and Predictions to Help Humans and Computers Click",
                        "contributors": [
                            "Alvitta Ottley"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:01:00Z",
                        "time_end": "2020-10-25T16:45:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Late-Breaking Work",
                        "contributors": [
                            "TBD, still taking submissions"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:45:00Z",
                        "time_end": "2020-10-25T17:00:00Z"
                    },
                    {
                        "type": "Zoom Breakout",
                        "title": "Discussion",
                        "contributors": [
                            "All"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T17:00:00Z",
                        "time_end": "2020-10-25T17:20:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Synthesis and Next Steps",
                        "contributors": [
                            "All"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T17:20:00Z",
                        "time_end": "2020-10-25T17:30:00Z"
                    }
                ]
            }
        ]
    },
    "t-colorbasics": {
        "event": "Color Basics for Creating Visualizations",
        "long_name": "Color Basics for Creating Visualizations",
        "event_type": "Tutorial",
        "event_description": "We provide an overview of the basics of color theory and demonstrate how to apply the concepts to visualization. Our tutorial is intended for a broad audience of individuals interested in understanding the mysteries of color. Our journey includes the introduction to the concepts of color models and harmony, a review of color vision principles, the defining of color gamut, spaces and systems, and demonstrating online and mobile apps for performing color analyses of digital media. Freely available commercial and research tools for your continued use in color selection and color deficiency assessments are highlighted. The tutorial includes concepts from art and design such as extending the fundamentals of the Bauhaus into data visualization as well as overviews of color perception and appearance principals for vision. Emerging trends in automated color selection and deep learning colorization are also highlighted.",
        "event_url": "",
        "sessions": [
            {
                "title": "Color Basics for Creating Visualizations",
                "session_id": "t-colorbasics",
                "chair": [
                    "Theresa-Marie Rhyne"
                ],
                "organizers": [
                    "Theresa-Marie Rhyne"
                ],
                "display_start": "2020-10-25T14:00:00Z",
                "time_start": "2020-10-25T14:00:00Z",
                "time_end": "2020-10-25T17:30:00Z",
                "discord_category": "color basics for creating visualizations",
                "discord_channel": "general",
                "discord_channel_id": "767932199571292160",
                "youtube_url": "https://youtu.be/RiG1Rn0Acn0",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Review of Color Fundamentals",
                        "contributors": [
                            "Theresa-Marie Rhyne"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:00:00Z",
                        "time_end": "2020-10-25T15:30:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Putting it all Together with Color Apps & Case Studies",
                        "contributors": [
                            "Theresa-Marie Rhyne"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:00:00Z",
                        "time_end": "2020-10-25T17:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-guides": {
        "event": "VisGuides: 3rd Workshop on the Creation, Curation, Critique and Conditioning of Principles and Guidelines in Visualization",
        "long_name": "VisGuides: 3rd Workshop on the Creation, Curation, Critique and Conditioning of Principles and Guidelines in Visualization",
        "event_type": "Workshop",
        "event_description": "Our VisGuides 2020 Workshop proposal focuses on the analysis, design, reflection, and discussion of applicable frameworks to mastering guidelines in visualization by the broader visualization community, embedded in a larger research agenda of visualization theory and practices.\n\nThis workshop follows-up the ideas from the IEEE VIS 2016 and 2018 Workshop on Creation, Curation, Critique and Conditioning of Principles and Guidelines in Visualization (C4PGV) (http://c4pgv.swansea.ac.uk, https://c4pgv.dbvis.de/).\n\nThe workshop wants to deliver concrete ideas and meta-guidelines about how the visualization community can contribute to the collection, storage, formulation, and dissemination of guidelines - within and beyond the visualization community. To make this possible, we aim at:\n\nDiscussing work-in-progress and current activities around visualization guidelines.\nCollect a comprehensive list of common and less common guidelines.\nBased on this collection, exercise, create, and discuss a possible framework, or template, or methodology to capture guidelines.\nDiscuss a research agenda on how to address open questions around guidelines and how on-going research - in any field of visualization - can contribute to sustainable management and discussion of guidelines.",
        "event_url": "https://nms.kcl.ac.uk/c4pgv/",
        "sessions": [
            {
                "title": "VisGuides: 3rd Workshop on the Creation, Curation, Critique and Conditioning of Principles and Guidelines in Visualization",
                "session_id": "w-guides",
                "chair": [
                    "Benjamin Bach",
                    "Alfie Abdul-Rahman",
                    "Alexandra Diehl"
                ],
                "organizers": [
                    "Benjamin Bach",
                    "Alfie Abdul-Rahman",
                    "Alexandra Diehl"
                ],
                "display_start": "2020-10-25T14:00:00Z",
                "time_start": "2020-10-25T14:00:00Z",
                "time_end": "2020-10-25T17:30:00Z",
                "discord_category": "visguides 3rd workshop on the creation curation critique and conditioning of principles and guideli",
                "discord_channel": "general",
                "discord_channel_id": "767932207381086228",
                "youtube_url": "https://youtu.be/x_7MngN-M8k",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Welcome",
                        "contributors": [
                            "Alfie Abdul-Rahman"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:00:00Z",
                        "time_end": "2020-10-25T14:05:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Keynote 1",
                        "contributors": [
                            "Silvia Miksch"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:05:00Z",
                        "time_end": "2020-10-25T14:30:00Z"
                    },
                    {
                        "type": "Live Q&A",
                        "title": "Questions",
                        "contributors": [
                            "Alfie Abdul-Rahman"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:30:00Z",
                        "time_end": "2020-10-25T14:40:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Presentation - Short Bio Panelists",
                        "contributors": [
                            "Alexandra Diehl"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:40:00Z",
                        "time_end": "2020-10-25T14:45:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Panel 1: Teaching Visualization Guidelines",
                        "contributors": [
                            "Sheelagh Carpendale",
                            "Arzu Coltekin",
                            "Robert S. Laramee",
                            "Lace Padilla",
                            "Jonathan Roberts"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:45:00Z",
                        "time_end": "2020-10-25T15:30:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Keynote 2",
                        "contributors": [
                            "Eser Kandogan"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:00:00Z",
                        "time_end": "2020-10-25T16:25:00Z"
                    },
                    {
                        "type": "Live Q&A",
                        "title": "Questions",
                        "contributors": [
                            "Alfie Abdul-Rahman"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:25:00Z",
                        "time_end": "2020-10-25T16:35:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Presentation - Short Bio Panelists",
                        "contributors": [
                            "Benjamin Bach"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:35:00Z",
                        "time_end": "2020-10-25T16:40:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Panel 2: Visualization Response in a Time of Pandemic",
                        "contributors": [
                            "Sara Fabriknat",
                            "Alexander Lex",
                            "Amanda Makulec",
                            "Cagatay Turkay",
                            "Tatiana von Landesberger"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:40:00Z",
                        "time_end": "2020-10-25T17:25:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Final Wrap up",
                        "contributors": [
                            "Alexandra Diehl"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T17:25:00Z",
                        "time_end": "2020-10-25T17:30:00Z"
                    }
                ]
            }
        ]
    },
    "t-paraview": {
        "event": "ParaView Tutorial",
        "long_name": "ParaView Tutorial",
        "event_type": "Tutorial",
        "event_description": "ParaView is a flexible, extensible, open source, visualization tool that can operate both serially and in distributed memory parallel modes. It can be used to visualize and analyze both large and small data, both interactively and in batch. In this tutorial, we will expose the pipeline based design of the tool and show attendees how they can get meaningful visualizations from their data. We will cover frequently used ParaView workflows, including data wrangling, common filters for transforming data for visualization, choosing representations, exploring the rendering capabilities, and manipulating color maps. Attendees will leave the tutorial confident in their ability to run ParaView and find solutions to their problems.",
        "event_url": "",
        "sessions": [
            {
                "title": "ParaView Tutorial",
                "session_id": "t-paraview",
                "chair": [
                    "Michael Migliore",
                    "Ethan V. Stam",
                    "John M. Patchett",
                    "Dan Lipsa",
                    "Ken Moreland"
                ],
                "organizers": [
                    "John M. Patchett",
                    "Ethan V. Stam",
                    "Dan Lipsa",
                    "Michael Migliore",
                    "Ken Moreland"
                ],
                "display_start": "2020-10-25T14:00:00Z",
                "time_start": "2020-10-25T14:00:00Z",
                "time_end": "2020-10-25T17:30:00Z",
                "discord_category": "paraview tutorial",
                "discord_channel": "general",
                "discord_channel_id": "767932215669555210",
                "youtube_url": "https://youtu.be/YnK_V8Sxteo",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "ParaView Introduction, Overview, Installation",
                        "contributors": [
                            "John Patchett"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:00:00Z",
                        "time_end": "2020-10-25T14:15:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "GUI by Example Section 1",
                        "contributors": [
                            "John Patchett",
                            "Ethan Stam"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T14:15:00Z",
                        "time_end": "2020-10-25T15:05:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "GUI by Example Section 2",
                        "contributors": [
                            "John Patchett"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T15:05:00Z",
                        "time_end": "2020-10-25T15:30:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "GUI by Example Section 3",
                        "contributors": [
                            "Ethan Stam"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:00:00Z",
                        "time_end": "2020-10-25T16:25:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Effective Color Usage",
                        "contributors": [
                            "Ken Moreland"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:25:00Z",
                        "time_end": "2020-10-25T16:45:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Advanced Rendering Techniques Section 1",
                        "contributors": [
                            "Dan Lipsa",
                            "Michael Migliore"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T16:45:00Z",
                        "time_end": "2020-10-25T17:05:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Advanced Rendering Techniques Section 2",
                        "contributors": [
                            "Dan Lipsa",
                            "Michael Migliore"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T17:05:00Z",
                        "time_end": "2020-10-25T17:25:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Closing Presentation",
                        "contributors": [
                            "John Patchett"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T17:25:00Z",
                        "time_end": "2020-10-25T17:30:00Z"
                    }
                ]
            }
        ]
    },
    "t-empiricaltransparent": {
        "event": "How to Make Your Empirical Research Transparent",
        "long_name": "How to Make Your Empirical Research Transparent",
        "event_type": "Tutorial",
        "event_description": "Two fundamental tenets of scientific research are that it can be scrutinized and built-upon. Both require that the collected data, supporting materials, and decision timing be shared, so others can examine, reuse, and extend them. This tutorial will teach how you can share the artifacts of your own research. You will learn about the benefits gained by making different components or stages of research transparent, including decision timing, data collection procedures, raw data, and analysis & code. And for each, there will be tips and tricks as well as a walkthrough on how to share your work using the Open Science Framework. Bringing your laptop is highly encouraged.\n\nWe will also discuss what to do (and what you do not need to do) when reviewing a paper with open materials. You will hopefully walk away with an improved ability to make your own research more empirically replicable and computationally reproducible. These skills can enable you to have a greater impact on the field by facilitating reuse and further development of your ideas by both other researchers and those who wish to apply your work.",
        "event_url": "",
        "sessions": [
            {
                "title": "How to Make Your Empirical Research Transparent",
                "session_id": "t-empiricaltransparent",
                "chair": [
                    "Xiaoying Pu",
                    "Steve Haroz"
                ],
                "organizers": [
                    "Steve Haroz",
                    "Xiaoying Pu"
                ],
                "display_start": "2020-10-25T18:00:00Z",
                "time_start": "2020-10-25T18:00:00Z",
                "time_end": "2020-10-25T21:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "https://us02web.zoom.us/j/86966045120?pwd=ay9iOXpNYWVOcDA1V09uYmg4SnB3Zz09",
                "zoom_password": "KTqnhNK2",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Zoom Only",
                        "title": "Introduction",
                        "contributors": [
                            "Steve Haroz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T18:00:00Z",
                        "time_end": "2020-10-25T18:30:00Z"
                    },
                    {
                        "type": "Zoom Only",
                        "title": "Preregistration",
                        "contributors": [
                            "Steve Haroz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T18:30:00Z",
                        "time_end": "2020-10-25T19:00:00Z"
                    },
                    {
                        "type": "Zoom Only",
                        "title": "Replicable Data Collection",
                        "contributors": [
                            "Steve Haroz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T19:00:00Z",
                        "time_end": "2020-10-25T19:30:00Z"
                    },
                    {
                        "type": "Zoom Only",
                        "title": "Storing Data",
                        "contributors": [
                            "Steve Haroz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:00:00Z",
                        "time_end": "2020-10-25T20:30:00Z"
                    },
                    {
                        "type": "Zoom Only",
                        "title": "Reproducible Computation and Analyses",
                        "contributors": [
                            "Steve Haroz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:30:00Z",
                        "time_end": "2020-10-25T21:00:00Z"
                    },
                    {
                        "type": "Zoom Only",
                        "title": "Finalizing Your project",
                        "contributors": [
                            "Steve Haroz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T21:00:00Z",
                        "time_end": "2020-10-25T21:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-trex": {
        "event": "TREX: Workshop on TRust and EXpertise in Visual Analytics",
        "long_name": "TREX: Workshop on TRust and EXpertise in Visual Analytics",
        "event_type": "Workshop",
        "event_description": "Visual analytics (VA) systems combine computational support and human cognitive and perceptual skills to explore and analyze data. Many of these systems have been incorporating machine learning (ML) models and algorithms to introduce some level of automation to the analytical process. However, within this relationship, there are a number of aspects that can impact the effectiveness of the human-machine teaming, including: 1) People\u2019s domain and system expertise; 2) Human biases, including cognitive and perceptual biases; 3) Trust in ML models and visual representation of data.\n\nExpertise, bias, and trust are intrinsically intertwined. Additionally, visual analytics systems are used in different fields and by people from various backgrounds, with different levels of domain expertise and experience with machine learning and visual analytics tools. This variety of experience and domain expertise among human users has opened the door for new research directions and challenges in the fields of visual analytics and machine learning. Designers who fail to consider the aforementioned diversities might introduce problems to the analysis effectiveness and user experience. Furthermore, experience and domain expertise might affect user trust in visual analytics tools; although, how and why they affect trust is still an open question. Trust will eventually affect how much the users would rely on and use the tool. While users will take advantage of their prior experiences to make better decisions with the assistance of analytic support, they might carry many cognitive biases that can negatively influence their decision-making or analysis process. Recent research shows trust in and reliance on the visual analytics systems/tools as well as user strategies and biases can be directly influenced by domain and system expertise (or lack of expertise). The goal of this workshop is to bring together researchers and practitioners from different disciplines to discuss and discover challenges in ML supported visual analytics tools and set the stage for future research directions and collaborations regarding these issues by proposing design guidelines, empirical findings, and VA techniques.",
        "event_url": "https://trexvis.github.io/Workshop2020/index.html",
        "sessions": [
            {
                "title": "TREX: Workshop on TRust and EXpertise in Visual Analytics",
                "session_id": "w-trex",
                "chair": [
                    "Emily Wall",
                    "Mahsan Nourani"
                ],
                "organizers": [
                    "Eric Ragan",
                    "Mahsan Nourani",
                    "Emily Wall",
                    "John Goodall",
                    "Aritra Dasgupta",
                    "Kristin Cook"
                ],
                "display_start": "2020-10-25T18:00:00Z",
                "time_start": "2020-10-25T18:00:00Z",
                "time_end": "2020-10-25T21:30:00Z",
                "discord_category": "trex workshop on trust and expertise in visual analytics",
                "discord_channel": "general",
                "discord_channel_id": "767932266508582942",
                "youtube_url": "https://youtu.be/LTGzCvhv05M",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Welcome",
                        "contributors": [
                            "Eric Ragan",
                            "Mahsan Nourani",
                            "Emily Wall",
                            "John Goodall",
                            "Aritra Dasgupta",
                            "Kristin Cook"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T18:00:00Z",
                        "time_end": "2020-10-25T18:10:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Your Totalitarian Brain",
                        "contributors": [
                            "Steven Franconeri"
                        ],
                        "abstract": "While a computer's state can be quickly rewritten, brains are relatively inflexible in the short term. As a totalitarian regime bends facts to fit ideology, the brain's inflexible structure can force evidence to bend to fit existing theories, across individuals and groups (Greenwald, 1980). I'll illustrate how this relatively fixed architecture contributes to perceptual and cognitive biases that affect how we process, remember, and reason with visualized data.",
                        "time_start": "2020-10-25T18:10:00Z",
                        "time_end": "2020-10-25T18:35:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Measure Utility, Gain Trust: Practical Advice for XAI Researchers",
                        "contributors": [
                            "Dustin Arendt"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T18:35:00Z",
                        "time_end": "2020-10-25T18:46:00Z",
                        "uid": "w-trex-7380"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Beyond Trust Building - Calibrating Trust in Visual Analytics ",
                        "contributors": [
                            "Wenkai Han"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T18:46:00Z",
                        "time_end": "2020-10-25T18:57:00Z",
                        "uid": "w-trex-6084"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Towards Visual Analytics for AI Transparency and Trust in Healthcare",
                        "contributors": [
                            "Bum Chul Kwon (\u201cBC\u201d)"
                        ],
                        "abstract": "Artificial intelligence (AI) techniques provide great opportunities for improving healthcare research and clinical practice. With the recent advancements along with ever-increasing clinical data, researchers have demonstrated successful application of AI techniques in predicting diagnosis, unexpected readmission, and mortality of patients. However, there has been limited adoption of the techniques for clinical use because of their black-box nature. Despite growing research in explainable AI methods, healthcare professionals may still find difficulties in understanding and using the techniques without visual aids. Visual analytics can help clinical experts to gain transparency and trust in using AI techniques for analyzing healthcare data. This talk aims to provide a brief overview of the stated problems and to describe the potential role of visual analytics in healthcare research and clinical practice by discussing previous research.",
                        "time_start": "2020-10-25T18:57:00Z",
                        "time_end": "2020-10-25T19:17:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Discussion",
                        "contributors": [
                            "Eric Ragan",
                            "Mahsan Nourani",
                            "Emily Wall",
                            "John Goodall",
                            "Aritra Dasgupta",
                            "Kristin Cook"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T19:17:00Z",
                        "time_end": "2020-10-25T19:30:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Welcome Back",
                        "contributors": [
                            "TREX Chairs"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:00:00Z",
                        "time_end": "2020-10-25T20:05:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Intelligibility Throughout the Machine Learning Life Cycle",
                        "contributors": [
                            "Jenn Wortman Vaughan"
                        ],
                        "abstract": "People play a central role in the machine learning life cycle. Consequently, building machine learning systems that are reliable, trustworthy, and fair requires that relevant stakeholders\u2014including developers, users, and the people affected by these systems\u2014have at least a basic understanding of how they work. Yet what makes a system \u201cintelligible\u201d is difficult to pin down. Intelligibility is a fundamentally human-centered concept that lacks a one-size-fits-all solution. I will explore the importance of evaluating methods for achieving intelligibility in context with relevant stakeholders, ways of empirically testing whether intelligibility techniques achieve their goals, and why we should expand our concept of intelligibility beyond machine learning models to other aspects of machine learning systems, such as datasets and performance metrics.",
                        "time_start": "2020-10-25T20:05:00Z",
                        "time_end": "2020-10-25T20:30:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Towards Trust-Augmented Visual Analytics for Data-Driven Energy Modeling",
                        "contributors": [
                            "Aritra Dasgupta"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:30:00Z",
                        "time_end": "2020-10-25T20:41:00Z",
                        "uid": "w-trex-1260"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "A Comparative Analysis of Industry Human-AI Interaction Guidelines",
                        "contributors": [
                            "Austin Wright"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:41:00Z",
                        "time_end": "2020-10-25T20:52:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Toward a Bias-Aware Future for Mixed-Initiative Visual Analytics",
                        "contributors": [
                            "Adam Coscia"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:52:00Z",
                        "time_end": "2020-10-25T21:03:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Discussion",
                        "contributors": [
                            "Eric Ragan",
                            "Mahsan Nourani",
                            "Emily Wall",
                            "John Goodall",
                            "Aritra Dasgupta",
                            "Kristin Cook"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T21:03:00Z",
                        "time_end": "2020-10-25T21:30:00Z"
                    }
                ]
            }
        ]
    },
    "t-theorycolortools": {
        "event": "Theory and Application of Visualization Color Tools and Strategies",
        "long_name": "Theory and Application of Visualization Color Tools and Strategies",
        "event_type": "Tutorial",
        "event_description": "In this tutorial, we will discuss the theory and usage of color encoding design for visualization, including state-of-the-art strategies and tools. Several new color palette and colormap construction tools such as Colorgorical, Color Crafter, ColorMoves, CCCtool have recently been released. This tutorial is designed to help participants understand their features, strengths and constraints in order to enable the selection of the tool that best aligns with their needs. We will first discuss principles, decisions, and mechanisms for designing effective color encodings and then explore these tools in hands-on sessions, creating color palettes and colormaps, and testing them on data. We will close with a discussion of how future tools may aid in open challenges for using color in visualization.",
        "event_url": "",
        "sessions": [
            {
                "title": "Theory and Application of Visualization Color Tools and Strategies",
                "session_id": "t-theorycolortools",
                "chair": [
                    "Stephanie Zeller",
                    "Francesca Samsel",
                    "Danielle Szafir",
                    "Karen Schloss",
                    "Pascal Nardini"
                ],
                "organizers": [
                    "Danielle Szafir",
                    "Francesca Samsel",
                    "Karen Schloss"
                ],
                "display_start": "2020-10-25T18:00:00Z",
                "time_start": "2020-10-25T18:00:00Z",
                "time_end": "2020-10-25T21:30:00Z",
                "discord_category": "theory and application of visualization color tools and strategies",
                "discord_channel": "general",
                "discord_channel_id": "767932274380505100",
                "youtube_url": "https://youtu.be/M_eA66NOSik",
                "zoom_meeting": "https://us02web.zoom.us/j/85775934118?pwd=VE9Pa2FSL0l0WVZpZHJjZUZlTGxmUT09",
                "zoom_password": "XNNJOLkE",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Session Introduction",
                        "contributors": [
                            "Francesca Samsel"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T18:00:00Z",
                        "time_end": "2020-10-25T18:10:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Color Discriminability",
                        "contributors": [
                            "Danielle Szafir"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T18:10:00Z",
                        "time_end": "2020-10-25T18:35:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Color Cognition",
                        "contributors": [
                            "Karen Schloss"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T18:35:00Z",
                        "time_end": "2020-10-25T19:00:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Color Interaction",
                        "contributors": [
                            "Francesca Samsel"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T19:00:00Z",
                        "time_end": "2020-10-25T19:25:00Z"
                    },
                    {
                        "type": "Live Presentation, attendees on Zoom",
                        "title": "Color Crafter",
                        "contributors": [
                            "Danielle Szafir"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:00:00Z",
                        "time_end": "2020-10-25T20:10:00Z"
                    },
                    {
                        "type": "Live Presentation, attendees on Zoom",
                        "title": "Cologorical",
                        "contributors": [
                            "Karen Schloss"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:10:00Z",
                        "time_end": "2020-10-25T20:20:00Z"
                    },
                    {
                        "type": "Zoom Breakout",
                        "title": "Color Crafter and Colorgorical Breakout Rooms",
                        "contributors": [
                            "Danielle Szafir",
                            "Karen Schloss"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:20:00Z",
                        "time_end": "2020-10-25T20:45:00Z"
                    },
                    {
                        "type": "Live Presentation, attendees on Zoom",
                        "title": "ColorMoves",
                        "contributors": [
                            "Francesca Samsel"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:45:00Z",
                        "time_end": "2020-10-25T20:55:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "CCCTool",
                        "contributors": [
                            "Pascal Nardini",
                            "Stephanie Zeller"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:55:00Z",
                        "time_end": "2020-10-25T21:04:00Z"
                    },
                    {
                        "type": "Zoom Breakout",
                        "title": "ColorMoves and CCCTool Breakout Rooms",
                        "contributors": [
                            "Francesca Samsel",
                            "Pascal Nardini",
                            "Stephanie Zeller"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T21:05:00Z",
                        "time_end": "2020-10-25T21:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-comm": {
        "event": "VisComm: Workshop on Visualization for Communication",
        "long_name": "VisComm: Workshop on Visualization for Communication",
        "event_type": "Workshop",
        "event_description": "Our proposed half-day workshop will bring together communicative visualization practitioners with researchers from several fields to address the questions raised by the rapidly growing communicative uses of visualization (e.g. in-news graphics, information graphics). These questions span issues of audience, application, evaluation, understanding and practice. To encourage participation from communities that do not typically attend IEEE Visualization and write academic papers, we will not only accept short papers but also visual case studies, and recruit program committee members from those communities. We have organized this workshop before with great success: at VIS 2018 with around 70 participants and nine papers and posters, and at VIS 2019 with around 60 participants (the room was over capacity, with people turned away) and 13 papers and posters.",
        "event_url": "https://viscomm.io/",
        "sessions": [
            {
                "title": "VisComm: Workshop on Visualization for Communication",
                "session_id": "w-comm",
                "chair": [
                    "Robert Kosara",
                    "Ben Watson"
                ],
                "organizers": [
                    "Alvitta Ottley",
                    "Adriana Arcia",
                    "Ben Watson",
                    "Robert Kosara"
                ],
                "display_start": "2020-10-25T18:00:00Z",
                "time_start": "2020-10-25T18:00:00Z",
                "time_end": "2020-10-25T21:30:00Z",
                "discord_category": "viscomm workshop on visualization for communication",
                "discord_channel": "general",
                "discord_channel_id": "767932280911822869",
                "youtube_url": "https://youtu.be/eDBc4Gwe5x4",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Welcome & Introduction",
                        "contributors": [
                            "Alvitta Ottley",
                            "Adriana Arcia",
                            "Ben Watson",
                            "Robert Kosara"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T18:00:00Z",
                        "time_end": "2020-10-25T18:10:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Mapping the Landscape of COVID-19 Crisis Visualizations",
                        "contributors": [
                            "Yixuan Zhang"
                        ],
                        "abstract": "A great number of visualizations have been created to communicate the constantly changing crisis of the COVID-19 pandemic. With the prevalence of these crisis visualizations, there is a critical need to organize and understand what and how visualizations have been produced and disseminated to the public, as information consumption can impact peoples' attitudes, responses to crisis and risk, behaviors, and thus ultimately the trajectory of the pandemic. We curated a list of 668 visualizations that communicate information about the pandemic. We performed a content analysis of these visualizations and derived six categories of intended messages in communication about the pandemic, including informing about severity; forecasting trends and influences; explaining the course of the disease; mirroring impact of the crisis; and communicating risk, vulnerability, and equity. We also identify issues and opportunities arising from COVID-19 crisis visualizations.",
                        "time_start": "2020-10-25T18:10:00Z",
                        "time_end": "2020-10-25T18:22:00Z",
                        "uid": "w-comm-1015"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "COVID-19 Health Equity Dashboard - Addressing Vulnerable Populations",
                        "contributors": [
                            "Star Liu"
                        ],
                        "abstract": "We present a case study of the COVID-19 Health Equity Dashboard, an open-source web-based interactive data visualization, that provides timely, localized, and actionable data of the ongoing COVID-19 pandemic. The dashboard features interactive maps and charts alongside population vulnerability characteristics, allowing for benchmarking county-level outcomes and disparities against the state and nation. While the dashboard faces several public health communication challenges, we continue to investigate and support data dissemination for public health officials' decision making.",
                        "time_start": "2020-10-25T18:22:00Z",
                        "time_end": "2020-10-25T18:32:00Z",
                        "uid": "w-comm-1013"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Eating with a Conscience: Toward a Visual and Contextual Nutrition Facts Label",
                        "contributors": [
                            "Darius Coelho"
                        ],
                        "abstract": "The large variety of food products available in today's market is making it increasingly difficult for the diet-conscious consumer to select the appropriate foods to maintain a balanced diet. To assist consumers, we design a visual nutrition facts label that provides the viewer with a quick overview of a food's nutritional content while presenting them with contextual information that compares it with similar foods. Our core design is based on the bullet graph as it is capable of displaying the level of a nutrient in a food product along with the range of nutrient levels in other similar foods thus providing a context. In our work, we create multiple design variations of our visual nutrition labels based on the bullet graph and test which design is the most preferable and effective.",
                        "time_start": "2020-10-25T18:32:00Z",
                        "time_end": "2020-10-25T18:44:00Z",
                        "uid": "w-comm-1011"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Making Numbers Meaningful \u2013 improving how we communicate numbers to patients and the public",
                        "contributors": [
                            "Natalie Benda"
                        ],
                        "abstract": "Numbers are a crucial piece of visual communication of health-related information. There are currently multiple resources for communicating health-related information in plain language, but an equivalent resource for numeric information is lacking. We are creating a web-based resource for providing guidance on how to visually communicate numbers related to health. Our resource was developed through a large systematic literature review under the advisement of a national expert panel on health numeracy. User testing is currently under way. This resource may be utilized by various end users including: health writers, health application developers, public health professionals, and journalists.",
                        "time_start": "2020-10-25T18:44:00Z",
                        "time_end": "2020-10-25T18:56:00Z",
                        "uid": "w-comm-1009"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Customized visualizations for audience-oriented communication in COVID-19 tracking story",
                        "contributors": [
                            "Liuhuaying Yang"
                        ],
                        "abstract": "This visual case study was conducted by zaobao.sg, the digital platform of the Chinese dailies in Singapore. In early February, to inform and communicate with local audiences about the COVID-19 situation in Singapore, we built an interactive webpage titled \u201cDynamic tracking of COVID-19 in Singapore\u201d, to visualize the daily situation reports released by the Ministry of Health. As the situation evolved, we kept updating the data every day until June 2nd, when Singapore ended its Circuit Breaker measures. The aid of the visualisation is to help better picture the relationship between cases within a cluster, how these clusters are inter-related and activities-based, and how cases are disseminated to various hospitals.",
                        "time_start": "2020-10-25T18:56:00Z",
                        "time_end": "2020-10-25T19:06:00Z",
                        "uid": "w-comm-1005"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Does Interaction Improve Bayesian Reasoning with Visualization?",
                        "contributors": [
                            "Ab Mosca"
                        ],
                        "abstract": "Interaction enables users to effectively navigate large amounts of data, supports cognitive processing, and increases methods of data representation. However, beyond popular beliefs, there have been few attempts to empirically demonstrate whether adding interaction to a static visualization improves its function. In this paper, we address this gap. We use a classic Bayesian reasoning task as a test bed for evaluating whether allowing users to interact with a static visualization can improve their reasoning. Through a crowdsourced study, we show that adding interaction to a static Bayesian reasoning visualization does not necessarily improve users' accuracy on a Bayesian reasoning task, and in some cases can significantly detract from it. Moreover, we demonstrate that changes in performance are modulated by the design of the underlying visualization, and users' spatial ability. Our work suggests that interaction is not as unambiguously good as we often believe.",
                        "time_start": "2020-10-25T19:06:00Z",
                        "time_end": "2020-10-25T19:18:00Z",
                        "uid": "w-comm-1018"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Flex Time & Brief Session Close",
                        "contributors": [
                            "Adriana Arcia"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T19:18:00Z",
                        "time_end": "2020-10-25T19:30:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Variable Biases: A Study of Scientists' Interpretation of Plot Types Commonly Used in Scientific Communication",
                        "contributors": [
                            "Laura Matzen"
                        ],
                        "abstract": "In scientific communication, there are visualization conventions that are widely used to convey uncertainty, such as representing the variability of a dataset with error bars. Yet prior research indicates that scientists frequently misinterpret error bars. In this study, we compared bar charts with error bars to four alternative visualizations: dot, box, violin, and density plots. Our goal was to determine whether these other plot types would produce fewer biases in interpretation relative to bar plots. Scientists who have experience generating and interpreting statistical graphs used plots to assess whether the difference between two datasets was statistically significant. Our results replicated the patterns of biases that have been observed in prior studies of error bar interpretation. However, we found that our participants still had the best overall performance for bar plots with error bars, because they were most familiar with this type of plot.",
                        "time_start": "2020-10-25T20:00:00Z",
                        "time_end": "2020-10-25T20:12:00Z",
                        "uid": "w-comm-1017"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "How do taxes, benefits and public spending evolve for a taxpayer during their lifetime?",
                        "contributors": [
                            "Thomas Hurtut"
                        ],
                        "abstract": "We present a visual case study conducted by the Research Chair in Taxation and Public Finance from Sherbrooke University, visually designed and developed in collaboration with Polytechnique Montreal. How do taxes, benefits and public spending evolve for a taxpayer during their lifetime? This project explores a narrative format to make a possible answer accessible to a general audience. We showed in a previous study that three different average life stories, depending on the life events that can affect an household (studies, changes in the size and composition of the household etc.), are reasonable ways to give representative and accessible answers to the title question. Based on these three life stories, we investigated a data-driven visual story based on three parallel visualizations combined to a scrollytelling interaction.",
                        "time_start": "2020-10-25T20:12:00Z",
                        "time_end": "2020-10-25T20:22:00Z",
                        "uid": "w-comm-1012"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Applying Racial Equity Awareness in Data Visualization",
                        "contributors": [
                            "Jonathan Schwabish"
                        ],
                        "abstract": "A data visualization style guide does for graphs what the Chicago Manual of Style does for English grammar: it defines the components of a graph and their proper, consistent use. At the Urban Institute, a nonprofit research institution based in Washington, DC, our data visualization style guide defines these styles for our research and communications staff. In the process of revising and expanding the style guide, we are taking a more diverse, equitable, and inclusive (DEI) perspective to our research, data, and visualizations. To date, our approach has been to create a set of recommendations and issues to consider, rather than a set of rules that researchers must follow. In this paper, we discuss eight techniques that data visualization producers should consider when creating visuals with this DEI approach. As with our existing style guide, we consider this a first step in our thought process of creating graphs and producing content through a DEI lens.",
                        "time_start": "2020-10-25T20:22:00Z",
                        "time_end": "2020-10-25T20:34:00Z",
                        "uid": "w-comm-1021"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Tile Narrative: Scrollytelling with Grid Maps",
                        "contributors": [
                            "Pratap Vardhan"
                        ],
                        "abstract": "This visual case study was built to study the effectiveness of scrollytelling in tandem with tile grid maps. Specifically, we built this interactive based on Google's Community Mobility data measuring people's movement before and during the COVID-19 pandemic. We designed a dynamic and interactive format to display auto-generated narratives synchronized with the context of the grid map. We believe the use of narratives in grid map context will be a useful storytelling format. This paper describes the design decisions that went into building this interactive format.",
                        "time_start": "2020-10-25T20:34:00Z",
                        "time_end": "2020-10-25T20:44:00Z",
                        "uid": "w-comm-1022"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Position: Visual Sentences: Definitions and Applications",
                        "contributors": [
                            "Mark Livingston"
                        ],
                        "abstract": "Visual forms of communication are ubiquitous in media, educational texts, government reports, and scientific publications. Two challenges immediately come to mind and drive the research agenda we have been following. We need to know that these graphs are provably understandable. We also need to know that the personnel have the skills to understand the information being presented when it is presented well. In this paper, we assert that the right way to achieve both goals relies on perceiving what are the visual equivalents of sentences from graphs.",
                        "time_start": "2020-10-25T20:44:00Z",
                        "time_end": "2020-10-25T20:56:00Z",
                        "uid": "w-comm-1029"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Analyzing Dorian Twitter data to understand how hurricane risk communication changes as threats unfold",
                        "contributors": [
                            "Qian Ma"
                        ],
                        "abstract": "This research explores diffusion of forecast messages on social media for Hurricane Dorian (2019) to understand how hurricane risk information is created, disseminated, and discussed in many-to-many communication modes. We collected streams of posts from professional Twitter accounts, conducted a descriptive statistical analysis of manually-coded tweets to identify hurricane risk message types, performed a network analysis to audit engagement among different types of accounts, and calculated sentiment scores to examine message opinions. The results showed that professional accounts are more likely to post original content than share, among which most of the organizational accounts post in regular intervals with a fixed message format and a neutral opinion while professional individual accounts post with no observable temporal regulation, variable forms, and sentiments. However, the forecaster accounts are less engaged than the mediator accounts.",
                        "time_start": "2020-10-25T20:56:00Z",
                        "time_end": "2020-10-25T21:06:00Z",
                        "uid": "w-comm-1028"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Beautiful Visualizations Slain by Ugly Facts: Redesigning the National Hurricane Center's 'Cone of Uncertainty' Map",
                        "contributors": [
                            "Barbara Millet"
                        ],
                        "abstract": "The Track Forecast Cone, commonly known as the \u201ccone of uncertainty\u201d, is the most popular hurricane and tropical storm forecast product that the National Hurricane Center produces. However, it is often misinterpreted by non-experts. In this study we first explored the most common misconceptions about the cone and produced two alternative redesigns that we expected to be more attractive to and easier to understand by non-expert readers. Our results were mixed, but reveal promising paths for future efforts.",
                        "time_start": "2020-10-25T21:06:00Z",
                        "time_end": "2020-10-25T21:18:00Z",
                        "uid": "w-comm-1023"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Flex Time & Brief Workshop Close",
                        "contributors": [
                            "Adriana Arcia"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T21:18:00Z",
                        "time_end": "2020-10-25T21:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-failfest": {
        "event": "Fail Fest: A Workshop Celebrating the Scientific Value of Failure",
        "long_name": "Fail Fest: A Workshop Celebrating the Scientific Value of Failure",
        "event_type": "Workshop",
        "event_description": "Failure is a crucial part of the scientific process, but it is rarely given the same respect as success. FailFest, a half day workshop at IEEE VIS 2020, is a chance for the visualization community to share personal research failures and celebrate the important knowledge gained through these experiences. The workshop will cumulate strategies for building a VIS community that acknowledges failure and embraces its scientific value.",
        "event_url": "https://failfest.github.io/",
        "sessions": [
            {
                "title": "Fail Fest: A Workshop Celebrating the Scientific Value of Failure",
                "session_id": "w-failfest",
                "chair": [
                    "Lonni Besan\u00e7on",
                    "Jordan Crouser",
                    "Paul Rosen",
                    "Jane Adams"
                ],
                "organizers": [
                    "Jane L. Adams",
                    "R. Jordan Crouser",
                    "Lonni Besan\u00e7on",
                    "Paul Rosen"
                ],
                "display_start": "2020-10-25T18:00:00Z",
                "time_start": "2020-10-25T18:00:00Z",
                "time_end": "2020-10-25T21:30:00Z",
                "discord_category": "fail fest a workshop celebrating the scientific value of failure",
                "discord_channel": "general",
                "discord_channel_id": "767932288653590549",
                "youtube_url": "https://youtu.be/Aunzhk634bU",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Opening Remarks",
                        "contributors": [
                            "Jane Adams",
                            "Jordan Crouser",
                            "Lonni Besan\u00e7on",
                            "Paul Rosen"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T18:00:00Z",
                        "time_end": "2020-10-25T18:10:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Workshop Organizer Lightning Talks",
                        "contributors": [
                            "Jane Adams",
                            "Jordan Crouser",
                            "Lonni Besan\u00e7on",
                            "Paul Rosen"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T18:10:00Z",
                        "time_end": "2020-10-25T18:30:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Walkthrough of VIS Review Process Template",
                        "contributors": [
                            "Leilani Battle"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T18:30:00Z",
                        "time_end": "2020-10-25T18:40:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Panel Discussion",
                        "contributors": [
                            "Kris Cook",
                            "Alark Joshi",
                            "Alberto Cairo",
                            "Stephanie Evergreen"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T18:40:00Z",
                        "time_end": "2020-10-25T19:20:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Panel Q&A",
                        "contributors": [
                            "Kris Cook",
                            "Alark Joshi",
                            "Alberto Cairo",
                            "Stephanie Evergreen"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T19:20:00Z",
                        "time_end": "2020-10-25T19:30:00Z"
                    },
                    {
                        "type": "Recorded Presentation",
                        "title": "Expectation Versus Reality: The Failed Evaluation of a Mixed-Initiative Visualization System",
                        "contributors": [
                            "Sunwoo Ha"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:00:00Z",
                        "time_end": "2020-10-25T20:15:00Z",
                        "uid": "w-failfest-5552"
                    },
                    {
                        "type": "Recorded Presentation",
                        "title": "How Visualization PhD Students Cope with Paper Rejections",
                        "contributors": [
                            "Shivam Agrawal"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:15:00Z",
                        "time_end": "2020-10-25T20:30:00Z",
                        "uid": "w-failfest-9490"
                    },
                    {
                        "type": "Recorded Presentation",
                        "title": "Visualization Iteration in Practice",
                        "contributors": [
                            "Nadieh Bremer"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T20:30:00Z",
                        "time_end": "2020-10-25T21:15:00Z"
                    },
                    {
                        "type": "Live Zoom",
                        "title": "Next Steps and Closing Remarks",
                        "contributors": [
                            "Paul Rosen"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-25T21:15:00Z",
                        "time_end": "2020-10-25T21:30:00Z"
                    }
                ]
            }
        ]
    },
    "a-vds": {
        "event": "VDS",
        "long_name": "Visualization in Data Science",
        "event_type": "Symposium",
        "event_description": "Transformations in many fields are enabled by rapid advances in our ability to acquire and generate data. The bottleneck to discovery is now our ability to analyze and make sense of heterogeneous, noisy, streaming, and often massive datasets. Extracting knowledge or insights from this abundance of data lies at the heart of 21st century discovery, which can be used to inform decisions, coordinate activities, optimize processes, improve products and services, as well as enhance productivity and innovation across a wide range of business and scientific problems.\n\nData science is the practice of deriving insights from data, enabled by statistical modeling, computational methods, interactive visual analysis, and domain-driven problem solving. Data science draws from methodology developed in such fields as applied mathematics, statistics, machine learning, data management, visualization, and HCI. It drives discoveries in business, economy, biology, medicine, environmental science, the physical sciences, the humanities and social sciences, and beyond.\n\nVisualization is an integral part of data science, and essential to enable sophisticated analysis of data. After four highly successful events, the sixth Symposium on Visualization in Data Science (VDS) will again be held at IEEE VIS 2020 in Salt Lake City, Utah. VDS will bring together domain scientists and methods researchers (including visualization, usability and HCI, data management, statistics, machine learning, and software engineering) to discuss common interests, talk about practical issues, and identify open research problems in visualization in data science.",
        "event_url": "http://www.visualdatascience.org/2020/index.html",
        "sessions": [
            {
                "title": "VDS",
                "session_id": "a-vds",
                "chair": [
                    "Adam Perer",
                    "Liang Gou",
                    "Hendrik Strobelt",
                    "Alvitta Ottley"
                ],
                "organizers": [
                    "Adam Perer",
                    "Hendrick Strobelt"
                ],
                "display_start": "2020-10-26T14:00:00Z",
                "time_start": "2020-10-26T14:00:00Z",
                "time_end": "2020-10-26T17:30:00Z",
                "discord_category": "vds",
                "discord_channel": "general",
                "discord_channel_id": "768305294396096573",
                "youtube_url": "https://youtu.be/q4TRSJk_ynQ",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Keynote: Interpretability and Human Validation of Machine Learning",
                        "contributors": [
                            "Finale Doshi-Velez"
                        ],
                        "abstract": "As machine learning systems become ubiquitous, there is a growing interest in interpretable machine learning -- that is, systems that can provide human-interpretable rationale for their predictions and decisions.  In this talk, I'll first give examples of why interpretability is needed in some of our work in machine learning for health, discussing how human input (which would be impossible without interpretability) is crucial for getting past fundamental limits of statistical validation.  Next, I'll speak about some of the work we are doing to understand interpretability more broadly: what exactly is interpretability, and how can we assess it?  By formalizing these notions, we can hope to identify universals of interpretability and also rigorously compare different kinds of systems for producing algorithmic explanations.\n\nIncludes joint work with Been Kim, Andrew Ross, Mike Wu, Michael Hughes, Menaka Narayanan, Sam Gershman, Emily Chen, Jeffrey He, Isaac Lage, Roy Perlis, Tom McCoy, Gabe Hope, Leah Weiner, Erik Sudderth, Sonali Parbhoo, Marzyeh Ghassemi, Pete Szolovits, Mornin Feng, Leo Celi, Nicole Brimmer, Tristan Naumann, Rohit Joshi, Anna Rumshisky, Omer Gottesman, Emma Brunskill, Yao Liu, Sonali Parbhoo, Joe Futoma, and the Berkman Klein Center.",
                        "time_start": "2020-10-26T14:00:00Z",
                        "time_end": "2020-10-26T14:45:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Passing the Data Baton: A Retrospective Analysis on Data Science Work and Workers",
                        "contributors": [
                            "Anamaria Crisan"
                        ],
                        "abstract": "Data science is a rapidly growing discipline and organizations increasingly depend on data science work. Yet the ambiguity around data science, what it is, and who data scientists are can make it difficult for visualization researchers to identify impactful research trajectories. We have conducted a retrospective analysis of data science work and workers as described within the data visualization, human computer interaction, and data science literature. From this analysis we synthesis a comprehensive model that describes data science work and breakdown to data scientists into nine distinct roles. We summarise and reflect on the role that visualization has throughout data science work and the varied needs of data scientists themselves for tooling support. Our findings are intended to arm visualization researchers with a more concrete framing of data science with the hope that it will help them surface innovative opportunities for impacting data science work.",
                        "time_start": "2020-10-26T14:45:00Z",
                        "time_end": "2020-10-26T15:00:00Z",
                        "uid": "a-vds-1002"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "LEGION: Visually compare modeling techniques for regression",
                        "contributors": [
                            "Subhajit Das"
                        ],
                        "abstract": "People construct machine learning (ML) models for various use cases such as in healthcare, financial modeling, etc. In doing so, they aim to improve a models' performance by adopting various strategies, such as changing input data, tuning model hyperparameters, performing feature engineering etc. However, how would users know which of these model construction strategies to adopt for their problem? This paper aims to solve the problem of how to construct models and how to select a modeling strategy by allowing users to compare incoherencies between multiple regression models (constructed by two different modeling strategies) and then learn not only about the model but also about their data. We present LEGION, a visual analytic tool that helps users to compare and select regression models constructed either by tuning their hyperparameters or by feature engineering methods. We also present two use cases on real world datasets validating the utility and effectiveness of our tool.",
                        "time_start": "2020-10-26T15:00:00Z",
                        "time_end": "2020-10-26T15:15:00Z",
                        "uid": "a-vds-1010"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "VIMA: Modeling and Visualization of High Dimensional Machine Sensor Data Leveraging Multiple Sources of Domain Knowledge",
                        "contributors": [
                            "Joscha Eirich"
                        ],
                        "abstract": "The highly integrated design of the electrified power train creates new challenges in the holistic testing of high-quality standards. Test technicians face the challenge that tests for such new technologies are just about to be developed. Thus, they cannot rely on their gut feeling, but require automated support, which is not yet available. We present VIMA, a system that processes high dimensional machine-sensor data to support test technicians with their analyses of produced parts and to interactively create labels. We demonstrate the usefulness of VIMA in a qualitative user study with four test technicians. The results indicate that VIMA helps to identify abnormal parts, that were not detected by the established testing procedures. Additionally, we use the labels, generated interactively through VIMA, to deploy a model running on a test station in a real manufacturing environment; the model outperforms the current testing procedure in detecting increased backlashes of electrical engines.",
                        "time_start": "2020-10-26T15:15:00Z",
                        "time_end": "2020-10-26T15:30:00Z",
                        "uid": "a-vds-1006"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Keynote: Why Interactive Analysis Needs Theories of Inference",
                        "contributors": [
                            "Jessica Hullman"
                        ],
                        "abstract": "Data analysis is a decidedly human task. As Tukey and Wilk once wrote, \u201cNothing\u2014not the careful logic of mathematics, not statistical models and theories, not the awesome arithmetic power of modern computers\u2014nothing can substitute here for the flexibility of the informed human mind.\u201d Research in supporting interactive and exploratory analysis has produced a number of sophisticated interfaces, many of which are optimized for easy pattern finding and data \"exposure.\" However, visualization tools are often used by analysts and others to make inferences beyond the data, and as my own and others' research has shown, these inferences often deviate from the predictions of statistical inference. I'll describe how an absence of theories of inference that ground our understanding of how to design for interactive analysis may threaten the validity of conclusions people draw from visualizations, and describe what we've learned by using theories of statistical inference to better understand and design for intuitive visual analysis.",
                        "time_start": "2020-10-26T16:00:00Z",
                        "time_end": "2020-10-26T16:45:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "dg2pix: Pixel-Based Visual Analysis of Dynamic Graphs",
                        "contributors": [
                            "Eren Cakmak"
                        ],
                        "abstract": "Presenting long sequences of dynamic graphs remains challenging due to the underlying large-scale and high-dimensional data. We propose dg2pix, a novel pixel-based visualization technique, to visually explore temporal and structural properties in long sequences of large-scale graphs. The approach consists of three main steps: (1) the multiscale modeling of the temporal dimension; (2) unsupervised graph embeddings to learn low-dimensional representations of the dynamic graph data; and (3) an interactive pixel-based visualization to explore the evolving data at different temporal aggregation scales simultaneously. dg2pix provides a scalable overview of a dynamic graph, supports the exploration of long sequences of high-dimensional graph data, and enables the identification and comparison of similar temporal states. We show the applicability of the technique to synthetic and real-world datasets, demonstrating that temporal patterns in dynamic graphs can be easily identified and interpreted over time. Our dg2pix contributes a suitable intermediate representation between node-link diagrams at the high detail end, and matrix representations on the low detail end.",
                        "time_start": "2020-10-26T16:45:00Z",
                        "time_end": "2020-10-26T17:00:00Z",
                        "uid": "a-vds-1001"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "ContiMap: Continuous Heatmap for Large Time Series Data",
                        "contributors": [
                            "Vung Pham"
                        ],
                        "abstract": "Limited human cognitive load, limited computing resources, and finite display resolutions are the major obstacles for developing interactive visualization systems in large-scale data analysis. Recent technological innovation has significantly improved computing power, such as faster CPUs and GPUs, as well as display resources, including ultra-high-resolution displays and video walls. However, large and complex data is still ahead in the run as we are generating huge amounts of data daily. Our strategy to bridge these gaps is to present the right amount of information through the use of compelling graphics. This paper proposes an approximation algorithm and a web prototype for representing a high-level abstraction of time series based on heatmap designs. Our approach aims to handle a significant amount of time series data arising from various application domains, such as cybersecurity, sensor network, and gene expression analysis.",
                        "time_start": "2020-10-26T17:00:00Z",
                        "time_end": "2020-10-26T17:15:00Z",
                        "uid": "a-vds-1011"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visualizing and Analyzing Disputed Areas in Soccer",
                        "contributors": [
                            "Jules Allegre"
                        ],
                        "abstract": "Space ownership models assign 2D areas to individuals, based on their ability to reach locations according to their direction and speed. In this paper, we investigate the case where two or more individuals can reach a given location simultaneously. We refer to those locations as disputed areas, as there is tension and uncertainty on ownership, which is an important spatial analysis tool, e. g., in sports where players share a space with adversaries. We present the process to calculate those disputed areas from existing space ownership models, and introduce several visualizations and analysis of those areas using sport tracking data from Liverpool 2019\u2019s goals. Those areas have been particularly insightful to understand assists, the ultimate pass that is critical for a team to score. We also report on feedback from experts both on the relevance of those areas as well as their visual design.",
                        "time_start": "2020-10-26T17:15:00Z",
                        "time_end": "2020-10-26T17:30:00Z",
                        "uid": "a-vds-1005"
                    }
                ]
            }
        ]
    },
    "a-vastchallenge": {
        "event": "VAST Challenge",
        "long_name": "IEEE Visual Analytics Science and Technology (VAST) Challenge",
        "event_type": "Associated Event",
        "event_description": "The goal of the annual IEEE Visual Analytics Science and Technology (VAST) Challenge is to advance the field of visual analytics through competition. The VAST Challenge is designed to help researchers understand how their software would be used in a variety of analytic tasks and encourage innovation in data transformations and interactive visualizations. VAST Challenge problems provide researchers with realistic tasks and data sets for evaluating their software.\n\nResearchers and software providers often use VAST Challange data sets as benchmarks to demonstrate and test the capabilities of their systems. The ground truth embedded in the data sets helps researchers evaluate and strengthen the utility of their visual analytic techniques\n\nVAST Challenges are open to participation by individuals and teams in industry, government, and academia. We encourage your submissions and look forward to seeing your innovative approaches to solving these challenges using visual analytics.",
        "event_url": "https://vast-challenge.github.io/2020/",
        "sessions": [
            {
                "title": "Session 1",
                "session_id": "a-vastchallenge-1",
                "chair": [
                    "Steve Gomez",
                    "Jordan Crouser"
                ],
                "organizers": [],
                "display_start": "2020-10-26T14:00:00Z",
                "time_start": "2020-10-26T14:00:00Z",
                "time_end": "2020-10-26T15:30:00Z",
                "discord_category": "vast challenge",
                "discord_channel": "session-1",
                "discord_channel_id": "768305302822715393",
                "youtube_url": "https://youtu.be/loPwVaxYSDk",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Intro, Agenda and Thanks",
                        "contributors": [
                            "Jordan Crouser"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:00:00Z",
                        "time_end": "2020-10-26T14:10:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Mini-Challenge 2 introduction - Object Detection and Visual Analytics",
                        "contributors": [
                            "Steve Gomez"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:10:00Z",
                        "time_end": "2020-10-26T14:25:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Uncovering the Missing Links with Interactive Object Detection - Tianjin University - Mini-Challenge 2 Award: Strong Visual Design to Support Classification and Task-Focused Filtering",
                        "contributors": [
                            "Shichao Jia"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:25:00Z",
                        "time_end": "2020-10-26T14:45:00Z",
                        "uid": "a-vastchallenge-1012"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "TotemFinder: A Visual Analytics Approach for Image-based Key Players Identification - Arizona State University - Mini-Challenge 2 Honorable Mention: Effective Use of Visual Encodings for Correcting Classification Errors",
                        "contributors": [
                            "Jinbin Huang"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:45:00Z",
                        "time_end": "2020-10-26T15:00:00Z",
                        "uid": "a-vastchallenge-1016"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "VisMCA: An Interactive Visual Analytics System for Misclassification Correction and Analysis - Texas Tech - Mini-Challenge 2 Honorable Mention: Detailed Analysis of Patterns of Misclassification",
                        "contributors": [
                            "Huyen Nguyen"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:00:00Z",
                        "time_end": "2020-10-26T15:15:00Z",
                        "uid": "a-vastchallenge-1042"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Posters - Introduction",
                        "contributors": [
                            "Steve Gomez"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:15:00Z",
                        "time_end": "2020-10-26T15:30:00Z"
                    }
                ]
            },
            {
                "title": "Session 2",
                "session_id": "a-vastchallenge-2",
                "chair": [
                    "Curtis Larimer"
                ],
                "organizers": [],
                "display_start": "2020-10-26T16:00:00Z",
                "time_start": "2020-10-26T16:00:00Z",
                "time_end": "2020-10-26T17:30:00Z",
                "discord_category": "vast challenge",
                "discord_channel": "session-2",
                "discord_channel_id": "768305354017734686",
                "youtube_url": "https://youtu.be/WkYWspDNpf0",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Mini-Challenge 1 Introduction - Visual Analytics for Subgraph Matching",
                        "contributors": [
                            "Curtis Larimer"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:00:00Z",
                        "time_end": "2020-10-26T16:15:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "GraphletMatchMaker: Visual Analytics Approaches to Graph Matching in Cybersecurity Communities - Mini-Challenge 1 Award: Outstanding Comprehensive Mini-Challenge 1 Solution",
                        "contributors": [
                            "Natkamon Tovanich",
                            "Alexis Pister",
                            "Ga\u00eblle Richer",
                            "Paola Valdivia"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:15:00Z",
                        "time_end": "2020-10-26T16:35:00Z",
                        "uid": "a-vastchallenge-1030"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "CA2: Cyber Attacks Analytics - Shandong University - Mini-Challenge 1 Honorable Mention: Streamlined Analysis Process",
                        "contributors": [
                            "Luyu Cheng"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:35:00Z",
                        "time_end": "2020-10-26T16:50:00Z",
                        "uid": "a-vastchallenge-1034"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Somewraps - Something with Graphs: Visual Analytics of Network Data - University of Konstanz - Mini-Challenge 1 Honorable Mention: Custom Tool Supporting Visual Comparison",
                        "contributors": [
                            "David Englert"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:50:00Z",
                        "time_end": "2020-10-26T17:05:00Z",
                        "uid": "a-vastchallenge-1001"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visual Network Analytics for Cyber Security - SAS Institute - Mini-Challenge 1 Honorable Mention: Clear Articulation of Analytical Process",
                        "contributors": [
                            "Falko Schulz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T17:05:00Z",
                        "time_end": "2020-10-26T17:20:00Z",
                        "uid": "a-vastchallenge-1024"
                    },
                    {
                        "type": "Live Zoom",
                        "title": "Feedback Session",
                        "contributors": [
                            "All"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T17:20:00Z",
                        "time_end": "2020-10-26T17:30:00Z"
                    }
                ]
            },
            {
                "title": "Session 3",
                "session_id": "a-vastchallenge-3",
                "chair": [
                    "Kristen Liggett",
                    "Jordan Crouser"
                ],
                "organizers": [],
                "display_start": "2020-10-26T18:00:00Z",
                "time_start": "2020-10-26T18:00:00Z",
                "time_end": "2020-10-26T19:30:00Z",
                "discord_category": "vast challenge",
                "discord_channel": "session-3",
                "discord_channel_id": "768305373622698094",
                "youtube_url": "https://youtu.be/ORyJa4eM6tU",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Panel",
                        "title": "Panel - VAST Challenge Beyond Awards",
                        "contributors": [
                            "Daniel Keim",
                            "Christopher Andrews",
                            "Catherine Plaisant",
                            "Alvitta Ottley",
                            "Riley Benson"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T18:00:00Z",
                        "time_end": "2020-10-26T19:00:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Mini-Challenge 3 Introduction - Design Challenge",
                        "contributors": [
                            "Kristen Liggett"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T19:00:00Z",
                        "time_end": "2020-10-26T19:10:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "ConstellationBuilder: High Level Situation Awareness and Team Assembly for Cyber Security Events - Purdue University - Mini-Challenge 3 Award: Effective Transformation of Task Decomposition into Conceptual Design",
                        "contributors": [
                            "Lu Ding",
                            "Jieqiong Zhao"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T19:10:00Z",
                        "time_end": "2020-10-26T19:30:00Z",
                        "uid": "a-vastchallenge-1037"
                    }
                ]
            },
            {
                "title": "Session 4",
                "session_id": "a-vastchallenge-4",
                "chair": [
                    "Jordan Crouser"
                ],
                "organizers": [],
                "display_start": "2020-10-26T20:00:00Z",
                "time_start": "2020-10-26T20:00:00Z",
                "time_end": "2020-10-26T20:50:00Z",
                "discord_category": "vast challenge",
                "discord_channel": "session-4",
                "discord_channel_id": "768305415099252790",
                "youtube_url": "https://youtu.be/u5FZzkcEwaA",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Announcements",
                        "contributors": [
                            "Jordan Crouser"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T20:00:00Z",
                        "time_end": "2020-10-26T20:05:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Capstone Presentation: The Role of Design in Visual Analytics",
                        "contributors": [
                            "Diane Staheli"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T20:05:00Z",
                        "time_end": "2020-10-26T20:50:00Z"
                    }
                ]
            }
        ]
    },
    "a-biovischallenge": {
        "event": "BioVis Challenges",
        "long_name": "BioVis Challenges",
        "event_type": "Workshop",
        "event_description": "The rapidly expanding field of biology creates enormous challenges for data visualization techniques that enable researchers to gain insight from their large and highly complex data sets. The BioVis Interest Group organizes this interdisciplinary workshop at IEEE VIS, covering aspects of visualization in biology. This workshop brings together researchers from the visualization, bioinformatics, and biology communities with the purpose of educating, inspiring, and engaging visualization researchers in problems in biological data visualization.",
        "event_url": "http://biovis.net/2020/biovisChallenges_vis/",
        "sessions": [
            {
                "title": "BioVis Challenge",
                "session_id": "a-biovischallenge",
                "chair": [
                    "Anamaria Crisan"
                ],
                "organizers": [
                    "Anamaria Crisan",
                    "Carolina Nobre"
                ],
                "display_start": "2020-10-26T14:00:00Z",
                "time_start": "2020-10-26T14:00:00Z",
                "time_end": "2020-10-26T17:30:00Z",
                "discord_category": "biovis challenges",
                "discord_channel": "biovis-challenge",
                "discord_channel_id": "768305310300504094",
                "youtube_url": "https://youtu.be/5CDu1d5AfdE",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Panel",
                        "title": "Intro",
                        "contributors": [
                            "Anamaria Crisan",
                            "Carolina Nobre"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:00:00Z",
                        "time_end": "2020-10-26T14:05:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Domain Expert Presentation: David",
                        "contributors": [
                            "David Aanensen"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:05:00Z",
                        "time_end": "2020-10-26T14:30:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Domain Expert Presentation: Gytis",
                        "contributors": [
                            "Gytis Dudas"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:30:00Z",
                        "time_end": "2020-10-26T14:55:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Domain Expert Presentation: Sidney",
                        "contributors": [
                            "Sideny Bell"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:55:00Z",
                        "time_end": "2020-10-26T15:25:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Extra Q&A",
                        "contributors": [
                            "All"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:25:00Z",
                        "time_end": "2020-10-26T15:30:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Data Challenge Speaker: Hilary",
                        "contributors": [
                            "Hilary Coon"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:00:00Z",
                        "time_end": "2020-10-26T16:30:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Interactive Visualization of Multivariate Clinical Data in Force Graph",
                        "contributors": [
                            "Mingi Ryu"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:30:00Z",
                        "time_end": "2020-10-26T16:35:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "VisFCAC: An Interactive Family Clinical Attribute Comparison",
                        "contributors": [
                            "Jake Gonzalez"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:35:00Z",
                        "time_end": "2020-10-26T16:40:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "An Enhanced MA Plot with R-Shiny to Ease Exploratory Analysis of Transcriptomic Data",
                        "contributors": [
                            "Michael Aupetit"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:40:00Z",
                        "time_end": "2020-10-26T16:45:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "The Sawtooh Chart for Compact Cumulative Data Visualization",
                        "contributors": [
                            "Romain Vuillemot"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:45:00Z",
                        "time_end": "2020-10-26T16:50:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Contest Questions",
                        "contributors": [
                            "All"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:50:00Z",
                        "time_end": "2020-10-26T16:55:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Captsone",
                        "contributors": [
                            "Nils Gehlenborg"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:55:00Z",
                        "time_end": "2020-10-26T17:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-psychology": {
        "event": "IEEE VIS Workshop on Visualization Psychology",
        "long_name": "IEEE VIS Workshop on Visualization Psychology",
        "event_type": "Workshop",
        "event_description": "Before 2010, each VIS conference typically featured 0-2 papers on empirical studies. The VisWeek 2010 in Salt Lake City became a turning point, and since then more and more empirical study papers have been presented at VIS. Between 2016 and 2019, there were some 60 empirical study papers in VIS/TVCG tracks. Many young talents who are knowledgeable in both VIS and psychology emerged in the VIS community, while many colleagues in psychology are authoring and co-authoring such papers and attending VIS conferences. It is therefore timely to ask: Is there a need for \u201cVisualization Psychology\u201d as a new interdisciplinary subject? Led by young researchers in both VIS and psychology, this proposed workshop will complement BELIV and VISxVISION by (i) identifying a broad range of visualization phenomena that cannot be adequately explained by existing theories and experiments in VIS and psychology; (ii) Exploring the research questions beyond the scope of visual perception and reaching out to more research findings in psychology in many areas of cognition; and (iii) Enabling visualization and visual analytics to become a rich playground for making fundamental discoveries in psychology and cognitive science in general.",
        "event_url": "https://sites.google.com/view/vispsych/",
        "sessions": [
            {
                "title": "IEEE VIS Workshop on Visualization Psychology",
                "session_id": "w-psychology",
                "chair": [
                    "Lace Padilla",
                    "Rita Borgo",
                    "Danielle Szafir",
                    "Darren Edwards"
                ],
                "organizers": [],
                "display_start": "2020-10-26T14:00:00Z",
                "time_start": "2020-10-26T14:00:00Z",
                "time_end": "2020-10-26T17:35:00Z",
                "discord_category": "ieee vis workshop on visualization psychology",
                "discord_channel": "general",
                "discord_channel_id": "768305318031261706",
                "youtube_url": "https://youtu.be/GLiFg3M70Mk",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Opening Presentation",
                        "contributors": [
                            "Danielle Szafir"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:00:00Z",
                        "time_end": "2020-10-26T14:05:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Keynote: How Graphics Communicate",
                        "contributors": [
                            "Barbara Tversky"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:05:00Z",
                        "time_end": "2020-10-26T14:45:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Using Resource-Rational Analysis to Understand Cognitive Biases in Interactive Data Visualizations",
                        "contributors": [
                            "Ryan Wesslen"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:45:00Z",
                        "time_end": "2020-10-26T14:55:00Z",
                        "uid": "w-psychology-4345"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "The Development of Visualization Psychology Analysis Tools to Account for Trust",
                        "contributors": [
                            "Rita Borgo"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:55:00Z",
                        "time_end": "2020-10-26T15:05:00Z",
                        "uid": "w-psychology-7955"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Did You Get The Gist Of It? Understanding How Visualization Impacts Decision-Making",
                        "contributors": [
                            "Melanie Bancilhon"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:05:00Z",
                        "time_end": "2020-10-26T15:15:00Z",
                        "uid": "w-psychology-2505"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Plenary Discussions",
                        "contributors": [
                            "Ryan Wesslen",
                            "Rita Borgo",
                            "Melanie Bancilhon"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:15:00Z",
                        "time_end": "2020-10-26T15:30:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Developing Effective Community Network Analysis Tools according to Visualization Psychology",
                        "contributors": [
                            "Darren J. Edwards"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:00:00Z",
                        "time_end": "2020-10-26T16:10:00Z",
                        "uid": "w-psychology-2243"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "What We See and What We Get from Visualization: Eye Tracking Beyond Gaze Distributions and Scanpaths",
                        "contributors": [
                            "Kuno Kurzhals"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:10:00Z",
                        "time_end": "2020-10-26T16:20:00Z",
                        "uid": "w-psychology-9740"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Task Matters",
                        "contributors": [
                            "Laura Matzen"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:20:00Z",
                        "time_end": "2020-10-26T16:30:00Z",
                        "uid": "w-psychology-9153"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Plenary Discussions",
                        "contributors": [
                            "Darren J. Edwards",
                            "Kuno Kurzhals",
                            "Laura Matzen"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:30:00Z",
                        "time_end": "2020-10-26T16:45:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Psychology of Visualization or (External) Representation?",
                        "contributors": [
                            "Amy Rae Fox"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:45:00Z",
                        "time_end": "2020-10-26T16:55:00Z",
                        "uid": "w-psychology-8679"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "The Huge Variable Space in Empirical Studies for Visualization - A Challenge as well as an opportunity for Visualization Psychology",
                        "contributors": [
                            "Alfie Abdul-Rahman"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:55:00Z",
                        "time_end": "2020-10-26T17:05:00Z",
                        "uid": "w-psychology-9923"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "How do Visualization Designers Think? Design Cognition as a Core Aspect of Visualization Psychology",
                        "contributors": [
                            "Paul Parsons"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T17:05:00Z",
                        "time_end": "2020-10-26T17:15:00Z",
                        "uid": "w-psychology-4831"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Plenary Discussions",
                        "contributors": [
                            "Amy Rae Fox",
                            "Alfie Abdul-Rahman",
                            "Paul Parsons"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T17:15:00Z",
                        "time_end": "2020-10-26T17:30:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Closing Presentation",
                        "contributors": [
                            "Rita Borgo"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T17:30:00Z",
                        "time_end": "2020-10-26T17:35:00Z"
                    }
                ]
            }
        ]
    },
    "t-scivishoudini": {
        "event": "Scientific Visualization in Houdini: How to use Visual Effects Software for a Cinematic Presentation of Science",
        "long_name": "Scientific Visualization in Houdini: How to use Visual Effects Software for a Cinematic Presentation of Science",
        "event_type": "Tutorial",
        "event_description": "Cinematic scientific visualization makes three dimensional scientific phenomena approachable for mass audiences by using the artistic language of film including elements like camera choreography, lighting design, comprehensive scenic environments, and more. Cinematic scientific visualizations are an engaging way for domain experts to communicate niche information with the public, to refute widely held misconceptions, and to inspire the scientists of the future. Science films that feature these visualizations are screened at science centers to millions of viewers over the span of 10+ years and bridge different languages and cultures. They are shared widely on social media, featured regularly in television programs, and contribute to the success of public lectures.\n\nIf you are a domain expert looking to share your data more widely, or a visualization designer who has focused on more analytical tools, what better way is there to get started with a Hollywood style than by using Hollywood tools? This tutorial will introduce participants to Houdini, a visual effects software package that can generate cinematic-quality data visualizations with ease and efficiency. It is used and appreciated by most major animation and visual effects film studios for its procedural architecture, its modular design, and out-of-the-box rendering algorithms, all important features for ease-of-use in the field of data visualization. Houdini is a general-purpose image-making software that differs from most traditional scientific visualization tools in that it is optimized for look development and design functionality.\n\nIn this tutorial, participants will learn how to use Houdini to create a production-quality visualization from start to finish. They will translate a tornado cloud simulation into a Houdini-friendly format using Python, then ingest it into Houdini, transform it, add an environment, a camera, and a light source which mimics a sunset. Participants will be able to experiment with their own data transfer functions, camera movement, and lighting design. Several pre-made Houdini sample scenes will be explored to show how to create derivative data, how to turn 2D images into 3D height fields, and how to manipulate polygons, point clouds, and volumes.",
        "event_url": "",
        "sessions": [
            {
                "title": "Scientific Visualization in Houdini: How to use Visual Effects Software for a Cinematic Presentation of Science",
                "session_id": "t-scivishoudini",
                "chair": [
                    "Kalina Borkiewicz",
                    "AJ Christensen"
                ],
                "organizers": [
                    "AJ Christensen",
                    "Kalina Borkiewicz"
                ],
                "display_start": "2020-10-26T14:00:00Z",
                "time_start": "2020-10-26T14:00:00Z",
                "time_end": "2020-10-26T17:30:00Z",
                "discord_category": "scientific visualization in houdini how to use visual effects software for a cinematic presentation",
                "discord_channel": "general",
                "discord_channel_id": "768305325467631637",
                "youtube_url": "https://youtu.be/J0_D5ykF9V4",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Introduction to Houdini",
                        "contributors": [
                            "AJ Christensen",
                            "Kalina Borkiewicz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:00:00Z",
                        "time_end": "2020-10-26T14:15:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "The Houdini User Interface",
                        "contributors": [
                            "AJ Christensen",
                            "Kalina Borkiewicz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:15:00Z",
                        "time_end": "2020-10-26T15:00:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Deriving Geometry from Data",
                        "contributors": [
                            "AJ Christensen",
                            "Kalina Borkiewicz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:00:00Z",
                        "time_end": "2020-10-26T15:15:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Working with external assets",
                        "contributors": [
                            "AJ Christensen",
                            "Kalina Borkiewicz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:15:00Z",
                        "time_end": "2020-10-26T15:30:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Programming and Data Processing",
                        "contributors": [
                            "AJ Christensen",
                            "Kalina Borkiewicz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:45:00Z",
                        "time_end": "2020-10-26T15:55:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Translating a tornado dataset from .GDA to .GEO",
                        "contributors": [
                            "AJ Christensen",
                            "Kalina Borkiewicz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:55:00Z",
                        "time_end": "2020-10-26T16:10:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Transforming the tornado",
                        "contributors": [
                            "AJ Christensen",
                            "Kalina Borkiewicz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:10:00Z",
                        "time_end": "2020-10-26T16:20:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Adding a shader to derive color and opacity from data",
                        "contributors": [
                            "AJ Christensen",
                            "Kalina Borkiewicz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:20:00Z",
                        "time_end": "2020-10-26T16:40:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Adding a ground plane to the scene",
                        "contributors": [
                            "AJ Christensen",
                            "Kalina Borkiewicz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:40:00Z",
                        "time_end": "2020-10-26T16:50:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Adding a light source to the scene",
                        "contributors": [
                            "AJ Christensen",
                            "Kalina Borkiewicz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:50:00Z",
                        "time_end": "2020-10-26T17:00:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Adding a camera to the scene",
                        "contributors": [
                            "AJ Christensen",
                            "Kalina Borkiewicz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T17:00:00Z",
                        "time_end": "2020-10-26T17:10:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Rendering the scene",
                        "contributors": [
                            "AJ Christensen",
                            "Kalina Borkiewicz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T17:10:00Z",
                        "time_end": "2020-10-26T17:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-activities": {
        "event": "VisActivities: Workshop on Data Vis Activities to Facilitate Learning, Reflecting, Discussing, and Designing",
        "long_name": "VisActivities: Workshop on Data Vis Activities to Facilitate Learning, Reflecting, Discussing, and Designing",
        "event_type": "Workshop",
        "event_description": "This workshop focuses on data-visualization activities, especially methods and challenges for teaching and engaging with data visualization concepts, knowledge, and practices. For example, sketching aids designers to consider alternative ideas; manipulating tokens help students conceptualize quantities for data visualization; user interviews and discussions help developers understand requirements. Workshops, classes, or collaborations with domain experts, often include hands-on data visualization activities that involve analog or digital tools and materials and more or less well defined protocols. Recent years have seen the emergence of such data visualization activities in different contexts, including education, visualization design, activism, self-reflection, and interdisciplinary collaboration. However, the broad range of contexts and target audiences that Data-Vis activities have been applied to makes it difficult to collect and identify commonalities and build knowledge in a systematic way. Thus, the goals of this workshop are i) start building an understanding and to synthesize protocols and materials used to lead data vis activities, ii) to bring together researchers, practitioners, and educators from within and outside of the visualization community, iii) brainstorm, design, experience, and try novel activities, and to iv) discuss issues around goals, methods, audiences, materials, and evaluation for teaching data visualization.",
        "event_url": "https://visactivities.github.io/",
        "sessions": [
            {
                "title": "VisActivities: Workshop on Data Vis Activities to Facilitate Learning, Reflecting, Discussing, and Designing",
                "session_id": "w-activities",
                "chair": [
                    "Benjamin Bach",
                    "Samuel Huron",
                    "Jonathan Roberts"
                ],
                "organizers": [
                    "Samuel Huron",
                    "Benjamin Bach",
                    "Uta Hinrichs",
                    "Jonathan C. Roberts",
                    "Mandy Keck"
                ],
                "display_start": "2020-10-26T14:00:00Z",
                "time_start": "2020-10-26T14:00:00Z",
                "time_end": "2020-10-26T17:30:00Z",
                "discord_category": "visactivities workshop on data vis activities to facilitate learning reflecting discussing and desi",
                "discord_channel": "general",
                "discord_channel_id": "768305333222375495",
                "youtube_url": "https://youtu.be/sYLTUEjsGzU",
                "zoom_meeting": "https://us02web.zoom.us/j/86917193702?pwd=VlZWV1hscXI2Vk1SellLZFNQNDM2dz09",
                "zoom_password": "BAZhaP0m",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Introduction",
                        "contributors": [
                            "Samuel Huron"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:00:00Z",
                        "time_end": "2020-10-26T14:15:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Teaching Data Viz to Kids",
                        "contributors": [
                            "Jonathan Schwabish"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:15:00Z",
                        "time_end": "2020-10-26T14:20:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "The VisTools Marketplace: An Activity to Understand the Landscape of Visualisation Tools",
                        "contributors": [
                            "Arran Ridley",
                            "Sarah Sch\u00f6ttler",
                            "Aba-Sah Dadzie",
                            "Benjamin Bach"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:20:00Z",
                        "time_end": "2020-10-26T14:25:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Surfacing Misconceptions Through Visualization Critique",
                        "contributors": [
                            "Amy Rae Fox",
                            "Taylor Jackson Scott"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:25:00Z",
                        "time_end": "2020-10-26T14:30:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "GoCo: A Gamified Activity for Winnowing Visualization Projects with Interdisciplinary - Experts",
                        "contributors": [
                            "Georgia Panagiotidou",
                            "Jan Aerts",
                            "Andrew Vande Moere"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:30:00Z",
                        "time_end": "2020-10-26T14:35:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Q&A for First set of Papers",
                        "contributors": [
                            "Jonathan Schwabish",
                            "Arran Ridley",
                            "Sarah Sch\u00f6ttler",
                            "Aba-Sah Dadzie",
                            "Benjamin Bach",
                            "Amy Rae Fox",
                            "Taylor Jackson Scott",
                            "Georgia Panagiotidou",
                            "Jan Aerts",
                            "Andrew Vande Moere"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:35:00Z",
                        "time_end": "2020-10-26T14:50:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Pandemic Pedagogy: Taking Data-Viz Learning Online",
                        "contributors": [
                            "Rahul Bhargava"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:50:00Z",
                        "time_end": "2020-10-26T14:55:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Data Crafting: Exploring Data Through Craft And Play",
                        "contributors": [
                            "Nathalie Alexandra Vladis",
                            "Aspen K. Hopkins",
                            "Arvind Satyanarayan"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:55:00Z",
                        "time_end": "2020-10-26T15:00:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Visualization Improvisation",
                        "contributors": [
                            "Swaroop Panda",
                            "Shatarupa Thakurta Roy"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:00:00Z",
                        "time_end": "2020-10-26T15:05:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Design Sprints for Online and On-Campus Visualization Courses",
                        "contributors": [
                            "Johanna Beyer",
                            "Hanspeter Pfister"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:05:00Z",
                        "time_end": "2020-10-26T15:10:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Q&A for Second set of Papers",
                        "contributors": [
                            "Rahul Bhargava",
                            "Nathalie Alexandra Vladis",
                            "Aspen K. Hopkins",
                            "Arvind Satyanarayan",
                            "Swaroop Panda",
                            "Shatarupa Thakurta Roy",
                            "Johanna Beyer",
                            "Hanspeter Pfister"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:10:00Z",
                        "time_end": "2020-10-26T15:25:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Introduction",
                        "contributors": [
                            "Benjamin Bach"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:00:00Z",
                        "time_end": "2020-10-26T16:05:00Z"
                    },
                    {
                        "type": "Zoom Breakout",
                        "title": "Break Out Session Work",
                        "contributors": [
                            "All Attendees"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:05:00Z",
                        "time_end": "2020-10-26T16:35:00Z"
                    },
                    {
                        "type": "Live Presentations",
                        "title": "Report and Presentation on Breakout Session",
                        "contributors": [
                            "All Attendees"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:35:00Z",
                        "time_end": "2020-10-26T17:05:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Community Building and Discussion",
                        "contributors": [
                            "All Attendees"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T17:05:00Z",
                        "time_end": "2020-10-26T17:20:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Conclusion and Future Announcement",
                        "contributors": [
                            "Mandy Keck"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T17:20:00Z",
                        "time_end": "2020-10-26T17:30:00Z"
                    }
                ]
            }
        ]
    },
    "t-ttk": {
        "event": "Topological Data Analysis Made Easy with the Topology ToolKit, What is New?",
        "long_name": "Topological Data Analysis Made Easy with the Topology ToolKit, What is New?",
        "event_type": "Tutorial",
        "event_description": "This tutorial presents topological methods for the analysis and visualization of scientific data from a user\u2019s perspective, with the Topology ToolKit (TTK), an open-source library for topological data analysis. Topological methods have gained considerably in popularity and maturity over the last twenty years and success stories of established methods have been documented in a wide range of applications (combustion, chemistry, astrophysics, material sciences, etc.) with both acquired and simulated data, in both post-hoc and in-situ contexts. This tutorial aims to fill a gap by providing a beginner\u2019s introduction to topological methods for practitioners, researchers, students, and lecturers. In particular, instead of focusing on theoretical aspects and algorithmic details, this tutorial focuses on how topological methods can be useful in practice for concrete data analysis tasks such as segmentation, feature extraction or tracking. The tutorial describes in detail how to achieve these tasks with TTK. In comparison to the last two iterations of this tutorial, this iteration emphasizes the features of TTK which now appear to be the most popular, as well as the latest additions to the library. First, we provide a general introduction to topological methods and their application in data analysis, and a brief overview of TTK\u2019s main entry point for end users, namely ParaView, will be presented. Second, we will proceed to a hands-on session demoing the main features of TTK as well as its most recent additions. Third, we will present advanced usages of TTK, including the usage of TTK with Python, the development of a new module for TTK as well as the integration of TTK into a pre-existing system. Presenters of this tutorial include experts in topological methods, core authors of TTK as well as active users, coming from academia and industry. A large part of the tutorial will be dedicated to hands-on exercises and a rich material package will be provided to the participants. This tutorial mostly targets students, practitioners and researchers who are not necessarily experts in topological methods but who are interested in using them in their daily tasks. We also target researchers already familiar to topological methods and who are interested in using or contributing to TTK. We kindly ask potential attendees to optionally pre-register at the following address, in order for us to reach out to them ahead of the tutorial with information updates (for instance, last minute updates, instructions for the download of the tutorial material package, etc.): https://forms.gle/CvrY3oWZB9hWSQJb9 Tutorial web page (including all material, TTK pre-installs in virtual machines, code, data, demos, video tutorials, slides, etc): https://topology-tool-kit.github.io/ieeeVisTutorial.html",
        "event_url": "https://topology-tool-kit.github.io/ieeeVisTutorial.html",
        "sessions": [
            {
                "title": "Topological Data Analysis Made Easy with the Topology ToolKit, What is New?",
                "session_id": "t-ttk",
                "chair": [
                    "Julien Tierny"
                ],
                "organizers": [
                    "Martin Falk",
                    "Christoph Garth",
                    "Charles Gueunet",
                    "Pierre Guillou",
                    "Attila Gyulassy",
                    "Lutz Hofmann",
                    "Christopher P Kappe",
                    "Joshua A Levine",
                    "Jonas Lukasczyk",
                    "Julien Tierny",
                    "Jules Vidal"
                ],
                "display_start": "2020-10-26T14:00:00Z",
                "time_start": "2020-10-26T14:00:00Z",
                "time_end": "2020-10-26T17:30:00Z",
                "discord_category": "topological data analysis made easy with the topology toolkit what is new",
                "discord_channel": "general",
                "discord_channel_id": "768305340068397106",
                "youtube_url": "https://youtu.be/l5CjWChCVbM",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "General Introduction",
                        "contributors": [
                            "Julien Tierny"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:00:00Z",
                        "time_end": "2020-10-26T14:05:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Introduction to Topological Methods for Data Analysis",
                        "contributors": [
                            "Attila Gyulassy"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:05:00Z",
                        "time_end": "2020-10-26T14:36:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Quick Introduction to ParaView's User Interface",
                        "contributors": [
                            "Charles Gueunet"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T14:36:00Z",
                        "time_end": "2020-10-26T15:01:00Z"
                    },
                    {
                        "type": "Live Zoom",
                        "title": "Q&A and Break",
                        "contributors": [
                            "Julien Tierny"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:01:00Z",
                        "time_end": "2020-10-26T15:16:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "General Usage of TTK",
                        "contributors": [
                            "Julien Tierny"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:16:00Z",
                        "time_end": "2020-10-26T15:27:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Running TTK with Docker",
                        "contributors": [
                            "Christoph Garth"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:27:00Z",
                        "time_end": "2020-10-26T15:37:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Segmenting Medical Data with Merge Trees",
                        "contributors": [
                            "Charles Gueunet"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:37:00Z",
                        "time_end": "2020-10-26T15:45:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Extracting Filament Structures with the Morse-Smale Complex",
                        "contributors": [
                            "Pierre Guillou"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:45:00Z",
                        "time_end": "2020-10-26T15:55:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Extracting Contours Associated with Critical Points",
                        "contributors": [
                            "Christopher Kappe"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T15:55:00Z",
                        "time_end": "2020-10-26T16:05:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Distances, Barycenters and Clusters",
                        "contributors": [
                            "Jules Vidal"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:05:00Z",
                        "time_end": "2020-10-26T16:16:00Z"
                    },
                    {
                        "type": "Live Zoom",
                        "title": "Q&A and Break",
                        "contributors": [
                            "Julien Tierny"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:16:00Z",
                        "time_end": "2020-10-26T16:31:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Using TTK with Python",
                        "contributors": [
                            "Lutz Hofmann"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:31:00Z",
                        "time_end": "2020-10-26T16:40:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Developing a New Module for TTK",
                        "contributors": [
                            "Jonas Lukasczyk"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:40:00Z",
                        "time_end": "2020-10-26T16:51:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "TTK Integration into Inviwo",
                        "contributors": [
                            "Martin Falk"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:51:00Z",
                        "time_end": "2020-10-26T17:05:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "TTK as a Teaching Platform",
                        "contributors": [
                            "Joshua Levine"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T17:05:00Z",
                        "time_end": "2020-10-26T17:16:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Concluding Remarks",
                        "contributors": [
                            "Julien Tierny"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T17:15:00Z",
                        "time_end": "2020-10-26T17:20:00Z"
                    },
                    {
                        "type": "Live Zoom",
                        "title": "Wrap Up Q&A",
                        "contributors": [
                            "Julien Tierny"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T17:20:00Z",
                        "time_end": "2020-10-26T17:30:00Z"
                    }
                ]
            }
        ]
    },
    "a-visinpractice": {
        "event": "VisInPractice",
        "long_name": "VisInPractice",
        "event_type": "Associated Event",
        "event_description": "VisInPractice provides an opportunity for practitioners and researchers to share experiences, insights, and ideas in applying visualization and visual analytics to real use cases.",
        "event_url": "https://visinpractice.github.io/",
        "sessions": [
            {
                "title": "Invited Talks 1",
                "session_id": "a-visinpractice-1",
                "chair": [
                    "Sean McKenna"
                ],
                "organizers": [
                    "Matthew Brehmer",
                    "Matt Larsen",
                    "Zicheng (Leo) Liu",
                    "Sean McKenna"
                ],
                "display_start": "2020-10-26T16:00:00Z",
                "time_start": "2020-10-26T16:00:00Z",
                "time_end": "2020-10-26T17:30:00Z",
                "discord_category": "visinpractice",
                "discord_channel": "invited-talks-1",
                "discord_channel_id": "768305346883485736",
                "youtube_url": "https://youtu.be/UqXqO4zx34s",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Intro to VisInPractice 2020",
                        "contributors": [
                            "Sean McKenna"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T16:00:00Z",
                        "time_end": "2020-10-26T16:05:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "More than a Style Guide: Data Visualization for Design Systems",
                        "contributors": [
                            "Alan Wilson"
                        ],
                        "abstract": "After a brief introduction to design systems I'll share what I've learned about creating data visualization guidelines and what we can do to make them better.\n",
                        "time_start": "2020-10-26T16:05:00Z",
                        "time_end": "2020-10-26T16:30:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Data visualization for machine learning practitioners",
                        "contributors": [
                            "Julia Silge"
                        ],
                        "abstract": "Visual representations of data inform how machine learning practitioners think, understand, and decide. Before charts are ever used for outward communication about a ML system, they are used by the system designers and operators themselves as a tool to make better modeling choices. Practitioners use visualization, from very familiar statistical graphics to creative and less standard plots, at the points of most important human decisions when other ways to validate those decisions can be difficult. Visualization approaches are used to understand both the data that serves as input for machine learning and the models that practitioners create. In this talk, learn about the process of building a ML model in the real world, how and when practitioners use visualization to make more effective choices, and considerations for ML visualization tooling.",
                        "time_start": "2020-10-26T16:35:00Z",
                        "time_end": "2020-10-26T17:00:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Machine Learning for Medicine: visualizations that accelerate drug discovery",
                        "contributors": [
                            "Jes Ford"
                        ],
                        "abstract": "Recursion is a digital biology company with a 4PB (and growing) dataset of images of human cells under various conditions. We are combining automation and data science to find new treatments for diseases, blurring the lines between traditional biology and technology. Visualization is an important tool for us to make sense of this enormous amount of high dimensional phenomic data. In this talk I'll walk through some of the visualizations we use throughout the drug discovery process: from handling raw images to unique strategies for making results from 1024-dimensional drug screens sensible on a 2-dimensional display, from assessing machine learning models to compound liabilities. Along the way I'll share some findings from the Recursion Cellular Image Classification challenge, a machine learning competition which was hosted on kaggle last year.",
                        "time_start": "2020-10-26T17:05:00Z",
                        "time_end": "2020-10-26T17:30:00Z"
                    }
                ]
            },
            {
                "title": "Invited Talk and Panel",
                "session_id": "a-visinpractice-3",
                "chair": [
                    "Leo Zhicheng Liu",
                    "Matthew Brehmer"
                ],
                "organizers": [
                    "Matthew Brehmer",
                    "Matt Larsen",
                    "Zicheng (Leo) Liu",
                    "Sean McKenna"
                ],
                "display_start": "2020-10-26T18:00:00Z",
                "time_start": "2020-10-26T18:00:00Z",
                "time_end": "2020-10-26T19:30:00Z",
                "discord_category": "visinpractice",
                "discord_channel": "invited-talk-and-panel",
                "discord_channel_id": "768305367430987796",
                "youtube_url": "https://youtu.be/ir1TMscZXHY",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "How Self-Employed Data Visualization Designers Make a Living",
                        "contributors": [
                            "Jane Zhang"
                        ],
                        "abstract": "Jane wrote an article of the same name published in Nightingale (https://medium.com/nightingale/how-self-employed-data-visualization-designers-make-a-living-23dc00ea5264?source=friends_link&sk=27527fd294b7eabd0872e49fb44ee7d8). In this article, Jane talks to 4 designers to understand how they are building their careers and how they make their income from dataviz.\n\nIn this talk, Jane expands on her article and add more insights and talk about things she's implemented since writing the article.",
                        "time_start": "2020-10-26T18:00:00Z",
                        "time_end": "2020-10-26T18:25:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Panel: Visualization Consulting & Freelancing",
                        "contributors": [
                            "Jane Zhang",
                            "Kristin Henry",
                            "Curran Kelleher"
                        ],
                        "abstract": "We've enlisted three practitioners (Jane Zhang, Kristin Henry, and Curran Kelleher) who can speak to the topic of visualization consulting and freelancing, on getting started and the types of challenges one faces. Some of the topics we may discuss include getting established as a freelancer / consultant, differences from other roles within organizations or academic institutions, making ends meet, types of projects, finding and attracting clients and balancing projects, and pitfalls to avoid.",
                        "time_start": "2020-10-26T18:25:00Z",
                        "time_end": "2020-10-26T19:30:00Z"
                    }
                ]
            },
            {
                "title": "Invited Talks and Closing",
                "session_id": "a-visinpractice-4",
                "chair": [
                    "Matt Larsen",
                    "Leo Zhicheng Liu"
                ],
                "organizers": [
                    "Matthew Brehmer",
                    "Matt Larsen",
                    "Zicheng (Leo) Liu",
                    "Sean McKenna"
                ],
                "display_start": "2020-10-26T20:00:00Z",
                "time_start": "2020-10-26T20:00:00Z",
                "time_end": "2020-10-26T21:30:00Z",
                "discord_category": "visinpractice",
                "discord_channel": "invited-talks-and-closing",
                "discord_channel_id": "768305407848349736",
                "youtube_url": "https://youtu.be/QeNPPxZD0H4",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Seaborn: One Python Visualization Tool to Rule Them All?",
                        "contributors": [
                            "Matt Harrison"
                        ],
                        "abstract": "Python has many plotting tools, each with pros and cons. It would be nice if there was a single library to do it all. This talk will discuss the pros and cons of Seaborn. Is it the one tool that does it all?",
                        "time_start": "2020-10-26T20:00:00Z",
                        "time_end": "2020-10-26T20:25:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visualizing Molecules",
                        "contributors": [
                            "Janet Iwasa"
                        ],
                        "abstract": "Life as we know it relies on the constant tireless work of tiny molecular machines. My work focuses on creating animations and illustrations of how these machines are hypothesized to operate, based on a variety of data collected by molecular biologists. In this talk, I'll discuss the process of creating molecular animations, how these visualizations help researchers better understand and communicate hypotheses, and share a few past and current projects.",
                        "time_start": "2020-10-26T20:30:00Z",
                        "time_end": "2020-10-26T20:55:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visualization of Data and Results for the FEBio Finite Element Software\n",
                        "contributors": [
                            "Steve Maas"
                        ],
                        "abstract": "FEBio is a finite element software designed for solving problems in computational biomechanics and biophysics. It can be used to solve problems in nonlinear structural mechanics, chemical reaction-diffusion, fluid mechanics, fluid-solid interactions, heat transfer, and more. In FEBio, the main focus is the implementation of constitutive models and modeling scenarios that are of particular interest to the biomechanics/biophysics research communities. FEBio, which is a command-line solver, is supported by FEBio Studio, which provides a graphical user interface to many of FEBio's features and allows users to setup their models, run FEBio, and visualize their results. FEBio Studio uses OpenGL for rendering and we developed many custom visualization algorithms for displaying FE results, including contour plots, vector plots, tensor plots, streamline plots, particle flow visualizations, and more.\n\nIn our most recent release of FEBio, we have greatly expanded the software's ability to represent heterogeneous model parameters, which is becoming an increasingly important aspect of computational modeling in biomechanics. This can be used, for instance, to model spatially varying material parameters, material anisotropy, and inhomogeneous boundary conditions or loads. This has posed challenges on both the FEBio solver side, as well as on the FEBio Studio side. In FEBio, various data structures were implemented that allow a variety of data representations over a finite element mesh: Data can be defined for nodes, elements, edges, or surface facets; Different levels of continuity across element and domain boundaries can be chosen. Various data representations can be converted between different formats (E.g. element data can be projected onto nodes). We are also working on mapping these various data representations between meshes to pave the path for doing adaptive mesh refinement in FEBio. On the FEBio Studio side, we are still working on supporting many of these new FEBio features, and many of the current development efforts revolve around how to best present these capabilities to the user. The challenges are not only related to how to best define spatially varying data from a user's perspective, but also in how to best visualize the data for verification purposes. In addition, spatially varying data often comes from images, so we are also working on integrating support for visualizing images and mapping image data onto the finite element mesh. Finally, since FEBio is a command line solver, communication with FEBio goes through different file formats. We use an xml-based input format, and a custom, binary output format for storing results. These file formats have been updated to support the many new data formats that are supported in FEBio.",
                        "time_start": "2020-10-26T21:00:00Z",
                        "time_end": "2020-10-26T21:25:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Conclusion to VisInPractice 2020",
                        "contributors": [
                            "Leo Zhicheng Liu"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T21:25:00Z",
                        "time_end": "2020-10-26T21:30:00Z"
                    }
                ]
            }
        ]
    },
    "a-visxai": {
        "event": "VISxAI",
        "long_name": "3rd Workshop on Visualization for AI Explainability",
        "event_type": "Workshop",
        "event_description": " The role of visualization in artificial intelligence (AI) gained significant attention in recent years. With the growing complexity of AI models, the critical need for understanding their inner-workings has increased. Visualization is potentially a powerful technique to fill such a critical need. The goal of this workshop is to initiate a call for \"explainables\" / \"explorables\" that explain how AI techniques work using visualization. We believe the VIS community can leverage their expertise in creating visual narratives to bring new insight into the often obfuscated complexity of AI systems. ",
        "event_url": "https://visxai.io/",
        "sessions": [
            {
                "title": "VISxAI",
                "session_id": "a-visxai",
                "chair": [
                    "Adam Perer",
                    "Menna El-Assady",
                    "Polo Chau",
                    "Hendrik Strobelt"
                ],
                "organizers": [
                    "VISxAI Organizers"
                ],
                "display_start": "2020-10-26T18:00:00Z",
                "time_start": "2020-10-26T18:00:00Z",
                "time_end": "2020-10-26T21:05:00Z",
                "discord_category": "visxai",
                "discord_channel": "general",
                "discord_channel_id": "768305360263708672",
                "youtube_url": "https://youtu.be/wEo9skwDfts",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "[Keynote] Facilitating interactive explanations with open-source libraries: An introduction to transfer learning in NLP and HuggingFace",
                        "contributors": [
                            "Thomas Wolf"
                        ],
                        "abstract": "In this talk I'll start by introducing the recent breakthroughs in NLP that resulted from the combination of Transfer Learning schemes and Transformer architectures. The second part of the talk will be dedicated to an introduction of the open-source tools released by HuggingFace, in particular our Transformers, Tokenizers and Datasets libraries and our models.",
                        "time_start": "2020-10-26T18:00:00Z",
                        "time_end": "2020-10-26T19:00:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "[Best submission] Comparing DNNs with UMAP Tour",
                        "contributors": [
                            "Mingwei Li"
                        ],
                        "abstract": "Neural networks currently perform as well \u2014 or nearly as well \u2014 as humans in a variety of tasks and domains. On the surface, modern neural networks may seem sophisticated since they can have billions of parameters. However, deep inside, they are well organized as a sequence of manageable functions, or 'layers'. These layers transform the input data into internal representations that themselves are eventually transformed into a solution of the task. The behavior of these layers is complex and multi-dimensional. The internal representations are not directly comparable across layers. How can we make them more readily visible?",
                        "time_start": "2020-10-26T19:00:00Z",
                        "time_end": "2020-10-26T19:15:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "[Best submission] How does a computer \"see\" ...",
                        "contributors": [
                            "Christopher Baronavski"
                        ],
                        "abstract": "Machine vision tools like facial recognition are increasingly being used for law enforcement, advertising, and other purposes. Pew Research Center itself recently used a machine vision system to measure the prevalence of men and women in online image search results. This kind of system develops its own rules for identifying men and women after seeing thousands of example images, but these rules can be hard for to humans to discern. To better understand how this works, we showed images of the Center\u2019s staff members to a trained machine vision system similar to the one we used to classify image searches. We then systematically obscured sections of each image to see which parts of the face caused the system to change its decision about the gender of the person pictured. Some of the results seemed intuitive, others baffling. In this interactive challenge, see if you can guess what makes the system change its decision.",
                        "time_start": "2020-10-26T19:15:00Z",
                        "time_end": "2020-10-26T19:30:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Theo Guesser",
                        "contributors": [
                            "Theo Jaunet"
                        ],
                        "abstract": "This work is at the crossroads of three different fields and explores techniques of data visualization to advance machine learning for robotics applications. We propose new techniques for the visualization of the sim2real gap, which provide insights into the difference in performance obtained when neural networks are applied out of distribution (OOD) in real settings. The objective is to pinpoint transfer problems and to assist researchers and engineers in the fields of machine learning for robotics to design neural models which better transfer to real world scenarios.",
                        "time_start": "2020-10-26T20:00:00Z",
                        "time_end": "2020-10-26T20:10:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Shared Interest: Human Annotations vs. AI Saliency",
                        "contributors": [
                            "Angie Boggust"
                        ],
                        "abstract": "As deep learning is applied to high stakes scenarios, it is increasingly important that a model is not only making accurate decisions, but doing so for the right reasons. Common explainability methods provide pixel attributions as an explanation for a model's decision on a single image. However, using these input-level explanations to understand patterns in model behavior is challenging for large datasets as it requires selecting and analyzing an interesting subset of inputs. By utilizing the human generated bounding boxes that represent ground truth object locations, we introduce metrics for scoring and ranking inputs based on the correspondence between the input\u2019s ground truth object location and the explainability method\u2019s explanation region. Our methodology is agnostic to model architecture, explanation method, and image dataset allowing it to be applied to many tasks and domains. We apply our method to two high profile scenarios: an open source image classification model widely used in the community and a melanoma prediction model, showing it surfaces patterns in model behavior by aligning model explanations with human annotations.",
                        "time_start": "2020-10-26T20:10:00Z",
                        "time_end": "2020-10-26T20:20:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "A Visual Exploration of Fair Evaluation for ML- Bridging the Gap Between Research and the Real World",
                        "contributors": [
                            "Anjana Arunkumar"
                        ],
                        "abstract": "A common complaint from industry over the years has been that models selected based on their success in existing datasets do not do well when deployed in real world applications. Questions which have remained unexplored over the years are: Are our leaderboards doing fair evaluation? Can we revamp our leaderboards in a way that can help industries select a 'better' model according to their requirements? In order to assist users in selecting the model best suited for their applications, we present an interactive tool that (i) illustrates a task-agnostic method for probing leaderboards to find out whether a model is dominating leaderboards just by solving `easy\u2019 questions, (ii) explains three new metrics proposed to customize leaderboard evaluation based on the application area of the end user (iii) educates user about the design of weights in these metrics by visualizing change in model ranking based on customization.",
                        "time_start": "2020-10-26T20:20:00Z",
                        "time_end": "2020-10-26T20:30:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Anomagram: An Interactive Visualization for Training and Evaluating Autoencoders on the task of Anomaly Detection",
                        "contributors": [
                            "Dr Victor C Dibia"
                        ],
                        "abstract": "Deep Neural Networks excel at approximating complex mapping functions and have found extensive applications across multiple domains. However, they rely on complex mathematical functions and can be challenging to understand and effectively apply. In this work, we introduce Anomagram - an interactive visualization designed to help users build intuition on how a deep feed forward autoencoder model can be applied to the task of Anomaly Detection. To achieve these goals, Anomagram provides two main modules - an \\textit{explainer} module that provides a discussions of concepts, paired with interactive visualizations, and a \\textit{trainer} module where users can build, train and evaluate an autoencoder from scratch. Anomagram integrates a direct manipulation model composer interface for specifying the autoencoder, and providers interactive charts that illustrate model performance (loss, evaluation metrics, histogram of errors, ROC curve, internal state dimensions) as training progresses. Anomagram addresses the lack of realism that frequently limits interactive learning tools by integrating a real world dataset (ECG5000). To further aid the learning process, it also allows the user to craft specialized (adversarial) input and observe performance. Anomagram runs entirely in the browser with no installation required, which makes it an accessible learning tool to any user with a modern web browser.",
                        "time_start": "2020-10-26T20:30:00Z",
                        "time_end": "2020-10-26T20:40:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "What does BERT dream of?",
                        "contributors": [
                            "Alex B\u00e4uerle"
                        ],
                        "abstract": "Feature visualization has proven to be helpful when analyzing neural networks. In recent years, more and more insights are published based on these techniques. However, currently feature visualization is limited to image data, while at the same time neural networks that operate on text data are getting more and more important. Thus, we investigated how feature visualization can be adapted to such models, and conducted a series of experiments along this line. In this explainable, we show how we adapted the techniques of feature visualization to text models. We present the results of such experiments with BERT, a transformer-based text model. Through our interactive environment, users can explore how feature visualization for text can be implemented, see the results of different feature visualization based experiments, learn about the limitations and problems with feature visualization for text, and get ideas for why some of the image-based results might not be transferable to text.",
                        "time_start": "2020-10-26T20:40:00Z",
                        "time_end": "2020-10-26T20:50:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Active Learning: A visual tour",
                        "contributors": [
                            "Zeel B Patel"
                        ],
                        "abstract": "Today, machine learning (ML) is applied to numerous fields, including, but not limited to Natural Language Processing (NLP), Computer-aided diagnosis, Optimization, and Bioinformatics. A significant proportion of this success is due to a subset of ML called supervised learning. There are three main reasons behind the success of supervised learning (and machine learning, generally): 1) availability of massive data; 2) better algorithms; and 3) powerful computational infrastructure jointly called the AI Trinity. Supervised learning techniques require labeled data. As the data turns into 'Big data,' the effort required to label it becomes more laborious. In this article, we will talk about active learning, a suite of techniques for intelligent and data-driven annotations.",
                        "time_start": "2020-10-26T20:50:00Z",
                        "time_end": "2020-10-26T21:00:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Closing Presentation",
                        "contributors": [
                            "VISxAI Organizers"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T21:00:00Z",
                        "time_end": "2020-10-26T21:05:00Z"
                    }
                ]
            }
        ]
    },
    "w-futures": {
        "event": "Vis Futures: Design Fiction Methods for Envisioning Tomorrow's Visualizations",
        "long_name": "Vis Futures: Design Fiction Methods for Envisioning Tomorrow's Visualizations",
        "event_type": "Workshop",
        "event_description": "The goal of this workshop is to initiate the creation of one or several cards decks through a series of hands-on activities, that will serve as a tool for generating visualization design fictions. Although methods exist to ideate visualization designs, they are not tailored to design fictions. And although some methods exist for generating design fictions, they are not tailored to visualization. We believe that creating a design fiction method for envisioning tomorrow\u2019s visualizations is essential for the research community to think about speculative design. There is also considerable pedagogical value in such a cards game, and these could result in a fun and creative in-class activity for teaching visualization design.",
        "event_url": "https://visfutures.github.io/",
        "sessions": [
            {
                "title": "Vis Futures: Design Fiction Methods for Envisioning Tomorrow's Visualizations",
                "session_id": "w-futures",
                "chair": [
                    "Charles Perin",
                    "Lora Oehlberg"
                ],
                "organizers": [
                    "Charles Perin",
                    "Wesley Willett",
                    "Katherine Currier",
                    "Lora Oehlberg",
                    "Sheelagh Carpendale"
                ],
                "display_start": "2020-10-26T18:00:00Z",
                "time_start": "2020-10-26T18:00:00Z",
                "time_end": "2020-10-26T21:30:00Z",
                "discord_category": "vis futures design fiction methods for envisioning tomorrows visualizations",
                "discord_channel": "general",
                "discord_channel_id": "768305380614340620",
                "youtube_url": "https://youtu.be/OySBMto-S8c",
                "zoom_meeting": "https://us02web.zoom.us/j/88277243739?pwd=dDNjR2FpNDVzR1ZncWVoTEhHSUE2Zz09",
                "zoom_password": "S72zUOhd",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Opening Presentation",
                        "contributors": [
                            "All Attendees"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T18:00:00Z",
                        "time_end": "2020-10-26T18:15:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Fast Forward",
                        "contributors": [
                            "All Attendees"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T18:15:00Z",
                        "time_end": "2020-10-26T18:45:00Z"
                    },
                    {
                        "type": "Zoom Breakout",
                        "title": "Identification of Factors",
                        "contributors": [
                            "All Attendees"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T18:45:00Z",
                        "time_end": "2020-10-26T19:30:00Z"
                    },
                    {
                        "type": "Zoom Breakout",
                        "title": "Trying Out",
                        "contributors": [
                            "All Attendees"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T20:00:00Z",
                        "time_end": "2020-10-26T20:40:00Z"
                    },
                    {
                        "type": "Live Presentations",
                        "title": "Groups Reporting",
                        "contributors": [
                            "All Attendees"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T20:40:00Z",
                        "time_end": "2020-10-26T21:00:00Z"
                    },
                    {
                        "type": "Live Presentations",
                        "title": "Group Discussion",
                        "contributors": [
                            "All Attendees"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T21:00:00Z",
                        "time_end": "2020-10-26T21:30:00Z"
                    }
                ]
            }
        ]
    },
    "t-artifactvr": {
        "event": "Artifact Based Rendering: VR Visualization by Hand",
        "long_name": "Artifact Based Rendering: VR Visualization by Hand",
        "event_type": "Tutorial",
        "event_description": "Artifact-Based Rendering (ABR) is a new approach to designing immersive data-driven visualizations entirely from physical materials. Introduced in last year\u2019s technical program, the theory behind ABR is that harnessing the richness of nature and traditional artistic materials can help us create more effective data-driven visualizations by expanding the visual language available to us in digital space. Participants will arrive to see tables with clay, paints, drawing materials and more. The tutorial consists of four sections: 1. introducing ABR concepts and tools, 2. active physical crafting and then digitizing artifacts, and 3. attaching the artifacts to example datasets, and 4. a VR/slideshow presentation of the visualizations created and reflections on the experience and future potential.",
        "event_url": "",
        "sessions": [
            {
                "title": "Artifact Based Rendering: VR Visualization by Hand",
                "session_id": "t-artifactvr",
                "chair": [
                    "Daniel F. Keefe"
                ],
                "organizers": [
                    "Daniel F. Keefe",
                    "Francesca Samsel",
                    "Bridger Herman"
                ],
                "display_start": "2020-10-26T18:00:00Z",
                "time_start": "2020-10-26T18:00:00Z",
                "time_end": "2020-10-26T21:30:00Z",
                "discord_category": "artifact based rendering vr visualization by hand",
                "discord_channel": "general",
                "discord_channel_id": "768305387413438525",
                "youtube_url": "https://youtu.be/QD-KRw9Jelo",
                "zoom_meeting": "https://us02web.zoom.us/j/85949978959?pwd=UEhqSUxXdk1QeHQ5Tlh6R3pQc2o2QT09",
                "zoom_password": "LYxHVuNb",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Introduction to Part 1",
                        "contributors": [
                            "Daniel F. Keefe"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T18:00:00Z",
                        "time_end": "2020-10-26T18:15:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "The Interface - Use It!",
                        "contributors": [
                            "Bridger Herman"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T18:15:00Z",
                        "time_end": "2020-10-26T18:30:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Design Theory",
                        "contributors": [
                            "Francesca Samsel"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T18:30:00Z",
                        "time_end": "2020-10-26T18:45:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Collecting and Creating Artifacts",
                        "contributors": [
                            "Francesca Samsel",
                            "Stephanie Zeller"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T18:45:00Z",
                        "time_end": "2020-10-26T19:00:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Loading in your own Data",
                        "contributors": [
                            "Greg Abram"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T19:00:00Z",
                        "time_end": "2020-10-26T19:15:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Wrap Up Part 1",
                        "contributors": [
                            "Daniel F. Keefe",
                            "Francesca Samsel"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T19:15:00Z",
                        "time_end": "2020-10-26T19:30:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Introduction to Part 2",
                        "contributors": [
                            "Daniel F. Keefe",
                            "Francesca Samsel"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T20:00:00Z",
                        "time_end": "2020-10-26T20:10:00Z"
                    },
                    {
                        "type": "Live with Breakout Rooms",
                        "title": "Breakout rooms for Lines, Color, and Glyphs",
                        "contributors": [
                            "Daniel F. Keefe",
                            "Francesca Samsel",
                            "Bridger Herman",
                            "Kiet Tran",
                            "Stephanie Zeller",
                            "Greg Abram"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T20:10:00Z",
                        "time_end": "2020-10-26T19:40:00Z"
                    },
                    {
                        "type": "Live with Breakout Rooms",
                        "title": "Two Breakout rooms for Making Visualizations with ABR",
                        "contributors": [
                            "Daniel F. Keefe",
                            "Francesca Samsel",
                            "Bridger Herman",
                            "Kiet Tran",
                            "Stephanie Zeller",
                            "Greg Abram"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T19:40:00Z",
                        "time_end": "2020-10-26T20:40:00Z"
                    },
                    {
                        "type": "Live Group",
                        "title": "Discussion/Show your Work",
                        "contributors": [
                            "Daniel F. Keefe",
                            "Francesca Samsel"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T20:40:00Z",
                        "time_end": "2020-10-26T21:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-astrophysics": {
        "event": "Visualization in Astrophysics: Developing New Methods, Discovering Our Universe, Educating the Earth",
        "long_name": "Visualization in Astrophysics: Developing New Methods, Discovering Our Universe, Educating the Earth",
        "event_type": "Workshop",
        "event_description": "We propose a workshop during VIS 2020 titled \u201cVisualization in Astrophysics: Developing New Methods, Discovering Our Universe, Educating the Earth\u201d. The workshop aims to bring together researchers in astronomy and visualization in addressing emerging directions in visualization for large and complex astronomical datasets. It will facilitate close interactions among the attendees with the aim of accelerating the convergence between astronomy and visualization research, specifically in the development of practical visualization techniques and tools for large astronomical data. The workshop aims to have research talks in astronomy and visualization, including talks with planetaria content. It also aims to have moderated discussions and hands-on sessions to discuss potential solutions to mini-challenges involving well-curated astronomy data.",
        "event_url": "http://www.sci.utah.edu/~beiwang/visastro2020/",
        "sessions": [
            {
                "title": "Visualization in Astrophysics: Developing New Methods, Discovering Our Universe, Educating the Earth",
                "session_id": "w-astrophysics",
                "chair": [
                    "Lauren Anderson",
                    "Bei Wang",
                    "Juna Kollmeier"
                ],
                "organizers": [
                    "Bei Wang",
                    "Juna Kollmeier",
                    "Lauren Anderson"
                ],
                "display_start": "2020-10-26T18:00:00Z",
                "time_start": "2020-10-26T18:00:00Z",
                "time_end": "2020-10-26T21:30:00Z",
                "discord_category": "visualization in astrophysics developing new methods discovering our universe educating the earth",
                "discord_channel": "general",
                "discord_channel_id": "768305394464194602",
                "youtube_url": "https://youtu.be/nB36qPnfoGc",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Introduction",
                        "contributors": [
                            "Bei Wang",
                            "Juna Kollmeier",
                            "Lauren Anderson"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T18:00:00Z",
                        "time_end": "2020-10-26T18:10:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Keynote Talk: Alyssa Goodman",
                        "contributors": [
                            "Alyssa Goodman"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T18:10:00Z",
                        "time_end": "2020-10-26T18:40:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Keynote Talk: Alexander Bock",
                        "contributors": [
                            "Alexander Bock"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T18:40:00Z",
                        "time_end": "2020-10-26T19:10:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Invited Talk: Jackie Faherty",
                        "contributors": [
                            "Jackie Faherty"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T19:10:00Z",
                        "time_end": "2020-10-26T19:30:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Invited Talk: Matthew J. Turk",
                        "contributors": [
                            "Matthew J. Turk"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T20:00:00Z",
                        "time_end": "2020-10-26T20:20:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Invited Talk: Michelle Borkin",
                        "contributors": [
                            "Michelle Borkin"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T20:20:00Z",
                        "time_end": "2020-10-26T20:40:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Invited Talk: Angus Forbes and Joseph N. Burchett",
                        "contributors": [
                            "Angus Forbes",
                            "Joseph N. Burchett"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T20:40:00Z",
                        "time_end": "2020-10-26T21:00:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Data Challenges and Discussions",
                        "contributors": [
                            "All"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T21:00:00Z",
                        "time_end": "2020-10-26T21:30:00Z"
                    }
                ]
            }
        ]
    },
    "w-movis": {
        "event": "MoVIS: Information Visualization of Geospatial Networks, Flows and Movement",
        "long_name": "MoVIS: Information Visualization of Geospatial Networks, Flows and Movement",
        "event_type": "Workshop",
        "event_description": "This workshop will bring Geographic Information Scientists to the VIS community and stimulate new insights by providing a forum that connects diverse methods and ideas. The goal of this workshop is to interactively share advances in the visualization of network, flow and movement data for knowledge discovery, exploratory spatial data analysis (ESDA) and decision-making.",
        "event_url": "http://move.geog.ucsb.edu/movis2020/",
        "sessions": [
            {
                "title": "MoVIS: Information Visualization of Geospatial Networks, Flows and Movement",
                "session_id": "w-movis",
                "chair": [
                    "Clio Andris",
                    "Alan MacEachren",
                    "Somayeh Dodge"
                ],
                "organizers": [
                    "Clio Andris",
                    "Somayeh Dodge",
                    "Alan MacEachren"
                ],
                "display_start": "2020-10-26T18:00:00Z",
                "time_start": "2020-10-26T18:00:00Z",
                "time_end": "2020-10-26T21:15:00Z",
                "discord_category": "movis information visualization of geospatial networks flows and movement",
                "discord_channel": "general",
                "discord_channel_id": "768305401804226642",
                "youtube_url": "https://youtu.be/SDM_Xb2YBvk",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Panel",
                        "title": "Hello and Welcome",
                        "contributors": [
                            "Clio Andris",
                            "Somayeh Dodge",
                            "Alan MacEachren"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T18:00:00Z",
                        "time_end": "2020-10-26T18:05:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Keynote",
                        "contributors": [
                            "Jeremy White"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T18:05:00Z",
                        "time_end": "2020-10-26T18:40:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Donut Visualizations for Network-Level and Regional-Level Overview of Spatial Social Networks",
                        "contributors": [
                            "Dipto Sarkar"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T18:40:00Z",
                        "time_end": "2020-10-26T18:50:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visualizing Movement Trajectories in an Immersive Space Time-Cube",
                        "contributors": [
                            "Jorge Wagner"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T18:50:00Z",
                        "time_end": "2020-10-26T19:00:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Affordable Visualization of Massive Movement Datasets \u2013 A Case Study",
                        "contributors": [
                            "Alexander Savelyev"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T19:00:00Z",
                        "time_end": "2020-10-26T19:10:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Exploring Movement Data in Notebook Environments",
                        "contributors": [
                            "Anita Graser"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T19:10:00Z",
                        "time_end": "2020-10-26T19:20:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "A Visualization System for High Dimensional Data Streams using Complex Event Processing",
                        "contributors": [
                            "Arnab Chakrabarti"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T19:20:00Z",
                        "time_end": "2020-10-26T19:30:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Work in progress: Transferring decision boundaries onto a geographic space: agent-rules extracted from movement data using classification trees",
                        "contributors": [
                            "Jugal Patel"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T20:00:00Z",
                        "time_end": "2020-10-26T20:10:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Work in progress: Framing Twitter Trajectory Visualization: A Comparison Between Hurricane Evacuation and Pandemic Lockdown",
                        "contributors": [
                            "Chenxiao Guo"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T20:10:00Z",
                        "time_end": "2020-10-26T20:20:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Work in progress: Using Animation to Visualize Spatio-Temporal Varying COVID-19 Data",
                        "contributors": [
                            "Hanan Samet"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T20:20:00Z",
                        "time_end": "2020-10-26T20:30:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Work in progress: An R Online Tutorial for Visualizing Spatial Social Networks",
                        "contributors": [
                            "Xiaofan Liang"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T20:30:00Z",
                        "time_end": "2020-10-26T20:40:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Discussion and Closing Remarks",
                        "contributors": [
                            "Clio Andris",
                            "Somayeh Dodge",
                            "Alan MacEachren"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-26T20:40:00Z",
                        "time_end": "2020-10-26T21:15:00Z"
                    }
                ]
            }
        ]
    },
    "f-papers": {
        "event": "VIS Full Papers",
        "long_name": "VIS Full Papers",
        "event_type": "Paper Presentations",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Creating & Sharing Visualizations",
                "session_id": "f-papers-create-share-vis",
                "chair": [
                    "Rita Borgo"
                ],
                "organizers": [],
                "display_start": "2020-10-27T17:30:00Z",
                "time_start": "2020-10-27T17:30:00Z",
                "time_end": "2020-10-27T19:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "creating-sharing-visualizations",
                "discord_channel_id": "767121797203755058",
                "youtube_url": "https://youtu.be/ZaJLPQSk-lA",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrdcx4xcPCUAFPLCqKjvjn0p",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "VAST, InfoVis, SciVis Full Papers Joint Opening",
                        "contributors": [
                            "Full Papers Chairs"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T17:30:00Z",
                        "time_end": "2020-10-27T17:45:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning",
                        "contributors": [
                            "Tan Tang"
                        ],
                        "abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.",
                        "time_start": "2020-10-27T18:00:00Z",
                        "time_end": "2020-10-27T18:15:00Z",
                        "uid": "f-info-1091"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Lyra 2: Designing Interactive Visualizations by Demonstration",
                        "contributors": [
                            "Jonathan Zong"
                        ],
                        "abstract": "Recent graphical interfaces offer direct manipulation mechanisms for authoring visualizations, but are largely restricted to static output. To author interactive visualizations, users must instead turn to textual specification, but such approaches impose a higher technical burden. To bridge this gap, we introduce Lyra 2, a system that extends a prior visualization design environment with novel methods for authoring interaction techniques by demonstration. Users perform an interaction (e.g., button clicks, drags, or key presses) directly on the visualization they are editing. The system interprets this performance using a set of heuristics and enumerates suggestions of possible interaction designs. These heuristics account for the properties of the interaction (e.g., target and event type) as well as the visualization (e.g., mark and scale types, and multiple views). Interaction design suggestions are displayed as thumbnails; users can preview and test these suggestions, iteratively refine them through additional demonstrations, and finally apply and customize them via property inspectors. We evaluate our approach through a gallery of diverse examples, and evaluate its usability through a first-use study and via an analysis of its cognitive dimensions.  We find that, in Lyra 2, interaction design by demonstration enables users to rapidly express a wide range of interactive visualizations.",
                        "time_start": "2020-10-27T18:15:00Z",
                        "time_end": "2020-10-27T18:30:00Z",
                        "uid": "f-info-1051"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "StructGraphics: Flexible Visualization Design through Data-Agnostic and Reusable Graphical Structures",
                        "contributors": [
                            "Theophanis Tsandilas"
                        ],
                        "abstract": "Information visualization research has developed powerful systems that enable users to author custom data visualizations without textual programming. These systems can support graphics-driven practices by bridging lazy data-binding mechanisms with vector-graphics editing tools. Yet, despite their expressive power, visualization authoring systems often assume that users want to generate visual representations that they already have in mind rather than explore designs. They also impose a data-to-graphics workflow, where binding data dimensions to graphical properties is a necessary step for generating visualization layouts. In this paper, we introduce StructGraphics, an approach for creating data-agnostic and fully reusable visualization designs. StructGraphics enables designers to construct visualization designs by drawing graphics on a canvas and then structuring their visual properties without relying on a dataset or data schema. In StructGraphics, tabular data structures are derived directly from the structure of the graphics. Later, designers can link these structures with real datasets through a spreadsheet user interface. StructGraphics supports the design and reuse of complex data visualizations by combining graphical property sharing, by-example design specification, and persistent layout constraints. We demonstrate the power of the approach through a gallery of visualization examples and reflect on its strengths and limitations in interaction with graphic designers and data visualization experts.",
                        "time_start": "2020-10-27T18:30:00Z",
                        "time_end": "2020-10-27T18:45:00Z",
                        "uid": "f-info-1011"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "VisCode: Embedding Information in Visualization Images using Encoder-Decoder Network",
                        "contributors": [
                            "Peiying Zhang"
                        ],
                        "abstract": "We present an approach called VisCode for embedding information into visualization images. This technology can implicitly embed data information specified by the user into a visualization while ensuring that the encoded visualization image is not distorted. The VisCode framework is based on a deep neural network. We propose to use visualization images and QR codes data as training data and design a robust deep encoder-decoder network. The designed model considers the salient features of visualization images to reduce the explicit visual loss caused by encoding. To further support large-scale encoding and decoding, we consider the characteristics of information visualization and propose a saliency-based QR code layout algorithm. We present a variety of practical applications of VisCode in the context of information visualization and conduct a comprehensive evaluation of the perceptual quality of encoding, decoding success rate, anti-attack capability, time performance, etc. The evaluation results demonstrate the effectiveness of VisCode.",
                        "time_start": "2020-10-27T18:45:00Z",
                        "time_end": "2020-10-27T19:00:00Z",
                        "uid": "f-info-1157"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Chartem: Reviving Chart Images with Data Embedding",
                        "contributors": [
                            "Bin Zhu"
                        ],
                        "abstract": "In practice, charts are widely stored as bitmap images. Although easily consumed by humans, they are not convenient for other uses. For example, changing the chart style or type or a data value in a chart image practically requires re-creating a new chart completely, which is often a time-consuming and error-prone process. To assist these tasks, many approaches have been proposed to automatically extract information from chart images with computer vision and machine learning techniques. Although they have achieved promising preliminary results, there are still a lot of challenges to overcome in terms of robustness and accuracy. In this paper, we propose a novel alternative approach called Chartem to address this issue directly from the root. Specifically, we design a data embedding schema to encode a significant amount of information into the background of a chart image without interfering human perception of the chart. The embedded information, when extracted from the image, can enable a variety of visualization applications to reuse or repurpose chart images. To evaluate the effectiveness of Chartem, we conduct a user study and performance experiments on Chartem embedding and extraction algorithms. We further present several prototype applications to demonstrate the utility of Chartem.",
                        "time_start": "2020-10-27T19:00:00Z",
                        "time_end": "2020-10-27T19:15:00Z",
                        "uid": "f-info-1149"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Dataless Sharing of Interactive Visualization",
                        "contributors": [
                            "Mohammad Raji"
                        ],
                        "abstract": "Interactive visualization has become a powerful insight-revealing medium. However, the close dependency of interactive visualization on its data inhibits its shareability. Users have to choose between the two extremes of (i) sharing non-interactive dataless formats such as images and videos, or (ii) giving access to their data and software to others with no control over how the data will be used. In this work, we fill the gap between the two extremes and present a new system, called Loom. Loom captures interactive visualizations as standalone dataless objects. Users can interact with Loom objects as if they still have the original software and data that created those visualizations. Yet, Loom objects are completely independent and can therefore be shared online without requiring the data or the visualization software. Loom objects are efficient to store and use, and provide privacy preserving mechanisms. We demonstrate Loom's efficacy with examples of scientific visualization using Paraview, information visualization using Tableau, and journalistic visualization from New York Times.",
                        "time_start": "2020-10-27T19:15:00Z",
                        "time_end": "2020-10-27T19:30:00Z",
                        "uid": "f-tvcg-2019020058"
                    }
                ]
            },
            {
                "title": "Libraries, Toolkits & Systems",
                "session_id": "f-papers-lib-tool-sys",
                "chair": [
                    "Dominik Moritz"
                ],
                "organizers": [],
                "display_start": "2020-10-27T18:00:00Z",
                "time_start": "2020-10-27T18:00:00Z",
                "time_end": "2020-10-27T19:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "libraries-toolkits-systems",
                "discord_channel_id": "767122067891552286",
                "youtube_url": "https://youtu.be/nJnE7mxbA2k",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrct9PRizs-7aQWvgrIdvVLQ",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "VisConnect: Distributed Event Synchronization for Collaborative Visualization",
                        "contributors": [
                            "Michail Schwab"
                        ],
                        "abstract": "Tools and interfaces are increasingly expected to be synchronous and distributed to accommodate remote collaboration. Yet, adoption of these techniques for data visualization is low partly because development is difficult: existing collaboration software systems either do not support simultaneous interaction or require expensive redevelopment of existing visualizations. We contribute VisConnect: a web-based synchronous distributed collaborative visualization system that supports most web-based SVG data visualizations, balances system safety with responsiveness, and supports simultaneous interaction from many collaborators. VisConnect works with existing visualization implementations with little-to-no code changes by synchronizing low-level JavaScript events across clients such that visualization updates proceed transparently across clients. This is accomplished via a peer-to-peer system that establishes consensus among clients on the per-element sequence of events, and uses a lock service to grant access over elements to clients. We contribute collaborative extensions of traditional visualization interaction techniques, such as drag, brush, and lasso, and discuss different strategies for collaborative visualization interactions. To demonstrate the utility of VisConnect, we present novel examples of collaborative visualizations in the healthcare domain, remote collaboration with annotation, and show in an education case study for e-learning with 22 participants that students found the ability to remotely collaborate on class activities helpful and enjoyable for understanding concepts.  A free copy of this paper and source code are available on OSF at osf.io/639fe and at visconnect.us.",
                        "time_start": "2020-10-27T18:00:00Z",
                        "time_end": "2020-10-27T18:15:00Z",
                        "uid": "f-info-1183"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Generic Framework and Library for Exploration of Small Multiples through Interactive Piling",
                        "contributors": [
                            "Fritz Lekschas"
                        ],
                        "abstract": "Small multiples are miniature representations of visual information used generically across many domains. Handling large numbers of small multiples imposes challenges on many analytic tasks like inspection, comparison, navigation, or annotation. To address these challenges, we developed a framework and implemented a library called Piling.js for designing interactive piling interfaces. Based on the piling metaphor, such interfaces afford flexible organization, exploration, and comparison of large numbers of small multiples by interactively aggregating visual objects into piles. Based on a systematic analysis of previous work, we present a structured design space to guide the design of visual piling interfaces. To enable designers to efficiently build their own visual piling interfaces, Piling.js provides a declarative interface to avoid having to write low-level code and implements common aspects of the design space. An accompanying GUI additionally supports the dynamic configuration of the piling interface. We demonstrate the expressiveness of Piling.js with examples from machine learning, immunofluorescence microscopy, genomics, and public health.",
                        "time_start": "2020-10-27T18:15:00Z",
                        "time_end": "2020-10-27T18:30:00Z",
                        "uid": "f-info-1273"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries",
                        "contributors": [
                            "Arpit Narechania"
                        ],
                        "abstract": "Natural language interfaces (NLIs) have shown great promise for visual data analysis, allowing people to flexibly specify and interact with visualizations. However, developing visualization NLIs remains a challenging task, requiring low-level implementation of natural language processing (NLP) techniques as well as knowledge of visual analytic tasks and visualization design. We present NL4DV, a toolkit for natural language-driven data visualization. NL4DV is a Python package that takes as input a tabular dataset and a natural language query about that dataset. In response, the toolkit returns an analytic specification modeled as a JSON object containing data attributes, analytic tasks, and a list of Vega-Lite specifications relevant to the input query. In doing so, NL4DV aids visualization developers who may not have a background in NLP, enabling them to create new visualization NLIs or incorporate natural language input within their existing systems. We demonstrate NL4DV's usage and capabilities through four examples: 1) rendering visualizations using natural language in a Jupyter notebook, 2) developing a NLI to specify and edit Vega-Lite charts, 3) recreating data ambiguity widgets from the DataTone system, and 4) incorporating speech input to create a multimodal visualization system.",
                        "time_start": "2020-10-27T18:30:00Z",
                        "time_end": "2020-10-27T18:45:00Z",
                        "uid": "f-info-1364"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "P6: A Declarative Language for Integrating Machine Learning in Visual Analytics",
                        "contributors": [
                            "Kelvin Li"
                        ],
                        "abstract": "We present P6, a declarative language for building high performance visual analytics systems through its support for specifying and integrating machine learning and interactive visualization methods. As data analysis methods based on machine learning and artificial intelligence continue to advance, a visual analytics solution can leverage these methods for better exploiting large and complex data. However, integrating machine learning methods with interactive visual analysis is challenging. Existing declarative programming libraries and toolkits for visualization lack support for coupling machine learning methods. By providing a declarative language for visual analytics, P6 can empower more developers to create visual analytics applications that combine machine learning and visualization methods for data analysis and problem-solving. Through a variety of example applications, we demonstrate P6's capabilities and show the benefits of using declarative specifications to build visual analytics systems. We also identify and discuss the research opportunities and challenges for declarative visual analytics.",
                        "time_start": "2020-10-27T18:45:00Z",
                        "time_end": "2020-10-27T19:00:00Z",
                        "uid": "f-vast-1261"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "PipelineProfiler: A Visual Analytics Tool for the Exploration of AutoML Pipelines",
                        "contributors": [
                            "Jorge Piazentin Ono"
                        ],
                        "abstract": "In recent years, a wide variety of automated machine learning (AutoML) methods have been proposed to generate end-to-end ML pipelines. While these techniques facilitate the creation of models, given their black-box nature, the complexity of the underlying algorithms, and the large number of pipelines they derive, they are difficult for developers to debug. It is also challenging for machine learning experts to select an AutoML system that is well suited for a given problem. In this paper, we present the PipelineProfiler, an interactive visualization tool that allows the exploration and comparison of the solution space of machine learning (ML) pipelines produced by AutoML systems. PipelineProfiler is integrated with Jupyter Notebook and can be combined with common data science tools to enable a rich set of analyses of the ML pipelines, providing users a better understanding of the algorithms that generated them as well as insights into how they can be improved. We demonstrate the utility of our tool through use cases where PipelineProfiler is used to better understand and improve a real-world AutoML system. Furthermore, we validate our approach by presenting a detailed analysis of a think-aloud experiment with six data scientists who develop and evaluate AutoML tools.",
                        "time_start": "2020-10-27T19:00:00Z",
                        "time_end": "2020-10-27T19:15:00Z",
                        "uid": "f-vast-1097"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Kyrix-S: Authoring Scalable Scatterplot Visualizations of Big Data",
                        "contributors": [
                            "Wenbo Tao"
                        ],
                        "abstract": "Static scatterplots often suffer from the overdraw problem on big datasets where object overlap causes undesirable visual clutter. The use of zooming in scatterplots can help alleviate this problem. With multiple zoom levels, more screen real estate is available, allowing objects to be placed in a less crowded way. We call this type of visualization scalable scatterplot visualizations, or SSV for short. Despite the potential of SSVs, existing systems and toolkits fall short in supporting the authoring of SSVs due to three limitations. First, many systems have limited scalability, assuming that data fits in the memory of one computer. Second, too much developer work, e.g., using custom code to generate mark layouts or render objects, is required. Third, many systems focus on only a small subset of the SSV design space (e.g. supporting a specific type of visual marks). To address these limitations, we have developed Kyrix-S, a system for easy authoring of SSVs at scale. Kyrix-S derives a declarative grammar that enables specification of a variety of SSVs in a few tens of lines of code, based on an existing survey of scatterplot tasks and designs. The declarative grammar is supported by a distributed layout algorithm which automatically places visual marks onto zoom levels. We store data in a multi-node database and use multi-node spatial indexes to achieve interactive browsing of large SSVs. Extensive experiments show that 1) Kyrix-S enables interactive browsing of SSVs of billions of objects, with response times under 500ms and 2) Kyrix-S achieves 4X-9X reduction in specification compared to a state-of-the-art authoring system.",
                        "time_start": "2020-10-27T19:15:00Z",
                        "time_end": "2020-10-27T19:30:00Z",
                        "uid": "f-info-1015"
                    }
                ]
            },
            {
                "title": "Perception & Color",
                "session_id": "f-papers-perception-color",
                "chair": [
                    "Lyn Bartram"
                ],
                "organizers": [],
                "display_start": "2020-10-27T18:00:00Z",
                "time_start": "2020-10-27T18:00:00Z",
                "time_end": "2020-10-27T19:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "perception-color",
                "discord_channel_id": "767122231125606410",
                "youtube_url": "https://youtu.be/B8Kum6RehbE",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrfhmC7WPtAnoRdj-kmqOep9",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Semantic Discriminability for Visual Communication",
                        "contributors": [
                            "Karen Schloss"
                        ],
                        "abstract": "To interpret information visualizations, observers must determine how visual features map onto concepts. First and foremost this ability depends on perceptual discriminability; e.g., observers must be able to see the difference between different colors for those colors to communicate different meanings. However, the ability to interpret visualizations also depends on semantic discriminabilty, the degree to which observers can differentiate which visual features map to each concept in the encoding system without help from verbal cues such as legends or labels. Previous evidence suggested that semantic discriminability was higher for a color-coding system that maximized semantic distance (maximizing association strength between assigned colors and concepts while minimizing association strength between unassigned colors and concepts), compared with a system that maximized color-concept association strength. However, increasing semantic distance also resulted in increased perceptual distance so it is unclear which factor was responsible for improved performance. In the present study, we conducted two experiments that tested for independent effects of semantic distance and perceptual distance on semantic discriminability of bar graph data visualizations. Perceptual distance was chosen to be large enough to ensure colors were more than just noticeably different. We found that increasing semantic distance improved performance, independent of variation in perceptual distance, and when these two factors were uncorrelated, responses were dominated by semantic distance. These results have implications for navigating trade-offs in color palette design optimization for visual communication.",
                        "time_start": "2020-10-27T18:00:00Z",
                        "time_end": "2020-10-27T18:15:00Z",
                        "uid": "f-info-1119"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Rainbows Revisited: Modeling Effective Colormap Design for Graphical Inference",
                        "contributors": [
                            "Khairi Reda"
                        ],
                        "abstract": "Color mapping is a foundational technique for visualizing scalar data. Prior literature offers guidelines for effective colormap design, such as emphasizing luminance variation while limiting changes in hue. However, empirical studies of color are largely focused on perceptual tasks. This narrow focus inhibits our understanding of how generalizable these guidelines are, particularly to tasks like visual inference that require synthesis and judgement across multiple percepts. Furthermore, the emphasis on traditional ramp designs (e.g., sequential or diverging) may sideline other key metrics or design strategies. We study how a cognitive metric---color name variation---impacts people's ability to make model-based judgments. In two graphical inference experiments, participants saw a series of color-coded scalar fields sampled from different models and assessed the relationships between these models. Contrary to conventional guidelines, participants were more accurate when viewing colormaps that cross a variety of uniquely nameable colors. We modeled participants' performance using this metric and found that it provides a better fit to the experimental data than do existing design principles. Our findings indicate cognitive advantages for colorful maps like rainbow, which exhibit high color categorization, despite their traditionally undesirable perceptual properties. We also found no evidence that color categorization would lead observers to infer false data features. Our results provide empirically grounded metrics for predicting a colormap's performance and suggest alternative guidelines for designing new quantitative colormaps to support inference. The data and materials for this paper are available at: https://osf.io/tck2r/",
                        "time_start": "2020-10-27T18:15:00Z",
                        "time_end": "2020-10-27T18:30:00Z",
                        "uid": "f-info-1036"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Testing Environment for Continuous Colormaps",
                        "contributors": [
                            "Pascal Nardini"
                        ],
                        "abstract": "Many computer science disciplines (e.g., combinatorial optimization, natural language processing, and information retrieval) use standard or established test suites for evaluating algorithms. In visualization, similar approaches have been adopted in some areas (e.g., volume visualization), while user testimonies and empirical studies have been the dominant means of evaluation in most other areas, such as designing colormaps. In this paper, we propose to establish a test suite for evaluating the design of colormaps. With such a suite, the users can observe the effects when different continuous colormaps are applied to planar scalar fields that may exhibit various characteristic features, such as jumps, local extrema, ridge or valley lines, different distributions of scalar values, different gradients, different signal frequencies, different levels of noise, and so on. The suite also includes an expansible collection of real-world data sets including the most popular data for colormap testing in the visualization literature. The test suite has been integrated into a web-based application for creating continuous colormaps (https://ccctool.com/), facilitating close inter-operation between design and evaluation processes. This new facility complements traditional evaluation methods such as user testimonies and empirical studies.",
                        "time_start": "2020-10-27T18:30:00Z",
                        "time_end": "2020-10-27T18:45:00Z",
                        "uid": "f-scivis-1012"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Truth or Square: Aspect Ratio Biases Recall of Position Encodings",
                        "contributors": [
                            "Cristina R. Ceja"
                        ],
                        "abstract": "Bar charts are among the most frequently used visualizations, in part because their position encoding leads them to convey data values precisely. Yet reproductions of single bars or groups of bars within a graph can be biased. Curiously, some previous work found that this bias resulted in an overestimation of reproduced data values, while other work found an underestimation. Across three empirical studies, we offer an explanation for these conflicting findings: this discrepancy is a consequence of the differing aspect ratios of the tested bar marks. Viewers are biased to remember a bar mark as being more similar to a prototypical square, leading to an overestimation of bars with a wide aspect ratio, and an underestimation of bars with a tall aspect ratio. Experiments 1 and 2 showed that the aspect ratio of the bar marks indeed influenced the direction of this bias. Experiment 3 confirmed that this pattern of misestimation bias was present for reproductions from memory, suggesting that this bias may arise when comparing values across sequential displays or views. We describe additional visualization designs that might be prone to this bias beyond bar graphs (e.g., Mekko charts and treemaps), and speculate that other visual channels might hold similar biases toward prototypical values.",
                        "time_start": "2020-10-27T18:45:00Z",
                        "time_end": "2020-10-27T19:00:00Z",
                        "uid": "f-info-1014"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "No mark is an island: Precision and category repulsion biases in data reproductions",
                        "contributors": [
                            "Caitlyn McColeman"
                        ],
                        "abstract": "Data visualization is powerful in large part because it facilitates visual extraction of values. Yet, existing measures of perceptual precision for data channels (e.g., position, length, orientation, etc.) are based largely on verbal reports of ratio judgments between two values (e.g., Cleveland & McGill 1984). Verbal report conflates multiple sources of error beyond actual visual precision, introducing a ratio computation between these values and a requirement to translate that ratio to a verbal number. Here we observe raw measures of precision by eliminating both ratio computations and verbal reports; we simply ask participants to reproduce marks (a single bar or dot) to match a previously seen one. We manipulated whether the mark was initially presented (and later drawn) alone, paired with a reference (e.g. a second '100%' bar also present at test, or a y-axis for the dot), or integrated with the reference (merging that reference bar into a stacked bar graph, or placing the dot directly on the axis). Reproductions of smaller values were overestimated, and larger values were underestimated, suggesting systematic memory biases. Average reproduction error was around 10% of the actual value, regardless of whether the reproduction was done on a common baseline with the original. In the reference and (especially) the integrated conditions, responses were repulsed from an implicit midpoint of the reference mark, such that values above 50% were overestimated, and values below 50% were underestimated. This reproduction paradigm may serve within a new suite of more fundamental measures of the precision of graphical perception.",
                        "time_start": "2020-10-27T19:00:00Z",
                        "time_end": "2020-10-27T19:15:00Z",
                        "uid": "f-info-1103"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Revealing Perceptual Proxies with Adversarial Examples",
                        "contributors": [
                            "Brian Ondov"
                        ],
                        "abstract": "Data visualizations convert numbers into visual marks so that our visual system can extract data from an image instead of raw numbers. Clearly, the visual system does not compute these values as a computer would, e.g., by calculating an arithmetic mean or a correlation. Instead, it extracts these patterns using perceptual proxies; heuristic shortcuts of the visual marks, such as a center of mass or a shape envelope. Understanding which proxies people actually use would lead to more effective visualizations. We present the results of a series of crowdsourced experiments that measure how powerfully a set of candidate proxies can explain human performance when comparing the mean and range of pairs of data series presented as bar charts. We generated datasets where the correct answer\u2014the series with the higher arithmetic mean or range\u2014was pitted against an \u201cadversarial\u201d series that should be seen as higher if the viewer uses a particular candidate proxy. Using a staircase design, we sought metrics of how strongly each adversarial proxy could drive viewers to answer incorrectly, yielding evidence for whether that proxy is consistent with the viewer's actual practice. We then use hierarchical modeling to investigate whether different individuals may choose different proxies. Finally, we attempt to construct adversarial datasets from scratch, using an iterative crowdsourcing procedure to perform black-box optimization.",
                        "time_start": "2020-10-27T19:15:00Z",
                        "time_end": "2020-10-27T19:30:00Z",
                        "uid": "f-info-1225"
                    }
                ]
            },
            {
                "title": "Vulnerabilities in Machine Learning",
                "session_id": "f-papers-ml-vuln",
                "chair": [
                    "Polo Chau"
                ],
                "organizers": [],
                "display_start": "2020-10-27T18:00:00Z",
                "time_start": "2020-10-27T18:00:00Z",
                "time_end": "2020-10-27T19:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "vulnerabilities-in-machine-learning",
                "discord_channel_id": "767122237819715645",
                "youtube_url": "https://youtu.be/BtxxhKdO6Ms",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrevyzMP-tQCr9Hqn78D7yLs",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics",
                        "contributors": [
                            "Tiankai Xie"
                        ],
                        "abstract": "Graph mining plays a pivotal role across a number of disciplines, and a variety of algorithms have been developed to answer who/what type questions. For example, what items shall we recommend to a given user on an e-commerce platform? The answers to such questions are typically returned in the form of a ranked list, and graph-based ranking methods are widely used in industrial information retrieval settings. However, these ranking algorithms have a variety of sensitivities, and even small changes in rank can lead to vast reductions in product sales and page hits. As such, there is a need for tools and methods that can help model developers and analysts explore the sensitivities of graph ranking algorithms with respect to perturbations within the graph structure. In this paper, we present a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm by performing perturbation-based what-if analysis. We demonstrate our framework through three case studies inspecting the sensitivity of two classic graph-based ranking algorithms (PageRank and HITS) as applied to rankings in political news media and social networks.",
                        "time_start": "2020-10-27T18:00:00Z",
                        "time_end": "2020-10-27T18:15:00Z",
                        "uid": "f-vast-1081"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visual Analysis of Discrimination in Machine Learning",
                        "contributors": [
                            "Qianwen Wang"
                        ],
                        "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory?\nIn this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.",
                        "time_start": "2020-10-27T18:15:00Z",
                        "time_end": "2020-10-27T18:30:00Z",
                        "uid": "f-info-1245"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
                        "contributors": [
                            "David Borland"
                        ],
                        "abstract": "The collection and visual analysis of large-scale data from complex systems, such as electronic health records or clickstream data, has become increasingly common across a wide range of industries.  This type of retrospective visual analysis, however, is prone to a variety of selection bias effects, especially for high-dimensional data where only a subset of dimensions is visualized at any given time. The risk of selection bias is even higher when analysts dynamically apply filters or perform grouping operations during ad hoc analyses. These bias effects threatens the validity and generalizability of insights discovered during visual analysis as the basis for decision making. Past work has focused on bias transparency, helping users understand when selection bias may have occurred. However, countering the effects of selection bias via bias mitigation is typically left for the user to accomplish as a separate process. Dynamic reweighting (DR) is a novel computational approach to selection bias mitigation that helps users craft bias-corrected visualizations.  This paper describes the DR workflow, introduces key DR visualization designs, and presents statistical methods that support the DR process.  Use cases from the medical domain, as well as findings from domain expert user interviews, are also reported.",
                        "time_start": "2020-10-27T18:30:00Z",
                        "time_end": "2020-10-27T18:45:00Z",
                        "uid": "f-vast-1147"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "ConfusionFlow: A model-agnostic visualization for temporal analysis of classifier confusion",
                        "contributors": [
                            "Andreas Hinterreiter"
                        ],
                        "abstract": "Classifiers are among the most widely used supervised machine learning algorithms. Many classification models exist, and choosing the right one for a given task is difficult. During model selection and debugging, data scientists need to assess classifiers' performances, evaluate their learning behavior over time, and compare different models. Typically, this analysis is based on single-number performance measures such as accuracy. A more detailed evaluation of classifiers is possible by inspecting class errors. The confusion matrix is an established way for visualizing these class errors, but it was not designed with temporal or comparative analysis in mind. More generally, established performance analysis systems do not allow a combined temporal and comparative analysis of class-level information. To address this issue, we propose ConfusionFlow, an interactive, comparative visualization tool that combines the benefits of class confusion matrices with the visualization of performance characteristics over time. ConfusionFlow is model-agnostic and can be used to compare performances for different model types, model architectures, and/or training and test datasets. We demonstrate the usefulness of ConfusionFlow in a case study on instance selection strategies in active learning. We further assess the scalability of ConfusionFlow and present a use case in the context of neural network pruning.",
                        "time_start": "2020-10-27T18:45:00Z",
                        "time_end": "2020-10-27T19:00:00Z",
                        "uid": "f-tvcg-2019090333"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data",
                        "contributors": [
                            "Xuemeng Wang"
                        ],
                        "abstract": "Time-series data is widely studied in various scenarios, like weather forecast, stock market, customer behavior analysis. To comprehensively learn about the dynamic environments, it is necessary to comprehend features from multiple data sources. This paper proposes a novel visual analysis approach for detecting and analyzing concept drifts from multi-sourced time-series.  We propose a visual detection scheme for discovering concept drifts from multiple sourced time-series based on prediction models. We design a drift level index to depict the dynamics, and a consistency judgment model to justify whether the concept drifts from various sources are consistent. Our integrated visual interface, ConceptExplorer, facilitates visual exploration, extraction, understanding, and comparison of concepts and concept drifts from multi-source time-series data.  We conduct three case studies and expert interviews to verify the effectiveness of our approach.",
                        "time_start": "2020-10-27T19:00:00Z",
                        "time_end": "2020-10-27T19:15:00Z",
                        "uid": "f-vast-1060"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Diagnosing Concept Drift with Visual Analytics",
                        "contributors": [
                            "Weikai Yang"
                        ],
                        "abstract": "Concept drift is a phenomenon in which the distribution of a data stream changes over time in unforeseen ways, causing prediction models built on historical data to become inaccurate. While a variety of automated methods have been developed to identify when concept drift occurs, there is limited support for analysts who need to understand and correct their models when drift is detected. In this paper, we present a visual analytics method, DriftVis, to support model builders and analysts in the identification and correction of concept drift in streaming data. DriftVis combines a distribution-based drift detection method with a streaming scatterplot to support the analysis of drift caused by the distribution changes of data streams and to explore the impact of these changes on the model's accuracy. A quantitative experiment and two case studies on weather prediction and text classification have been conducted to demonstrate our proposed tool and illustrate how visual analytics can be used to support the detection, examination, and correction of concept drift.",
                        "time_start": "2020-10-27T19:15:00Z",
                        "time_end": "2020-10-27T19:30:00Z",
                        "uid": "f-vast-1087"
                    }
                ]
            },
            {
                "title": "Making Sense of Text",
                "session_id": "f-papers-sense-text",
                "chair": [
                    "Florian Heimerl"
                ],
                "organizers": [],
                "display_start": "2020-10-28T14:00:00Z",
                "time_start": "2020-10-28T14:00:00Z",
                "time_end": "2020-10-28T15:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "making-sense-of-text",
                "discord_channel_id": "767561647702278195",
                "youtube_url": "https://youtu.be/TKXfu7k_ylQ",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrd1EH-tHYrmyrdjweBold-E",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "TransVis: Integrated Distant and Close Reading of Othello Translations",
                        "contributors": [
                            "Robert Laramee"
                        ],
                        "abstract": "Studying variation among time-evolved translations is a valuable research area for cultural heritage. Understanding how and why translations vary reveals cultural, ideological, and even political influences on literature as well as author relations. In this paper, we introduce a novel integrated visual application to support distant and close reading of a collection of Othello translations. We present a new interactive application that provides an alignment overview of all the translations and their correspondences in parallel with smooth zooming and panning capability to integrate distant and close reading within the same view. We provide a range of filtering and selection options to customize the alignment overview as well as focus on specific subsets. Selection and filtering are responsive to expert user preferences and update the analytical text metrics interactively. Also, we introduce a customized view for close reading which preserves the history of selections and the alignment overview state and enables backtracing and re-examining them. Finally, we present a new Term-Level Comparisons view (TLC) to compare and convey relative term weighting in the context of an alignment. Our visual design is guided by, used and evaluated by a domain expert specialist in German translations of Shakespeare.",
                        "time_start": "2020-10-28T14:00:00Z",
                        "time_end": "2020-10-28T14:15:00Z",
                        "uid": "f-tvcg-2019080291"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visual Analysis of Argumentation in Essays",
                        "contributors": [
                            "Dora Kiesel"
                        ],
                        "abstract": "This paper presents a visual analytics system for exploring, analyzing and comparing argument structures in essay corpora. We provide an overview of the corpus by a list of ArguLines which represent the argument units of each essay by a sequence of glyphs. Each glyph encodes the stance, the depth and the relative position of an argument unit. The overview can be ordered in various ways to reveal patterns and outliers. Subsets of essays can be selected and analyzed in detail using the Argument Unit Occurrence Tree which aggregates the argument structures using hierarchical histograms. This hierarchical view facilitates the estimation of statistics and trends concerning the progression of the argumentation in the essays. It also provides insights into the commonalities and differences between selected subsets. The text view is the necessary textual basis to verify conclusions from the other views and the annotation process. Linking the views and interaction techniques for visual filtering, studying the evolution of stance within a subset of essays and scrutinizing the order of argumentative units enable a deep analysis of essay corpora. Our expert reviews confirmed the utility of the system and revealed detailed and previously unknown information about the argumentation in our sample corpus.",
                        "time_start": "2020-10-28T14:15:00Z",
                        "time_end": "2020-10-28T14:30:00Z",
                        "uid": "f-vast-1074"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Survey of Text Alignment Visualization",
                        "contributors": [
                            "Tariq Yousef"
                        ],
                        "abstract": "Text alignment is one of the fundamental techniques text-related domains like natural language processing, computational linguistics, and digital humanities. It compares two or more texts with each other aiming to find similar textual patterns, or to estimate in general how different or similar the texts are. Visualizing alignment results is an essential task, because it helps researchers getting a comprehensive overview of individual findings and the overall pattern structure. Different approaches have been developed to visualize and help making sense of these patterns depending on text size, alignment methods, and, most importantly, the underlying research tasks demanding for alignment. On the basis of those tasks, we reviewed existing text alignment visualization approaches, and discuss their advantages and drawbacks. We finally derive design implications and shed light on related future challenges.",
                        "time_start": "2020-10-28T14:30:00Z",
                        "time_end": "2020-10-28T14:45:00Z",
                        "uid": "f-info-1261"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Virtual Reality Memory Palace Variant Aids Knowledge Retrieval from Scholarly Articles",
                        "contributors": [
                            "Fumeng Yang"
                        ],
                        "abstract": "We present exploratory research of virtual reality techniques and mnemonic devices to assist in retrieving knowledge from scholarly articles. We used abstracts of scientific publications to represent scientific knowledge in scholarly articles; participants were asked to read, remember, and retrieve knowledge from a set of abstracts. We conducted an experiment to compare participants' recall and recognition performance in three different conditions: a control condition without a pre-specified strategy to test baseline individual memory ability, a condition using an image-based variant of a mnemonic called a \u201cmemory palace,\u201d and a condition using a virtual reality-based variant of a memory palace. Our analyses show that using a virtual reality-based memory palace variant greatly increased the amount of knowledge retrieved and retained over the baseline, and it shows a moderate improvement over the other image-based memory palace variant. Anecdotal feedback from participants suggested that personalizing a memory palace variant would be appreciated. Our results support the value of virtual reality for some high-level cognitive tasks and help improve future applications of",
                        "time_start": "2020-10-28T14:45:00Z",
                        "time_end": "2020-10-28T15:00:00Z",
                        "uid": "f-tvcg-2019080278"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "ArchiText: Interactive Hierarchical Topic Modeling",
                        "contributors": [
                            "Hannah Kim"
                        ],
                        "abstract": "Human-in-the-loop topic modeling allows users to explore and steer the process to produce better quality topics that align with their needs. When integrated into visual analytic systems, many existing automated topic modeling algorithms are given interactive parameters to allow users to tune or adjust them. However, this has limitations when the algorithms cannot be easily adapted to changes, and it is difficult to realize interactivity closely supported by underlying algorithms. Instead, we emphasize the concept of tight integration, which advocates for the need to co-develop interactive algorithms and interactive visual analytic systems in parallel to allow flexibility and scalability. In this paper, we describe design goals for efficiently and effectively executing the concept of tight integration among computation, visualization, and interaction for hierarchical topic modeling of text data. We propose computational base operations for interactive tasks to achieve the design goals. To instantiate our concept, we present ArchiText, a prototype system for interactive hierarchical topic modeling, which offers fast, flexible, and algorithmically valid analysis via tight integration. Utilizing interactive hierarchical topic modeling, our technique lets users generate, explore, and flexibly steer hierarchical topics to discover more informed topics and their document memberships.",
                        "time_start": "2020-10-28T15:00:00Z",
                        "time_end": "2020-10-28T15:15:00Z",
                        "uid": "f-tvcg-2019030092"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models",
                        "contributors": [
                            "Joseph DeRose",
                            "Jiayao Wang"
                        ],
                        "abstract": "Advances in language modeling have led to the development of deep attention-based models that are performant across a wide variety of natural language processing (NLP) problems. These language models are typified by a pre-training process on large unlabeled text corpora and subsequently fine-tuned for specific tasks. Although considerable work has been devoted to understanding the attention mechanisms of pre-trained models, it is less understood how a model's attention mechanisms change when trained for a target NLP task. In this paper, we propose a visual analytics approach to understanding fine-tuning in attention-based language models. Our visualization, Attention Flows, is designed to support users in querying, tracing, and comparing attention within layers, across layers, and amongst attention heads in Transformer-based language models. To help users gain insight on how a classification decision is made, our design is centered on depicting classification-based attention at the deepest layer and how attention from prior layers flows throughout words in the input. Attention Flows helps users understand such attention propagation for a single model, as well as for multiple models, to visually compare similarities and differences between pre-trained and fine-tuned models. We use Attention Flows to study attention mechanisms in various sentence understanding tasks and highlight how attention evolves to address the nuances of solving these tasks.",
                        "time_start": "2020-10-28T15:15:00Z",
                        "time_end": "2020-10-28T15:30:00Z",
                        "uid": "f-vast-1187"
                    }
                ]
            },
            {
                "title": "Sense & Explainabilities",
                "session_id": "f-papers-sense-explain",
                "chair": [
                    "Nadia Boukhelifa"
                ],
                "organizers": [],
                "display_start": "2020-10-28T14:00:00Z",
                "time_start": "2020-10-28T14:00:00Z",
                "time_end": "2020-10-28T15:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "sense-explainabilities",
                "discord_channel_id": "767561656041472020",
                "youtube_url": "https://youtu.be/TuLFY3Y7yHU",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojreDU0Xe1Pm3fhbxMfoE2Smq",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "OoDAnalyzer: Interactive Analysis of Out-of-Distribution Samples",
                        "contributors": [
                            "Changjian Chen"
                        ],
                        "abstract": "One major cause of performance degradation in predictive models is that the test samples are not well covered by the training data. Such not well-represented samples are called OoD samples. In this paper, we propose OoDAnalyzer, a visual analysis approach for interactively identifying OoD samples and explaining them in context. Our approach integrates an ensemble OoD detection method and a grid-based visualization. The detection method is improved from deep ensembles by combining more features with algorithms in the same family. To better analyze and understand the OoD samples in context, we have developed a novel kNN-based grid layout algorithm motivated by Hall's theorem. The algorithm approximates the optimal layout and has O(kN^2) time complexity, faster than the grid layout algorithm with overall best performance but O(N^3) time complexity. Quantitative evaluation and case studies were performed on several datasets to demonstrate the effectiveness and usefulness of OoDAnalyzer.",
                        "time_start": "2020-10-28T14:00:00Z",
                        "time_end": "2020-10-28T14:15:00Z",
                        "uid": "f-tvcg-2019080258"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "HypoML: Visual Analysis for Hypothesis-based Evaluation of Machine Learning Models",
                        "contributors": [
                            "Weikai Yang"
                        ],
                        "abstract": "In this paper, we present a visual analytics tool for enabling hypothesis-based evaluation of machine learning (ML) models. We describe a novel ML-testing framework that combines the traditional statistical hypothesis testing (commonly used in empirical research) with logical reasoning about the conclusions of multiple hypotheses. The framework defines a controlled configuration for testing a number of hypotheses as to whether and how some extra information about a \u201cconcept\u201d or \u201cfeature\u201d may benefit or hinder an ML model. Because reasoning multiple hypotheses is not always straightforward, we provide HypoML as a visual analysis tool, with which, the multi-thread testing results are first transformed to analytical results using statistical and logical inferences, and then to a visual representation for rapid observation of the conclusions and the logical flow between the testing results and hypotheses. We have applied HypoML to a number of hypothesized concepts, demonstrating the intuitive and explainable nature of the visual analysis.",
                        "time_start": "2020-10-28T14:15:00Z",
                        "time_end": "2020-10-28T14:30:00Z",
                        "uid": "f-vast-1078"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Explainable Matrix - Visualization for Global and Local Interpretability of Random Forest Classification Ensembles",
                        "contributors": [
                            "M\u00e1rio Popolin Neto"
                        ],
                        "abstract": "Over the past decades, classification models have proven to be essential machine learning tools given their potential and applicability in various domains. In these years, the north of the majority of the researchers had been to improve quality metrics, notwithstanding the lack of information about models' decisions such metrics convey. Recently, this paradigm has shifted, and strategies beyond tables and numbers to assist in interpreting models' decisions are increasing in importance. Part of this trend, visualization techniques have been extensively used to support the interpretability of classification models, with a significant focus on rule-based techniques.  Despite the advances, the existing approaches present limitations in terms of visual scalability, and the visualization of large and complex models, such as the ones produced by the Random Forest (RF) technique, remains a challenge. In this paper, we propose Explainable Matrix (ExMatrix), a novel visualization method for RF interpretability that can handle models with massive quantities of rules. It employs a simple yet powerful matrix-like visual metaphor, where rows are rules, columns are features, and cells are rules predicates, enabling the analysis of entire models and auditing classification results. ExMatrix applicability is confirmed via different examples, showing how it can be used in practice to promote RF models interpretability.",
                        "time_start": "2020-10-28T14:30:00Z",
                        "time_end": "2020-10-28T14:45:00Z",
                        "uid": "f-vast-1289"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models",
                        "contributors": [
                            "Furui Cheng"
                        ],
                        "abstract": "With machine learning models being increasingly applied to various decision-making scenarios, people have spent growing efforts to make machine learning models more transparent and explainable. Among various explanation techniques, counterfactual explanations have the advantages of being human-friendly and actionable \u2014 a counterfactual explanation tells the user how to gain the desired prediction with minimal changes to the input. Besides, counterfactual explanations can also serve as efficient probes to the models' decisions. In this work, we exploit the potential of counterfactual explanations to understand and explore the behavior of machine learning models. We design DECE, an interactive visualization system that helps understand and explore a model's decisions on individual instances and data subsets, supporting users ranging from decision-subjects to model developers. DECE supports exploratory analysis of model decisions by combining the strengths of counterfactual explanations at instance- and subgroup-levels. We also introduce a set of interactions that enable users to customize the generation of counterfactual explanations to find more actionable ones that can suit their needs. Through three use cases and an expert interview, we demonstrate the effectiveness of DECE in supporting decision exploration tasks and instance explanations.",
                        "time_start": "2020-10-28T14:45:00Z",
                        "time_end": "2020-10-28T15:00:00Z",
                        "uid": "f-vast-1290"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visual Analysis of Class Separations with Locally Linear Segments",
                        "contributors": [
                            "Yuxin Ma"
                        ],
                        "abstract": "High-dimensional labeled data widely exists in many real-world applications such as classification and clustering. One main task in analyzing such datasets is to explore class separations and class boundaries derived from machine learning models. Dimension reduction techniques are commonly applied to support analysts in exploring the underlying decision boundary structures by depicting a low-dimensional representation of the data distributions from multiple classes. However, such projection-based analyses are limited due to their lack of ability to show separations in complex non-linear decision boundary structures and can suffer from heavy distortion and low interpretability. To overcome these issues of separability and interpretability,} we propose a visual analysis approach that utilizes the power of explainability from linear projections to support analysts when exploring non-linear separation structures. Our approach is to extract a set of locally linear segments that approximate the original non-linear separations. Unlike traditional projection-based analysis where the data instances are mapped to a single scatterplot, our approach supports the exploration of complex class separations through multiple local projection results. We conduct case studies on two labeled datasets to demonstrate the effectiveness of our approach.",
                        "time_start": "2020-10-28T15:00:00Z",
                        "time_end": "2020-10-28T15:15:00Z",
                        "uid": "f-tvcg-2019080262"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Visual Analytics Approach for Exploratory Causal Analysis: Exploration, Validation, and Applications",
                        "contributors": [
                            "Xiao Xie"
                        ],
                        "abstract": "Using causal relations to guide decision making has become an essential analytical task across various domains, from marketing and medicine to education and social science. While powerful statistical models have been developed for inferring causal relations from data, domain practitioners still lack effective visual interface for interpreting the causal relations and applying them in their decision-making process. Through interview studies with domain experts, we characterize their current decision-making workflows, challenges, and needs. Through an iterative design process, we developed a visualization tool that allows analysts to explore, validate, and apply causal relations in real-world decision-making scenarios. The tool provides an uncertainty-aware causal graph visualization for presenting a large set of causal relations inferred from high-dimensional data. On top of the causal graph, it supports a set of intuitive user controls for performing what-if analyses and making action plans. We report on two case studies in digital marketing and student advising scenarios to demonstrate that users can effectively explore causal relations and iteratively design action plans for reaching their goals.",
                        "time_start": "2020-10-28T15:15:00Z",
                        "time_end": "2020-10-28T15:30:00Z",
                        "uid": "f-vast-1041"
                    }
                ]
            },
            {
                "title": "Software Visualization",
                "session_id": "f-papers-software-vis",
                "chair": [
                    "Michael Behrisch"
                ],
                "organizers": [],
                "display_start": "2020-10-28T14:00:00Z",
                "time_start": "2020-10-28T14:00:00Z",
                "time_end": "2020-10-28T15:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "software-visualization",
                "discord_channel_id": "767561664500989963",
                "youtube_url": "https://youtu.be/h1isZznN1oI",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrdvf46szwh0rlAaP4lRsE2a",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "SpotSDC: Revealing the Silent Data Corruption Propagation in High-performance Computing Systems",
                        "contributors": [
                            "Zhimin Li"
                        ],
                        "abstract": "The trend of rapid technology scaling is expected to make the hardware of high-performance computing (HPC) systems more susceptible to computational errors due to random bit flips. Some bit flips may cause a program to crash or have a minimal effect on the output, but others may lead to silent data corruption (SDC), i.e., undetected yet significant output errors. Classical fault injection analysis methods employ uniform sampling of random bit flips during program execution to derive a statistical resiliency profile. However, summarizing such fault injection results with sufficient detail is difficult, and understanding the behavior of the fault-corrupted program is still a challenge. In this work, we introduce SpotSDC, a visualization system to facilitate the analysis of a program's resilience to SDC. SpotSDC provides multiple perspectives at various levels of detail of the impact on the output relative to where in the source code the flipped bit occurs, which bit is flipped, and when during the execution it happens. SpotSDC also enables users to study the code protection and provide new insights to understand the behavior of a fault-injected program. Based on lessons learned, we demonstrate how what we found can improve the fault injection campaign method.",
                        "time_start": "2020-10-28T14:00:00Z",
                        "time_end": "2020-10-28T14:15:00Z",
                        "uid": "f-tvcg-2019090312"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Vis-a-Vis: Visual Exploration of Visualization Source Code Evolution",
                        "contributors": [
                            "Fabian Bolte"
                        ],
                        "abstract": "Developing an algorithm for a visualization prototype often involves the direct comparison of different development stages and design decisions, and even minor modifications may dramatically affect the results. While existing development tools provide visualizations for gaining general insight into performance and structural aspects of the source code, they neglect the central importance of result images unique to graphical algorithms. In this paper, we present a novel approach that enables visualization programmers to simultaneously explore the evolution of their algorithm during the development phase together with its corresponding visual outcomes by providing an automatically updating meta visualization. Our interactive system allows for the direct comparison of all development states on both the visual and the source code level, by providing easy to use navigation and comparison tools. The on-the-fly construction of difference images, source code differences, and a visual representation of the source code structure further enhance the user's insight into the states' interconnected changes over time. Our solution is accessible via a web-based interface that provides GPU-accelerated live execution of C++ and GLSL code, as well as supporting a domain-specific programming language for scientific visualization.",
                        "time_start": "2020-10-28T14:15:00Z",
                        "time_end": "2020-10-28T14:30:00Z",
                        "uid": "f-tvcg-2019010020"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visualizing Hierarchical Performance Profiles of Parallel Codes using CallFlow",
                        "contributors": [
                            "Suraj Kesavan"
                        ],
                        "abstract": "Calling context trees (CCTs) couple performance metrics with call paths, helping understand the execution and performance of parallel programs. To identify performance bottlenecks, programmers and performance analysts visually explore CCTs to form and validate hypotheses regarding degraded performance. However, due to the complexity of parallel programs, existing visual representations do not scale to applications running on a large number of processors. We present CALLFLOW , an interactive visual analysis tool that provides a high-level overview of CCTs together with semantic refinement operations to progressively explore the CCTs. Using a flow-based metaphor, we visualize a CCT by treating execution time as a resource spent during a call chain, and demonstrate the effectiveness of our design with case studies on large-scale, production simulation codes.",
                        "time_start": "2020-10-28T14:30:00Z",
                        "time_end": "2020-10-28T14:45:00Z",
                        "uid": "f-tvcg-2019010024"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Githru: visual analytics of understanding software development context through git historical metadata analysis",
                        "contributors": [
                            "Youngtaek Kim"
                        ],
                        "abstract": "Git metadata contains rich information for developers to understand the overall context of a large software development project. Thus it can help new developers, managers, and testers understand the history of development without needing to dig into a large pile of unfamiliar source code. However, the current tools for Git visualization are not adequate to analyze and explore the metadata: They focus mainly on improving the usability of Git commands instead of on helping users understand the development history. Furthermore, they do not scale for large and complex Git commit graphs, which can play an important role in understanding the overall development history. In this paper, we present Githru, an interactive visual analytics system that enables developers to effectively understand the context of development history through the interactive exploration of Git metadata. We design an interactive visual encoding idiom to represent a large Git graph in a scalable manner while preserving the topological structures in the Git graph. To enable scalable exploration of a large Git commit graph, we propose novel techniques (graph reconstruction, clustering, and Context-Preserving Squash Merge (CSM) methods) to abstract a large-scale Git commit graph. Based on these Git commit graph abstraction techniques, Githru provides an interactive summary view to help users gain an overview of the development history and a comparison view in which users can compare different clusters of commits. The efficacy of Githru has been demonstrated by case studies with domain experts using real-world, in-house datasets from a large software development team at a major international IT company. A controlled user study with 12 developers comparing Githru to previous tools also confirms the effectiveness of Githru in terms of task completion time.",
                        "time_start": "2020-10-28T14:45:00Z",
                        "time_end": "2020-10-28T15:00:00Z",
                        "uid": "f-vast-1117"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Visual Analytics Approach to Debugging Cooperative, Autonomous Multi-Robot System",
                        "contributors": [
                            "Sandra Bae"
                        ],
                        "abstract": "Autonomous multi-robot systems, where a team of robots shares information to perform tasks that are beyond an individual robot's abilities, hold great promise for a number of applications, such as planetary exploration missions. Each robot in a multi-robot system that uses the shared-world coordination paradigm autonomously schedules which robot should perform a given task, and when, using its worldview\u2013the robot's internal representation of its belief about both its own state, and other robots' states. A key problem for operators is that robots' worldviews can fall out of sync (often due to weak communication links), leading to desynchronization of the robots' scheduling decisions and inconsistent emergent behavior (e.g., tasks not performed, or performed by multiple robots). Operators face the time-consuming and difficult task of making sense of the robots' scheduling decisions, detecting de-synchronizations, and pinpointing the cause by comparing every robot's worldview. To address these challenges, we introduce MOSAIC Viewer, a visual analytics system that helps operators (i) make sense of the robots' schedules and (ii) detect and conduct a root cause analysis of the robots' desynchronized worldviews. Over a year-long partnership with roboticists at the NASA Jet Propulsion Laboratory, we conduct a formative study to identify the necessary system design requirements and a qualitative evaluation with 12 roboticists. We find that MOSAIC Viewer is faster- and easier-to-use than the users' current approaches, and it allows them to stitch low-level details to formulate a high-level understanding of the robots' schedules and detect and pinpoint the cause of the desynchronized worldviews.",
                        "time_start": "2020-10-28T15:00:00Z",
                        "time_end": "2020-10-28T15:15:00Z",
                        "uid": "f-vast-1214"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "CcNav: Understanding Compiler Optimizations in Binary Code",
                        "contributors": [
                            "Sabin Devkota"
                        ],
                        "abstract": "Program developers spend significant time on optimizing and tuning programs. During this iterative process, they apply optimizations, analyze the resulting code, and modify the compilation until they are satisfied. Understanding what the compiler did with the code is crucial to this process but is very time-consuming and labor-intensive. Users need to navigate through thousands of lines of binary code and correlate it to source code concepts to understand the results of the compilation and to identify optimizations. We present a design study in collaboration with program developers and performance analysts. Our collaborators work with various artifacts related to the program such as binary code, source code, control flow graphs, and call graphs. Through interviews, feedback, and pair-analytics sessions, we analyzed their tasks and workflow. Based on this task analysis and through a human-centric design process, we designed a visual analytics system Compilation Navigator (CcNav) to aid exploration of the effects of compiler optimizations on the program. CcNav provides a streamlined workflow and a unified context that integrates disparate artifacts. CcNav supports consistent interactions across all the artifacts making it easy to correlate binary code with source code concepts. CcNav enables users to navigate and filter large binary code to identify and summarize optimizations such as inlining, vectorization, loop unrolling, and code hoisting. We evaluate CcNav through guided sessions and semi-structured interviews. We reflect on our design process, particularly the immersive elements, and on the transferability of design studies through our experience with a previous design study on program analysis.",
                        "time_start": "2020-10-28T15:15:00Z",
                        "time_end": "2020-10-28T15:30:00Z",
                        "uid": "f-vast-1203"
                    }
                ]
            },
            {
                "title": "Supporting Experts",
                "session_id": "f-papers-support-experts",
                "chair": [
                    "Matthew Berger"
                ],
                "organizers": [],
                "display_start": "2020-10-28T16:00:00Z",
                "time_start": "2020-10-28T16:00:00Z",
                "time_end": "2020-10-28T17:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "supporting-experts",
                "discord_channel_id": "767561701351751702",
                "youtube_url": "https://youtu.be/Pz32r3DdjjQ",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrfwc60T1HCMqczEAbLxnJBu",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "ShuttleSpace: Exploring and Analyzing Movement Trajectory in Immersive Visualization",
                        "contributors": [
                            "Shuainan Ye"
                        ],
                        "abstract": "We present ShuttleSpace, an immersive analytics system to assist experts in analyzing trajectory data in badminton. Trajectories in sports, such as the movement of players and balls, contain rich information on player behavior and thus have been widely analyzed by coaches and analysts to improve the players' performance. However, existing visual analytics systems often present the trajectories in court diagrams that are abstractions of reality, thereby causing difficulty for the the experts to imagine the situation on the court and understand why the player acted in a certain way. With recent developments in immersive technologies, such as virtual reality (VR), experts gradually have the opportunity to see, feel, explore, and understand these 3D trajectories from the player's perspective. Yet, few research has studied how to support immersive analysis of sports data from such a perspective. Specific challenges are rooted in data presentation (\\emph{e.g.}, how to seamlessly combine 2D and 3D visualizations) and interaction (\\emph{e.g.}, how to naturally interact with data without keyboard and mouse) in VR. To address these challenges, we have worked closely with domain experts who have worked for a top national badminton team to design ShuttleSpace, an immersive analytics system tailored for VR environments. Our system leverages 1) the peripheral vision to combine the 2D and 3D visualizations and 2) the VR controller to support natural interactions via a stroke metaphor. We demonstrate the effectiveness of ShuttleSpace through three case studies conducted by the experts with useful insights. We further conduct interviews with the experts whose feedback confirms that our first-person immersive analytics system is suitable and useful for analyzing badminton data.",
                        "time_start": "2020-10-28T16:00:00Z",
                        "time_end": "2020-10-28T16:15:00Z",
                        "uid": "f-info-1223"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "QLens: Visual Analytics of Multi-step Problem-solving Behaviors for Improving Question Design",
                        "contributors": [
                            "Meng Xia"
                        ],
                        "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements.  In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.",
                        "time_start": "2020-10-28T16:15:00Z",
                        "time_end": "2020-10-28T16:30:00Z",
                        "uid": "f-vast-1234"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visilant: Visual Support for the Exploration and Analytical Process Tracking in Criminal Investigations",
                        "contributors": [
                            "Krist\u00edna Z\u00e1kop\u010danov\u00e1"
                        ],
                        "abstract": "The daily routine of criminal investigators consists of a thorough analysis of highly complex and heterogeneous data of crime cases. Such data can consist of case descriptions, testimonies, criminal networks, spatial and temporal information, and virtually any other data that is relevant for the case. Criminal investigators work under heavy time pressure to analyze the data for relationships, propose and verify several hypotheses, and derive final conclusions, while the data can be incomplete or inconsistent and is changed and updated over the course of the investigation, as new findings are added to the case. Based on a four-year intense collaboration with criminalists, we present a conceptual design for a visual tool supporting the investigation workflow and Visilant, a web-based tool for the exploration and analysis of criminal data guided by the proposed design. Visilant aims to support namely the exploratory part of the investigation pipeline, from case overview, through exploration and hypothesis generation, to the case presentation. Visilant tracks the reasoning process, and as the data is changing, it informs investigators which hypotheses are affected by the data change and should be revised. The tool was evaluated by senior criminology experts within two sessions and their feedback is summarized in the paper. Additional supplementary material contains the technical details and exemplary case study.",
                        "time_start": "2020-10-28T16:30:00Z",
                        "time_end": "2020-10-28T16:45:00Z",
                        "uid": "f-vast-1239"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
                        "contributors": [
                            "Mar\u00eda Virginia Sabando",
                            "Pavol Ulbrich"
                        ],
                        "abstract": "In the modern drug discovery process, medicinal chemists deal with the complexity of analysis of large ensembles of candidate molecules. Computational tools, such as dimensionality reduction (DR) and classification, are commonly used to efficiently process the multidimensional space of features. These underlying calculations often hinder interpretability of results and prevent experts from assessing the impact of individual molecular features on the resulting representations. To provide a solution for scrutinizing such complex data, we introduce ChemVA, an interactive application for the visual exploration of large molecular ensembles and their features. Our tool consists of multiple coordinated views: Hexagonal view, Detail view, 3D view, Table view, and a newly proposed Difference view designed for the comparison of DR projections. These views display DR projections combined with biological activity, selected molecular features, and confidence scores for each of these projections. This conjunction of views allows the user to drill down through the dataset and to efficiently select candidate compounds. Our approach was evaluated on two case studies of finding structurally similar ligands with similar binding affinity to a target protein, as well as on an external qualitative evaluation. The results suggest that our system allows effective visual inspection and comparison of different high-dimensional molecular representations. Furthermore, ChemVA assists in the identification of candidate compounds while providing information on the certainty behind different molecular representations.",
                        "time_start": "2020-10-28T16:45:00Z",
                        "time_end": "2020-10-28T17:00:00Z",
                        "uid": "f-scivis-1080"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Visualization Framework for Multi-scale Coherent Structures in Taylor-Couette Turbulence",
                        "contributors": [
                            "Duong Nguyen"
                        ],
                        "abstract": "Taylor-Couette flow (TCF) is the turbulent fluid motion created between two concentric and independently rotating cylinders. It has been heavily researched in fluid mechanics thanks to the various nonlinear dynamical phenomena that are exhibited in the flow. As many dense coherent structures overlap each other in TCF, it is challenging to isolate and visualize them, especially when the cylinder rotation ratio is changing. Previous approaches rely on 2D cross sections to study TCF due to its simplicity, which cannot provide the complete information of TCF. In the meantime, standard visualization techniques, such as volume rendering/iso-surfacing of certain attributes and the placement of integral curves/surfaces, usually produce cluttered visualization. To address this challenge and to support domain experts in the analysis of TCF, we developed a visualization framework to separate large-scale structures from the dense, small-scale structures and provide an effective visual representation of these structures. Instead of using a single physical attribute as the standard approach which cannot efficiently separate structures in different scales for TCF, we adapt the feature level-set method to combine multiple attributes and use them as a filter to separate large- and small-scale structures. To visualize these structures, we apply the iso-surface extraction on the kernel density estimate of the distance field generated from the feature level-set. The proposed methods successfully reveal 3D large-scale coherent structures of TCF with different control parameter settings, which are difficult to achieve with the conventional methods.",
                        "time_start": "2020-10-28T17:00:00Z",
                        "time_end": "2020-10-28T17:15:00Z",
                        "uid": "f-scivis-1119"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Conceptual Model of Visual Analytics for Hands-on Cybersecurity Training",
                        "contributors": [
                            "Radek O\u0161lej\u0161ek"
                        ],
                        "abstract": "Hands-on training is an effective way to practice theoretical cybersecurity concepts and increase participants' skills. In this paper, we discuss the application of visual analytics principles to the design, execution, and evaluation of training sessions. We propose a conceptual model employing visual analytics that supports the sensemaking activities of users involved in various phases of the training life cycle. The model emerged from our long-term experience in designing and organizing diverse hands-on cybersecurity training sessions. It provides a classification of visualizations and can be used as a framework for developing novel visualization tools supporting phases of the training life-cycle. We demonstrate the model application on examples covering two types of cybersecurity training programs.",
                        "time_start": "2020-10-28T17:15:00Z",
                        "time_end": "2020-10-28T17:30:00Z",
                        "uid": "f-tvcg-2019070231"
                    }
                ]
            },
            {
                "title": "Time",
                "session_id": "f-papers-time",
                "chair": [
                    "J\u00fcrgen Bernard"
                ],
                "organizers": [],
                "display_start": "2020-10-28T16:00:00Z",
                "time_start": "2020-10-28T16:00:00Z",
                "time_end": "2020-10-28T17:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "time",
                "discord_channel_id": "767561707672436767",
                "youtube_url": "https://youtu.be/mIadBtuBq1w",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrcA6dnjMRKdD1FWNQainDmj",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "A Visual Analytics Framework for Reviewing Multivariate Time-Series Data with Dimensionality Reduction",
                        "contributors": [
                            "Takanori Fujiwara"
                        ],
                        "abstract": "Data-driven problem solving in many real-world applications involves analysis of time-dependent multivariate data, for which dimensionality reduction (DR) methods are often used to uncover the intrinsic structure and features of the data. However, DR is usually applied to a subset of data that is either single-time-point multivariate or univariate time-series, resulting in the need to manually examine and correlate the DR results out of different data subsets. When the number of dimensions is large either in terms of the number of time points or attributes, this manual task becomes too tedious and infeasible. In this paper, we present MulTiDR, a new DR framework that enables processing of time-dependent multivariate data as a whole to provide a comprehensive overview of the data. With the framework, we employ DR in two steps. When treating the instances, time points, and attributes of the data as a 3D array, the first DR step reduces the three axes of the array to two, and the second DR step visualizes the data in a lower-dimensional space. In addition, by coupling with a contrastive learning method and interactive visualizations, our framework enhances analysts' ability to interpret DR results. We demonstrate the effectiveness of our framework with four case studies using real-world datasets.",
                        "time_start": "2020-10-28T16:00:00Z",
                        "time_end": "2020-10-28T16:15:00Z",
                        "uid": "f-vast-1254"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams",
                        "contributors": [
                            "Siming Chen"
                        ],
                        "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.",
                        "time_start": "2020-10-28T16:15:00Z",
                        "time_end": "2020-10-28T16:30:00Z",
                        "uid": "f-vast-1264"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "MultiSegVA: Using Visual Analytics to Segment Biologging Time Series on Multiple Scales",
                        "contributors": [
                            "Philipp Meschenmoser"
                        ],
                        "abstract": "Segmenting biologging time series of animals on multiple temporal scales is an essential step that requires complex techniques with careful parameterization and possibly cross-domain expertise. Yet, there is a lack of visual-interactive tools that strongly support such multi-scale segmentation. To close this gap, we present our MultiSegVA platform for interactively defining segmentation techniques and parameters on multiple temporal scales. MultiSegVA primarily contributes tailored, visual-interactive means and visual analytics paradigms for segmenting unlabeled time series on multiple scales. Further, to flexibly compose the multi-scale segmentation, the platform contributes a new visual query language that links a variety of segmentation techniques. To illustrate our approach, we present a domain-oriented set of segmentation techniques derived in collaboration with movement ecologists. We demonstrate the applicability and usefulness of MultiSegVA in two real-world use cases from movement ecology, related to behavior analysis after environment-aware segmentation, and after progressive clustering. Expert feedback from movement ecologists shows the effectiveness of tailored visual-interactive means and visual analytics paradigms at segmenting multi-scale data, enabling them to perform semantically meaningful analyses. A third use case demonstrates that MultiSegVA is generalizable to other domains.",
                        "time_start": "2020-10-28T16:30:00Z",
                        "time_end": "2020-10-28T16:45:00Z",
                        "uid": "f-vast-1277"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "MineTime Insight: Visualizing Meeting Habits to Promote Informed Scheduling Decisions",
                        "contributors": [
                            "Tobias G\u00fcnther"
                        ],
                        "abstract": "Corporate meetings are a crucial part of business activities. While numerous academic papers investigated how to make the scheduling process of meetings faster or even automatic, little work has been done yet to facilitate the retrospective reasoning about how time is spent on meetings. Traditional calendar applications do not allow users to extract actionable statistics although it has been shown that reflection-oriented design can increase the users' understanding of their habits and can thereby encourage a shift towards better practices. In this paper, we present MineTime Insight, a tool made of multiple coordinated views for the exploration of personal calendar data, with the overarching goal of improving short and long-term scheduling decisions. Despite being focused on the working environment, our work builds upon recent results in the field of Personal Visual Analytics, as it targets users not necessarily expert in visualization and data analysis. We demonstrate the potential of MineTime Insight, when applied to the agenda of an executive manager. Finally, we discuss the results of an informal user study and a field study. Our results suggest that our visual representations are perceived as easy to understand and helpful towards a change in the scheduling habits.",
                        "time_start": "2020-10-28T16:45:00Z",
                        "time_end": "2020-10-28T17:00:00Z",
                        "uid": "f-tvcg-2018110393"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "SineStream: Improving the Readability of Streamgraphs by Minimizing Sine Illusion Effects",
                        "contributors": [
                            "Chuan Bu"
                        ],
                        "abstract": "In this paper, we propose SineStream, a new variant of streamgraphs that improves their readability by minimizing sine illusion effects. Such effects reflect the tendency of humans to take the orthogonal rather than the vertical distance between two curves as their distance. In SineStream, we connect the readability of streamgraphs with minimizing sine illusions and by doing so provide a perceptual foundation for their design. As the geometry of a streamgraph is controlled by its baseline (the bottom-most curve) and the ordering of the layers, we re-interpret baseline computation and layer ordering algorithms in terms of reducing sine illusion effects. For baseline computation, we improve previous methods by introducing a Gaussian weight to penalize layers with large thickness changes. For layer ordering, three design requirements are proposed and implemented through a hierarchical clustering algorithm. Quantitative experiments and user studies demonstrate that SineStream improves the readability and aesthetics of streamgraphs compared to state-of-the-art methods",
                        "time_start": "2020-10-28T17:00:00Z",
                        "time_end": "2020-10-28T17:15:00Z",
                        "uid": "f-info-1348"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "SplitStreams: A Visual Metaphor for Evolving Hierarchies",
                        "contributors": [
                            "Fabian Bolte"
                        ],
                        "abstract": "The visualization of hierarchically structured data over time is an ongoing challenge and several approaches exist trying to solve it. Techniques such as animated or juxtaposed tree visualizations are not capable of providing a good overview of the time series and lack expressiveness in conveying changes over time. Nested streamgraphs provide a better understanding of the data evolution, but lack the clear outline of hierarchical structures at a given timestep. Furthermore, these approaches are often limited to static hierarchies or exclude complex hierarchical changes in the data, limiting their use cases. We propose a novel visual metaphor capable of providing a static overview of all hierarchical changes over time, as well as clearly outlining the hierarchical structure at each individual time step. Our method allows for smooth transitions between tree maps and nested streamgraphs, enabling the exploration of the trade-off between dynamic behavior and hierarchical structure. As our technique handles topological changes of all types, it is suitable for a wide range of applications. We demonstrate the utility of our method on several use cases, evaluate it with a user study, and provide its full source code.",
                        "time_start": "2020-10-28T17:15:00Z",
                        "time_end": "2020-10-28T17:30:00Z",
                        "uid": "f-tvcg-2019100378"
                    }
                ]
            },
            {
                "title": "Molecules, Cells, & Vessels",
                "session_id": "f-papers-mol-cell-vessel",
                "chair": [
                    "Michael Krone"
                ],
                "organizers": [],
                "display_start": "2020-10-28T16:00:00Z",
                "time_start": "2020-10-28T16:00:00Z",
                "time_end": "2020-10-28T17:15:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "molecules-cells-vessels",
                "discord_channel_id": "767561713754964008",
                "youtube_url": "https://youtu.be/GVfO0F-4T7g",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrcbKoAbChWKVPFR6jh3DW0G",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "HyperLabels: Browsing of Dense and Hierarchical Molecular 3D Models",
                        "contributors": [
                            "David Kou\u0159il"
                        ],
                        "abstract": " We present a method for the browsing of hierarchical 3D models in which we combine the typical navigation of hierarchical structures in a 2D environment---using clicks on nodes, links, or icons---with a 3D spatial data visualization. Our approach is motivated by large molecular models, for which the traditional single-scale navigational metaphors are not suitable. Multi-scale phenomena, e.g., in astronomy or geography, are complex to navigate due to their large data spaces and multi-level organization. Models from structural biology are in addition also densely crowded in space and scale. Cutaways are needed to show individual model subparts. The camera has to support exploration on the level of a whole virus, as well as on the level of a small molecule. We address these challenges by employing HyperLabels: active labels that---in addition to their annotational role---also support user interaction. Clicks on HyperLabels select the next structure to be explored. Then, we adjust the visualization to showcase the inner composition of the selected subpart and enable further exploration. Finally, we use a breadcrumbs panel for orientation and as a mechanism to traverse upwards in the model hierarchy. We demonstrate our concept of hierarchical 3D model browsing using two exemplary models from meso-scale biology.",
                        "time_start": "2020-10-28T16:00:00Z",
                        "time_end": "2020-10-28T16:15:00Z",
                        "uid": "f-tvcg-2019100368"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visual cohort comparison for spatial single-cell omics-data",
                        "contributors": [
                            "Antonios Somarakis"
                        ],
                        "abstract": "Spatially-resolved omics-data enable researchers to precisely distinguish cell types in tissue and explore their spatial interactions, enabling deep understanding of tissue functionality. To understand what causes or deteriorates a disease and identify related biomarkers, clinical researchers regularly perform large-scale cohort studies, requiring the comparison of such data at cellular level. In such studies, with little a-priori knowledge of what to expect in the data, explorative data analysis is a necessity. Here, we present an interactive visual analysis workflow for the comparison of cohorts of spatially-resolved omics-data. Our workflow allows the comparative analysis of two cohorts based on multiple levels-of-detail, from simple abundance of contained cell types over complex co-localization patterns to individual comparison of complete tissue images. As a result, the workflow enables the identification of cohort-differentiating features, as well as outlier samples at any stage of the workflow. During the development of the workflow, we continuously consulted with domain experts.  To show the effectiveness of the workflow, we conducted multiple case studies with domain experts from different application areas and with different data modalities.",
                        "time_start": "2020-10-28T16:15:00Z",
                        "time_end": "2020-10-28T16:30:00Z",
                        "uid": "f-vast-1159"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Improving the Usability of Virtual Reality Neuron Tracing with Topological Elements",
                        "contributors": [
                            "Torin McDonald"
                        ],
                        "abstract": "Researchers in the field of connectomics are working to reconstruct a map of neural connections in the brain in order to understand at a fundamental level how the brain processes information. Constructing this wiring diagram is done by tracing neurons\nthrough high-resolution image stacks acquired with fluorescence microscopy imaging techniques. While a large number of automatic tracing algorithms have been proposed, these frequently rely on local features in the data and fail on noisy data or ambiguous cases, requiring time-consuming manual correction. As a result, manual and semi-automatic tracing methods remain the state-of-the-art for creating accurate neuron reconstructions. We propose a new semi-automatic method that uses topological features to guide users in tracing neurons and integrate this method within a virtual reality (VR) framework previously used for manual tracing. Our approach augments both visualization and interaction with topological elements, allowing rapid understanding and tracing of complex morphologies. In our pilot study, neuroscientists demonstrated a strong preference for using our tool over prior approaches, reported\nless fatigue during tracing, and commended the ability to better understand possible paths and alternatives. Quantitative evaluation of the traces reveals that users' tracing speed increased, while retaining similar accuracy compared to a fully manual approach.",
                        "time_start": "2020-10-28T16:30:00Z",
                        "time_end": "2020-10-28T16:45:00Z",
                        "uid": "f-scivis-1091"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "CMed: Crowd Analytics for Medical Imaging Data",
                        "contributors": [
                            "Ji Hwan Park"
                        ],
                        "abstract": "We present a visual analytics framework, CMed, for exploring medical image data annotations acquired from crowdsourcing. CMed can be used to visualize, classify, and filter crowdsourced clinical data based on a number of different metrics such as detection rate, logged events, and clustering of the annotations. CMed provides several interactive linked visualization components to analyze the crowd annotation results for a particular video and the associated workers. Additionally, all results of an individual worker can be inspected using multiple linked views in our CMed framework. We allow a crowdsourcing application analyst to observe patterns and gather insights into the crowdsourced medical data, helping him/her design future crowdsourcing applications for optimal output from the workers. We demonstrate the efficacy of our framework with two medical crowdsourcing studies: polyp detection in virtual colonoscopy videos and lung nodule detection in CT thin-slab maximum intensity projection videos. We also provide experts' feedback to show the effectiveness of our framework. Lastly, we share the lessons we learned from our framework with suggestions for integrating our framework into a clinical workflow.",
                        "time_start": "2020-10-28T16:45:00Z",
                        "time_end": "2020-10-28T17:00:00Z",
                        "uid": "f-tvcg-2018120457"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Void Space Surfaces to Convey Depth in Vessel Visualizations",
                        "contributors": [
                            "Julian Kreiser"
                        ],
                        "abstract": "To enhance depth perception and thus data comprehension, additional depth cues are often used in 3D visualizations of complex vascular structures. There is a variety of different approaches described in theliterature, ranging from chromadepth color coding over depth of field to glyph-based encodings. Unfortunately, the majority of existing approaches suffers from the same problem: As these cues are directly applied to the geometry's surface, the display of additional information on the vessel wall, such as other modalities or derived attributes, is impaired. To overcome this limitation we propose Void Space Surfaces which utilizes empty space in between vessel branches to communicate depth and their relative positioning. This allows us to enhance the depth perception of vascular structures without interfering with the spatial data and potentially superimposed parameter information. With this paper, we introduce Void Space Surfaces, describe their technical realization, and show their application to various vessel trees. Moreover, we report the outcome of two user studies which we have conducted in order to evaluate the perceptual impact of Void Space Surfaces compared to existing vessel visualization techniques and discuss expert feedback.",
                        "time_start": "2020-10-28T17:00:00Z",
                        "time_end": "2020-10-28T17:15:00Z",
                        "uid": "f-tvcg-2018080304"
                    }
                ]
            },
            {
                "title": "Dynamic Graphs & Hypergraphs",
                "session_id": "f-papers-dyn-graphs",
                "chair": [
                    "Jian Zhao"
                ],
                "organizers": [],
                "display_start": "2020-10-28T18:00:00Z",
                "time_start": "2020-10-28T18:00:00Z",
                "time_end": "2020-10-28T19:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "dynamic-graphs-hypergraphs",
                "discord_channel_id": "767561742846394389",
                "youtube_url": "https://youtu.be/D20CJC1MAO8",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrcLEXJNWgd4Xg-K4aHPH2U0",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "A Visual Analytics Approach for Ecosystem Dynamics based on Empirical Dynamic Modeling",
                        "contributors": [
                            "Hiroaki Natsukawa"
                        ],
                        "abstract": "An important approach for scientific inquiry across many disciplines involves using observational time series data to understand the relationships between key variables to gain mechanistic insights into the underlying rules that govern the given system. In real systems, such as those found in ecology, the relationships between time series variables are generally not static; instead, these relationships are dynamical and change in a nonlinear or state-dependent manner. To further understand such systems, we investigate integrating methods that appropriately characterize these dynamics (i.e., methods that measure interactions as they change with time-varying system states) with visualization techniques that can help analyze the behavior of the system. Here, we focus on empirical dynamic modeling (EDM) as a state-of-the-art method that specifically identifies causal variables and measures changing state-dependent relationships between time series variables. Instead of using approaches centered on parametric equations, EDM is an equation-free approach that studies systems based on their dynamic attractors. We propose a visual analytics system to support the identification and mechanistic interpretation of system states using an EDM-constructed dynamic graph. This work, as detailed in four analysis tasks and demonstrated with a GUI, provides a novel synthesis of EDM and visualization techniques such as brush-link visualization and visual summarization to interpret dynamic graphs representing ecosystem dynamics. We applied our proposed system to ecological simulation data and real data from a marine mesocosm study as two key use cases. Our case studies show that our visual analytics tools support the identification and interpretation of the system state by the user, and enable us to discover both confirmatory and new findings in ecosystem dynamics. Overall, we demonstrated that our system can facilitate an understanding of how systems function beyond the intuitive analysis of high-dimensional information based on specific domain knowledge.",
                        "time_start": "2020-10-28T18:00:00Z",
                        "time_end": "2020-10-28T18:15:00Z",
                        "uid": "f-vast-1216"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Multiscale Snapshots: Visual Analysis of Temporal Summaries in Dynamic Graphs",
                        "contributors": [
                            "Eren Cakmak"
                        ],
                        "abstract": "The overview-driven visual analysis of large-scale dynamic graphs poses a major challenge. We propose Multiscale Snapshots, a visual analytics approach to analyze temporal summaries of dynamic graphs at multiple temporal scales. First, we recursively generate temporal summaries to abstract overlapping sequences of graphs into compact snapshots. Second, we apply graph embeddings to the snapshots to learn low-dimensional representations of each sequence of graphs to speed up specific analytical tasks (e.g., similarity search). Third, we visualize the evolving data from a coarse to fine-granular snapshots to semi-automatically analyze temporal states, trends, and outliers. The approach enables us to discover similar temporal summaries (e.g., reoccurring states), reduces the temporal data to speed up automatic analysis, and to explore both structural and temporal properties of a dynamic graph. We demonstrate the usefulness of our approach by a quantitative evaluation and the application to a real-world dataset.",
                        "time_start": "2020-10-28T18:15:00Z",
                        "time_end": "2020-10-28T18:30:00Z",
                        "uid": "f-vast-1267"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "The Effectiveness of Interactive Visualization Techniques for Time Navigation of Dynamic Graphs on Large Displays",
                        "contributors": [
                            "Alexandra Lee"
                        ],
                        "abstract": "Dynamic networks can be challenging to analyze visually, especially if they span a large time range during which new nodes and edges can appear and disappear. Although it is straightforward to provide interfaces for visualization that represent multiple states of the network (i.e., multiple timeslices) either simultaneously (e.g., through small multiples) or interactively (e.g., through interactive animation), these interfaces might not support tasks in which disjoint timeslices need to be compared. Since these tasks are key for understanding the dynamic aspects of the network, understanding which interactive visualizations best support these tasks is important. We present the results of a series of laboratory experiments comparing two traditional approaches (small multiples and interactive animation), with a more recent approach based on interactive timeslicing. The tasks were performed on a large display through a touch interface. Participants completed 24 trials of three tasks with all techniques. The results show that interactive timeslicing brings benefit when comparing distant points in time, but less benefits when analyzing contiguous intervals of time.",
                        "time_start": "2020-10-28T18:30:00Z",
                        "time_end": "2020-10-28T18:45:00Z",
                        "uid": "f-info-1030"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Staged Animation Strategies for Online Dynamic Networks",
                        "contributors": [
                            "Tarik Crnovrsanin"
                        ],
                        "abstract": "Dynamic networks\u2014networks that change over time\u2014can be categorized into two types: offline dynamic networks, whereall states of the network are known, and online dynamic networks, where only the past states of the network are known. Research onstaging animated transitions in dynamic networks has focused more on offline data, where rendering strategies can take into accountpast and future states of the network. Rendering online dynamic networks is a more challenging problem since it requires a balancebetween timeliness for monitoring tasks\u2014so that the animations do not lag too far behind the events\u2014and clarity for comprehensiontasks\u2014to minimize simultaneous changes that may be difficult to follow. To illustrate the challenges placed by these requirements,we explore three strategies to stage animations for online dynamic networks: time-based, event-based, and a new hybrid approachthat we introduce by combining the advantages of the first two. We illustrate the advantages and disadvantages of each strategy inrepresenting low- and high-throughput data and conduct a user study involving monitoring and comprehension of dynamic networks.We also conduct a follow-up, think-aloud study combining monitoring and comprehension with experts in dynamic network visualization.Our findings show that animation staging strategies that emphasize comprehension do better for participant response times andaccuracy. However, the notion of \u201ccomprehension\u201d is not always clear when it comes to complex changes in highly dynamic networks,requiring some iteration in staging that the hybrid approach affords. Based on our results, we make recommendations for balancingevent-based and time-based parameters for our hybrid approach.",
                        "time_start": "2020-10-28T18:45:00Z",
                        "time_end": "2020-10-28T19:00:00Z",
                        "uid": "f-info-1381"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Analyzing Dynamic Hypergraphs with Parallel Aggregated Ordered Hypergraph Visualization",
                        "contributors": [
                            "Paola Valdivia"
                        ],
                        "abstract": "Parallel Aggregated Ordered Hypergraph (PAOH) is a novel technique to visualize dynamic hypergraphs. Hypergraphs are a generalization of graphs where edges can connect several vertices. Hypergraphs can be used to model networks of business partners or co-authorship networks with multiple authors per article. A dynamic hypergraph evolves over discrete time slots. PAOH represents vertices as parallel horizontal bars and hyperedges as vertical lines, using dots to depict the connections to one or more vertices. We describe a prototype implementation of Parallel Aggregated Ordered Hypergraph, report on a usability study with 9 participants analyzing publication data, and summarize the improvements made. Two case studies and several examples are provided. We believe that PAOH is the first technique to provide a highly readable representation of dynamic hypergraphs. It is easy to learn and well suited for medium size dynamic hypergraphs (50-500 vertices) such as those commonly generated by digital humanities projects\u2014our driving application domain.",
                        "time_start": "2020-10-28T19:00:00Z",
                        "time_end": "2020-10-28T19:15:00Z",
                        "uid": "f-tvcg-2018110386"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visual Analytics for Temporal Hypergraph Model Exploration",
                        "contributors": [
                            "Maximilian T. Fischer"
                        ],
                        "abstract": "Many processes, from gene interaction in biology to computer networks to social media, can be modeled more precisely as temporal hypergraphs than by regular graphs. This is because hypergraphs generalize graphs by extending edges to connect any number of vertices, allowing complex relationships to be described more accurately and predict their behavior over time. However, the interactive exploration and seamless refinement of such hypergraph-based prediction models still pose a major challenge. We contribute Hyper-Matrix, a novel visual analytics technique that addresses this challenge through a tight coupling between machine-learning and interactive visualizations. In particular, the technique incorporates a geometric deep learning model as a blueprint for problem-specific models while integrating visualizations for graph-based and category-based data with a novel combination of interactions for an effective user-driven exploration of hypergraph models. To eliminate demanding context switches and ensure scalability, our matrix-based visualization provides drill-down capabilities across multiple levels of semantic zoom, from an overview of model predictions down to the content. We facilitate a focused analysis of relevant connections and groups based on interactive user-steering for filtering and search tasks, a dynamically modifiable partition hierarchy, various matrix reordering techniques, and interactive model feedback. We evaluate our technique in a case study and through formative evaluation with law enforcement experts using real-world internet forum communication data. The results show that our approach surpasses existing solutions in terms of scalability and applicability, enables the incorporation of domain knowledge, and allows for fast search-space traversal. With the proposed technique, we pave the way for the visual analytics of temporal hypergraphs in a wide variety of domains.",
                        "time_start": "2020-10-28T19:15:00Z",
                        "time_end": "2020-10-28T19:30:00Z",
                        "uid": "f-vast-1032"
                    }
                ]
            },
            {
                "title": "Neural Networks",
                "session_id": "f-papers-neural-net",
                "chair": [
                    "Fernando Paulovich"
                ],
                "organizers": [],
                "display_start": "2020-10-28T18:00:00Z",
                "time_start": "2020-10-28T18:00:00Z",
                "time_end": "2020-10-28T19:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "neural-networks",
                "discord_channel_id": "767561751285334076",
                "youtube_url": "https://youtu.be/M-pUfWMjXhQ",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojreWyMVMMW7tMs9JwdXBG9RL",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "CNNPruner: Pruning Convolutional Neural Networks with Visual Analytics",
                        "contributors": [
                            "Guan Li"
                        ],
                        "abstract": "Convolutional neural networks (CNNs) have demonstrated extraordinarily good performance in many computer vision tasks. The increasing size of CNN models, however, prevents them from being widely deployed to devices with limited computational resources, e.g., mobile/embedded devices. The emerging topic of model pruning strives to address this problem by removing less important neurons and fine-tuning the pruned networks to minimize the accuracy loss. Nevertheless, existing automated pruning solutions often rely on a numerical threshold of the importance criteria for pruning, lacking the flexibility to optimally balance the trade-off between efficiency and accuracy. Moreover, the complicated interplay between the stages of neuron pruning and model fine-tuning makes this process opaque, and therefore becomes difficult to optimize. In this paper, we address these challenges through a visual analytics approach, named CNNPruner. It considers the importance of convolutional filters through both instability and sensitivity, and allows users to interactively create pruning plans according to a desired goal on model size or accuracy. Also, CNNPruner integrates state-of-the-art filter visualization techniques to help users understand the roles that different filters played and refine their pruning plans. Through comprehensive case studies on CNNs with real-world sizes, we validate the effectiveness of CNNPruner.",
                        "time_start": "2020-10-28T18:00:00Z",
                        "time_end": "2020-10-28T18:15:00Z",
                        "uid": "f-vast-1186"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visual Neural Decomposition to Explain Multivariate Data Sets",
                        "contributors": [
                            "Johannes Knittel"
                        ],
                        "abstract": "Investigating relationships between variables in multi-dimensional data sets is a common task for data analysts and engineers. More specifically, it is often valuable to understand which ranges of which input variables lead to particular values of a given target variable. Unfortunately, with an increasing number of independent variables, this process may become cumbersome and time-consuming due to the many possible combinations that have to be explored. In this paper, we propose a novel approach to visualize correlations between input variables and a target output variable that scales to hundreds of variables. We developed a visual model based on neural networks that can be explored in a guided way to help analysts find and understand such correlations. First, we train a neural network to predict the target from the input variables. Then, we visualize the inner workings of the resulting model to help understand relations within the data set. We further introduce a new regularization term for the backpropagation algorithm that encourages the neural network to learn representations that are easier to interpret visually. We apply our method to artificial and real-world data sets to show its utility.",
                        "time_start": "2020-10-28T18:15:00Z",
                        "time_end": "2020-10-28T18:30:00Z",
                        "uid": "f-vast-1294"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes",
                        "contributors": [
                            "Yuxin Ma"
                        ],
                        "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.",
                        "time_start": "2020-10-28T18:30:00Z",
                        "time_end": "2020-10-28T18:45:00Z",
                        "uid": "f-vast-1096"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization",
                        "contributors": [
                            "Zijie J. Wang"
                        ],
                        "abstract": "Deep learning's great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN's structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern deep learning techniques.",
                        "time_start": "2020-10-28T18:45:00Z",
                        "time_end": "2020-10-28T19:00:00Z",
                        "uid": "f-vast-1080"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "HyperTendril: Visual Analytics for User-Driven Hyperparameter Optimization of Deep Neural Networks",
                        "contributors": [
                            "Heungseok Park"
                        ],
                        "abstract": "To mitigate the pain of manually tuning hyperparameters of deep neural networks, automated machine learning (AutoML) methods have been developed to search for an optimal set of hyperparameters in large combinatorial search spaces. However, the search results of AutoML methods are significantly affected by initial configurations, and it is a non-trivial task to find a proper configuration for them. Therefore, human intervention via a visual analytic approach bears huge potential in this task. In response, we propose HyperTendril, a web-based visual analytics system that supports user-driven hyperparameter tuning processes in model-agnostic environment. HyperTendril utilizes a novel approach to effectively steering hyperparameter optimization (HyperOpt) through an iterative, interactive tuning procedure that allows users to refine the search spaces and the configuration of AutoML method based on their own insights from given results. Using HyperTendril, users can obtain insights into the complex behaviors of various hyperparameter search algorithms and diagnose their configurations. In addition, HyperTendril supports variable importance analysis to help the users refine their search spaces based on the analysis of relative importance of different hyperparameters and their interaction effects. We present an evaluation focusing on how HyperTendril helps users steer their tuning process via a longitudinal user study based on the analysis of interaction logs and in-depth interviews while we deploy our system in a professional industrial environment.",
                        "time_start": "2020-10-28T19:00:00Z",
                        "time_end": "2020-10-28T19:15:00Z",
                        "uid": "f-vast-1105"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Analyzing the Noise Robustness of Deep Neural Networks",
                        "contributors": [
                            "Mengchen Liu"
                        ],
                        "abstract": "Adversarial examples, generated by adding small but intentionally imperceptible perturbations to normal examples, can mislead deep neural networks (DNNs) to make incorrect predictions. Although much work has been done on both adversarial attack and defense, a fine-grained understanding of adversarial examples is still lacking. To address this issue, we present a visual analysis method to explain why adversarial examples are misclassified. The key is to compare and analyze the datapaths of both the adversarial and normal examples. A datapath is a group of critical neurons along with their connections. We formulate the datapath extraction as a subset selection problem and solve it by constructing and training a neural network. A multi-level visualization consisting of a network-level visualization of data flows, a layer-level visualization of feature maps, and a neuron-level visualization of learned features, has been designed to help investigate how datapaths of adversarial and normal examples diverge and merge in the prediction process. A quantitative evaluation and a case study were conducted to demonstrate the promise of our method to explain the misclassification of adversarial examples.",
                        "time_start": "2020-10-28T19:15:00Z",
                        "time_end": "2020-10-28T19:30:00Z",
                        "uid": "f-tvcg-2019100374"
                    }
                ]
            },
            {
                "title": "Uncertainty",
                "session_id": "f-papers-uncertainty",
                "chair": [
                    "Lars Linsen"
                ],
                "organizers": [],
                "display_start": "2020-10-28T18:00:00Z",
                "time_start": "2020-10-28T18:00:00Z",
                "time_end": "2020-10-28T19:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "uncertainty",
                "discord_channel_id": "767561758209867776",
                "youtube_url": "https://youtu.be/hJq6IDmJXs0",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrftSD7APNpta0ZGwSG2xyxW",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Direct Volume Rendering with Nonparametric Models of Uncertainty",
                        "contributors": [
                            "Tushar Athawale"
                        ],
                        "abstract": "We present a nonparametric statistical framework for the quantification, analysis, and propagation of data uncertainty in direct volume rendering (DVR). The state-of-the-art spline-based statistical DVR framework allows for preserving the transfer function (TF) of the ground truth function when visualizing uncertain data; however, the existing framework is restricted to parametric noise assumption. In this paper, we address the limitations of the existing DVR framework by extending the DVR framework for nonparametric noise distributions. We exploit the quantile interpolation technique to derive probability distributions representing uncertainty in viewing-ray sample intensities in closed form. The closed-form formulation of our framework allows for accurate and efficient computation of results. We demonstrate the effectiveness of our proposed nonparametric statistical models through qualitative and quantitative comparisons with the mean-field and parametric statistical models, such as uniform, Gaussian, and Gaussian mixtures. In addition to our contributions in nonparametric statistics, we present an extension of the state-of-the-art spline-based parametric framework to 2D transfer functions for improved DVR classifications. We show the applicability of our uncertainty quantification framework to ensemble, downsampled, and bivariate versions of scalar field datasets.",
                        "time_start": "2020-10-28T18:00:00Z",
                        "time_end": "2020-10-28T18:15:00Z",
                        "uid": "f-scivis-1130"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Uncertainty Visualization of 2D Morse Complex Ensembles Using Statistical Summary Maps",
                        "contributors": [
                            "Tushar Athawale"
                        ],
                        "abstract": "Morse complexes are gradient-based topological descriptors with close connections to Morse theory. They are widely applicable in scientific visualization as they serve as important abstractions for gaining insights into the topology of scalar fields. Data uncertainty inherent to scalar fields due to randomness in their acquisition and processing, however, limits our understanding of Morse complexes as structural abstractions. We, therefore, explore uncertainty visualization of an ensemble of 2D Morse complexes that arises from scalar fields coupled with data uncertainty. We propose statistical summary maps as new entities for quantifying structural variations and visualizing positional uncertainties of Morse complexes in ensembles. Specifically, we introduce three types of statistical summary maps \u2013 the probabilistic map, the significance map, and the survival map \u2013 to characterize the uncertain behaviors of gradient flows. We demonstrate the utility of our proposed approach using wind, flow, and ocean eddy simulation datasets.",
                        "time_start": "2020-10-28T18:15:00Z",
                        "time_end": "2020-10-28T18:30:00Z",
                        "uid": "f-tvcg-2019120446"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Uncertainty-Oriented Ensemble Data Visualization and Exploration using Variable Spatial Spreading",
                        "contributors": [
                            "Mingdong Zhang"
                        ],
                        "abstract": "As an important method of handling potential uncertainties in numerical simulations, ensemble simulation has been widely applied in many disciplines. Visualization is a promising and powerful ensemble simulation analysis method. However, conventional visualization methods mainly aim at data simplification and highlighting of important information on the basis of domain expertise instead of providing a flexible data exploration and intervention mechanism. Trial-and-error procedures have to be repeatedly conducted by such approaches. To resolve this issue, we propose a new perspective of ensemble data analysis using the attribute variable dimension as the primary analysis dimension. Particularly, we propose a variable uncertainty calculation method based on variable spatial spreading. On the basis of this method, we design an interactive ensemble analysis framework that provides flexible interactive exploration of the ensemble data. Particularly, the proposed spreading curve view, the region stability heat map view, and the temporal analysis view, together with the commonly used 2D map view, jointly support uncertainty distribution perception, region selection, and temporal analysis, as well as other analysis requirements.  We verify our approach by analyzing a real-world ensemble simulation dataset. Feedback from domain experts demonstrates the efficacy of our framework.",
                        "time_start": "2020-10-28T18:30:00Z",
                        "time_end": "2020-10-28T18:45:00Z",
                        "uid": "f-scivis-1096"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Uncertainty in Continuous Scatterplots, Continuous Parallel Coordinates, and Fibers",
                        "contributors": [
                            "Boyang Zheng"
                        ],
                        "abstract": "In this paper, we introduce uncertainty to continuous scatterplots and continuous parallel coordinates. We derive respective models, validate them with sampling-based brute-force schemes, and present acceleration strategies for their computation. At the same time, we show that our approach lends itself as well for introducing uncertainty into the definition of fibers in bivariate data. We demonstrate the properties and the utility of our approach using specifically designed synthetic cases and simulated data.",
                        "time_start": "2020-10-28T18:45:00Z",
                        "time_end": "2020-10-28T19:00:00Z",
                        "uid": "f-scivis-1045"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology",
                        "contributors": [
                            "Ghulam Jilani Quadri"
                        ],
                        "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on 4 factors---distribution size of clusters, number of points, size of points, and opacity of points---that influence cluster identification in scatterplots. From these parameters, we have constructed 2 models, a distance-based model and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.",
                        "time_start": "2020-10-28T19:00:00Z",
                        "time_end": "2020-10-28T19:15:00Z",
                        "uid": "f-info-1054"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "ProReveal: Progressive Visual Analytics with Safeguards",
                        "contributors": [
                            "Jaemin Jo"
                        ],
                        "abstract": "We present a new visual exploration concept\u2014Progressive Visual Analytics with Safeguards\u2014that helps people manage the uncertainty arising from progressive data exploration. Despite its potential benefits, intermediate knowledge from progressive analytics can be incorrect due to various machine and human factors, such as a sampling bias or misinterpretation of uncertainty. To alleviate this problem, we introduce PVA-Guards, safeguards people can leave on uncertain intermediate knowledge that needs to be verified, and derive seven PVA-Guards based on previous visualization task taxonomies. PVA-Guards provide a means of ensuring the correctness of the conclusion and understanding the reason when intermediate knowledge becomes invalid. We also present ProReveal, a proof-of-concept system designed and developed to integrate the seven safeguards into progressive data exploration. Finally, we report a user study with 14 participants, which show people voluntarily employed PVA-Guards to safeguard their findings and ProReveal's PVA-Guard view provides an overview of uncertain intermediate knowledge. We believe our new concept can also offer better consistency in progressive data exploration, alleviating people's heterogeneous interpretation of uncertainty.",
                        "time_start": "2020-10-28T19:15:00Z",
                        "time_end": "2020-10-28T19:30:00Z",
                        "uid": "f-tvcg-20192962404"
                    }
                ]
            },
            {
                "title": "Planets & Space",
                "session_id": "f-papers-planets-space",
                "chair": [
                    "Patric Ljung"
                ],
                "organizers": [],
                "display_start": "2020-10-29T14:00:00Z",
                "time_start": "2020-10-29T14:00:00Z",
                "time_end": "2020-10-29T15:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "planets-space",
                "discord_channel_id": "768678984921251870",
                "youtube_url": "https://youtu.be/0oV9JINvYyE",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrcfLoW8apIz67DYRG4P_s8j",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "InCorr: Interactive Data-Driven Correlation Panels for Digital Outcrop Analysis",
                        "contributors": [
                            "Thomas Ortner"
                        ],
                        "abstract": "Geological analysis of 3D Digital Outcrop Models (DOMs) for reconstruction of ancient habitable environments is a key aspect of the upcoming ESA ExoMars 2022 Rosalind Franklin Rover and the NASA 2020 Rover Perseverance missions in seeking signs of past life on Mars. Geologists measure and interpret 3D DOMs, create sedimentary logs and combine them in `correlation panels' to map the extents of key geological horizons, and build a stratigraphic model to understand their position in the ancient landscape. Currently, the creation of correlation panels is completely manual and therefore time-consuming, and inflexible. With InCorr we present a visualization solution that encompasses a 3D logging tool and an interactive data-driven correlation panel that evolves with the stratigraphic analysis. For the creation of InCorr we closely cooperated with leading planetary geologists in the form of a design study. We verify our results by recreating an existing correlation analysis with InCorr and validate our correlation panel against a manually created illustration. Further, we conducted a user-study with a wider circle of geologists. Our evaluation shows that InCorr efficiently supports the domain experts in tackling their research questions and that it has the potential to significantly impact how geologists work with digital outcrop representations in general.",
                        "time_start": "2020-10-29T14:00:00Z",
                        "time_end": "2020-10-29T14:15:00Z",
                        "uid": "f-vast-1130"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Extraction and Visualization of Poincar\u00e9 Map Topology for Spacecraft Trajectory Design",
                        "contributors": [
                            "Xavier Tricoche"
                        ],
                        "abstract": "Mission designers must study many dynamical models to plan a low-cost spacecraft trajectory that satisfies mission constraints. They routinely use Poincar\u00e9 maps to search for a suitable path through the interconnected web of periodic orbits and invariant manifolds found in multi-body gravitational systems. This paper is concerned with the extraction and interactive visual exploration of this structural landscape to assist spacecraft trajectory planning. We propose algorithmic solutions that address the specific challenges posed by the characterization of the topology in astrodynamics problems and allow for an effective visual analysis of the resulting information. This visualization framework is applied to the circular restricted three-body problem (CR3BP), where it reveals novel periodic orbits with their relevant invariant manifolds in a suitable format for interactive transfer selection. Representative design problems illustrates how spacecraft path planners can leverage our topology visualization to fully exploit the natural dynamics pathways for energy efficient trajectory designs.",
                        "time_start": "2020-10-29T14:15:00Z",
                        "time_end": "2020-10-29T14:30:00Z",
                        "uid": "f-scivis-1199"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "IsoTrotter: Visually Guided Empirical Modelling of Convection in Meteorology",
                        "contributors": [
                            "Juraj Palenik"
                        ],
                        "abstract": "Empirical models, fitted to data from observations, are often used in natural sciences to describe physical behaviour and support discoveries. However, with more complex models, the regression of parameters quickly becomes insufficient, requiring a visual parameter space analysis to understand and optimize the models. In this work, we present a design study for building a model describing atmospheric convection. We present a mixed-initiative approach to visually guided modelling, integrating an interactive visual parameter space analysis with partial automatic parameter optimization. Our approach includes a new, semi-automatic technique called IsoTrotting, where we optimize the procedure by navigating along isocontours of the model. We evaluate the model with unique observational data of atmospheric convection based on flight trajectories of paragliders.",
                        "time_start": "2020-10-29T14:30:00Z",
                        "time_end": "2020-10-29T14:45:00Z",
                        "uid": "f-scivis-1101"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Interactive Visualization of Atmospheric Effects for Celestial Bodies",
                        "contributors": [
                            "Jonathas Costa"
                        ],
                        "abstract": "We present an atmospheric model tailored for the interactive visualization of planetary surfaces. As the exploration of the solar system is progressing with increasingly accurate missions and instruments, the faithful visualization of planetary environments is gaining increasing interest in space research, mission planning, and science communication and education. Atmospheric effects are crucial in data analysis and to provide contextual information for planetary data. Our model correctly accounts for the non-linear path of the light inside the atmosphere (in Earth's case), the light absorption effects by molecules and dust particles, such as the ozone layer and the Martian dust, and a wavelength-dependent phase function for Mie scattering. The mode focuses on interactivity, versatility, and customization, and a comprehensive set of interactive controls make it possible to adapt its appearance dynamically. We demonstrate our results using Earth and Mars as examples. However, it can be readily adapted for the exploration of other atmospheres found on, for example, of exoplanets. For Earth's atmosphere, we visually compare our results with pictures taken from the International Space Station and against the CIE clear sky model. The Martian atmosphere is reproduced based on available scientific data, feedback from domain experts, and is compared to images taken by the Curiosity rover. The work presented here has been implemented in the OpenSpace system, which enables interactive parameter setting and real-time feedback visualization targeting presentations in a wide range of environments, from immersive dome theaters to virtual reality headsets.",
                        "time_start": "2020-10-29T14:45:00Z",
                        "time_end": "2020-10-29T15:00:00Z",
                        "uid": "f-scivis-1137"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Interactive Black-Hole Visualization",
                        "contributors": [
                            "Annemieke Verbraeck"
                        ],
                        "abstract": "We present an efficient algorithm for visualizing the effect of black holes on its distant surroundings as seen from an observer nearby in orbit. Our solution is GPU-based and builds upon a two-step approach, where we first derive an adaptive grid to map the 360-view around the observer to the distorted celestial sky, which can be directly reused for different camera orientations. Using a grid, we can rapidly trace rays back to the observer through the distorted spacetime, avoiding the heavy workload of standard tracing solutions at real-time rates. By using a novel interpolation technique we can also simulate an observer path by smoothly transitioning between multiple grids. Our approach accepts real star catalogues and environment maps of the celestial sky and generates the resulting black-hole deformations in real time.",
                        "time_start": "2020-10-29T15:00:00Z",
                        "time_end": "2020-10-29T15:15:00Z",
                        "uid": "f-scivis-1170"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Polyphorm: Structural Analysis of Cosmological Datasets via Interactive Physarum Polycephalum Visualization",
                        "contributors": [
                            "Oskar Elek"
                        ],
                        "abstract": "This paper introduces Polyphorm, an interactive visualization and model fitting tool that provides a novel approach for investigating cosmological datasets. Through a fast computational simulation method inspired by the behavior of Physarum polycephalum, an unicellular slime mold organism that efficiently forages for nutrients, astrophysicists are able to extrapolate from sparse datasets, such as galaxy maps archived in the Sloan Digital Sky Survey, and then use these extrapolations to inform analyses of a wide range of other data, such as spectroscopic observations captured by the Hubble Space Telescope. Researchers can interactively update the simulation by adjusting model parameters, and then investigate the resulting visual output to form hypotheses about the data. We describe details of Polyphorm's simulation model and its interaction and visualization modalities, and we evaluate Polyphorm through three scientific use cases that demonstrate the effectiveness of our approach.",
                        "time_start": "2020-10-29T15:15:00Z",
                        "time_end": "2020-10-29T15:30:00Z",
                        "uid": "f-scivis-1208"
                    }
                ]
            },
            {
                "title": "Guidelines & Design Spaces",
                "session_id": "f-papers-guidelines-design",
                "chair": [
                    "Cody Dunne"
                ],
                "organizers": [],
                "display_start": "2020-10-29T14:00:00Z",
                "time_start": "2020-10-29T14:00:00Z",
                "time_end": "2020-10-29T15:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "guidelines-design-spaces",
                "discord_channel_id": "768678993787617290",
                "youtube_url": "https://youtu.be/LebtfJxfGGk",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrcGWHw3VRbKx_scVOwY1cSI",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "What Makes a Data-GIF Understandable?",
                        "contributors": [
                            "Xinhuan Shu"
                        ],
                        "abstract": "GIFs are enjoying increasing popularity on social media as a format for data-driven storytelling with visualization; simple visual messages are embedded in short animations that usually last less than 15 seconds and are played in automatic repetition. In this paper, we ask the question, \"What makes a data-GIF understandable?\" While other storytelling formats such as data videos, infographics, or data comics are relatively well studied, we have little knowledge about the design factors and principles for \"data-GIFs\". To close this gap, we provide results from semi-structured interviews and an online study with a total of 118 participants investigating the impact of design decisions on the understandability of data-GIFs. The study and our consequent analysis are informed by a systematic review and structured design space of 108 data-GIFs that we found online. Our results show the impact of design dimensions from our design space such as animation encoding, context preservation, or repetition on viewers' understanding of the GIF's core message. The paper concludes with a list of suggestions for creating more effective Data-GIFs.",
                        "time_start": "2020-10-29T14:00:00Z",
                        "time_end": "2020-10-29T14:15:00Z",
                        "uid": "f-info-1232"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Guidelines For Pursuing and Revealing Data Abstractions",
                        "contributors": [
                            "Alex Bigelow"
                        ],
                        "abstract": "Many data abstraction types, such as networks or set relationships, remain unfamiliar to data workers beyond the visualization research community. We conduct a survey and series of interviews about how people describe their data, either directly or indirectly. We refer to the latter as latent data abstractions. We conduct a Grounded Theory analysis that (1) interprets the extent to which latent data abstractions exist, (2) reveals the far-reaching effects that the interventionist pursuit of such abstractions can have on data workers, (3) describes why and when data workers may resist such explorations, and (4) suggests how to take advantage of opportunities and mitigate risks through transparency about visualization research perspectives and agendas. We then use the themes and codes discovered in the Grounded Theory analysis to develop guidelines for data abstraction in visualization projects. To continue the discussion, we make our dataset open along with a visual interface for further exploration.",
                        "time_start": "2020-10-29T14:15:00Z",
                        "time_end": "2020-10-29T14:30:00Z",
                        "uid": "f-info-1251"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Composition and Configuration Patterns in Multiple-View Visualizations",
                        "contributors": [
                            "Xi Chen"
                        ],
                        "abstract": "Multiple-view visualization (MV) is a layout design technique often employed to help users see a large number of data attributes and values in a single cohesive representation. Because of its generalizability, the MV design has been widely adopted by the visualization community to help users examine and interact with large, complex, and high-dimensional data. However, although ubiquitous, there has been little work to categorize and analyze MVs in order to better understand its design space. As a result, there has been little to no guideline in how to use the MV design effectively. In this paper, we present an in-depth study of how MVs are designed in practice. We focus on two fundamental measures of multiple-view patterns: composition, which quantifies what view types and how many are there; and configuration, which characterizes spatial arrangement of view layouts in the display space. We build a new dataset containing 360 images of MVs collected from IEEE VIS, EuroVis, and PacificVis publications 2011 to 2019, and make fine-grained annotations of view types and layouts for these visualization images. From this data we conduct composition and configuration analyses using quantitative metrics of term frequency and layout topology. We identify common practices around MVs, including relationship of view types, popular view layouts, and correlation between view types and layouts. We combine the findings into a MV recommendation system, providing interactive tools to explore the design space, and support example-based design.",
                        "time_start": "2020-10-29T14:30:00Z",
                        "time_end": "2020-10-29T14:45:00Z",
                        "uid": "f-info-1088"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Comparative Layouts Revisited: Design Space, Guidelines, and Future Directions",
                        "contributors": [
                            "Sehi L'Yi"
                        ],
                        "abstract": "We present a systematic review on three comparative layouts\u2014juxtaposition, superposition, and explicit-encoding\u2014which are information visualization (InfoVis) layouts designed to support comparison tasks. For the last decade, these layouts have served as fundamental idioms in designing many visualization systems. However, we found that the layouts have been used with inconsistent terms and confusion, and the lessons from previous studies are fragmented. The goal of our research is to distill the results from previous studies into a consistent and reusable framework. We review 127 research papers, including 15 papers with quantitative user studies, which employed comparative layouts. We \ufb01rst alleviate the ambiguous boundaries in the design space of comparative layouts by suggesting lucid terminology (e.g., chart-wise and item-wise juxtaposition). We then identify the diverse aspects of comparative layouts, such as the advantages and concerns of using each layout in the real-world scenarios and researchers' approaches to overcome the concerns. Building our knowledge on top of the initial insights gained from the Gleicher et al.'s survey [19], we elaborate on relevant empirical evidence that we distilled from our survey (e.g., the actual effectiveness of the layouts in different study settings) and identify novel facets that the original work did not cover (e.g., the familiarity of the layouts to people). Finally, we show the consistent and contradictory results on the performance of comparative layouts and offer practical implications for using the layouts by suggesting trade-offs and seven actionable guidelines.",
                        "time_start": "2020-10-29T14:45:00Z",
                        "time_end": "2020-10-29T15:00:00Z",
                        "uid": "f-info-1372"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "LineSmooth: An Analytical Framework for Evaluating the Effectiveness of Smoothing Techniques on Line Charts",
                        "contributors": [
                            "Paul Rosen"
                        ],
                        "abstract": "We present a comprehensive framework for evaluating line chart smoothing methods under a variety of visual analytics tasks. Line charts are commonly used to visualize a series of data samples. When the number of samples is large, or the data are noisy, smoothing can be applied to make the signal more apparent. However, there are a wide variety of smoothing techniques available, and the effectiveness of each depends upon both nature of the data and the visual analytics task at hand. To date, the visualization community lacks a summary work for analyzing and classifying the various smoothing methods available. In this paper, we establish a framework, based on 8 measures of the line smoothing effectiveness tied to 8 low-level visual analytics tasks. We then analyze 12 methods coming from 4 commonly used classes of line chart smoothing\u2014rank filters, convolutional filters, frequency domain filters, and subsampling. The results show that while no method is ideal for all situations, certain methods, such as Gaussian filters and topology-based subsampling, perform well in general. Other methods, such as low-pass cutoff filters and Douglas-Peucker subsampling, perform well for specific visual analytics tasks. Almost as importantly, our framework demonstrates that several methods, including the commonly used uniform subsampling, produce low-quality results, and should, therefore, be avoided, if possible.",
                        "time_start": "2020-10-29T15:00:00Z",
                        "time_end": "2020-10-29T15:15:00Z",
                        "uid": "f-vast-1250"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "StackGenVis: Alignment of Data, Algorithms, and Models for Stacking Ensemble Learning Using Performance Metrics",
                        "contributors": [
                            "Angelos Chatzimparmpas"
                        ],
                        "abstract": "In machine learning (ML), ensemble methods\u2014such as bagging, boosting, and stacking\u2014are widely-established approaches that regularly achieve top-notch predictive performance. Stacking (also called \u201cstacked generalization\u201d) is an ensemble method that combines heterogeneous base models, arranged in at least one layer, and then employs another metamodel to summarize the predictions of those models. Although it may be a highly-effective approach for increasing the predictive performance of ML, generating a stack of models from scratch can be a cumbersome trial-and-error process. This challenge stems from the enormous space of available solutions, with different sets of data instances and features that could be used for training, several algorithms to choose from, and instantiations of these algorithms using diverse parameters (i.e., models) that perform differently according to various metrics. In this work, we present a knowledge generation model, which supports ensemble learning with the use of visualization, and a visual analytics system for stacked generalization. Our system, StackGenVis, assists users in dynamically adapting performance metrics, managing data instances, selecting the most important features for a given data set, choosing a set of top-performant and diverse algorithms, and measuring the predictive performance. In consequence, our proposed tool helps users to decide between distinct models and to reduce the complexity of the resulting stack by removing overpromising and underperforming models. The applicability and effectiveness of StackGenVis are demonstrated with two use cases: a real-world healthcare data set and a collection of data related to sentiment/stance detection in texts. Finally, the tool has been evaluated through interviews with three ML experts.",
                        "time_start": "2020-10-29T15:15:00Z",
                        "time_end": "2020-10-29T15:30:00Z",
                        "uid": "f-vast-1197"
                    }
                ]
            },
            {
                "title": "Immersion",
                "session_id": "f-papers-immersion",
                "chair": [
                    "Manuela Waldner"
                ],
                "organizers": [],
                "display_start": "2020-10-29T14:00:00Z",
                "time_start": "2020-10-29T14:00:00Z",
                "time_end": "2020-10-29T15:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "immersion",
                "discord_channel_id": "768679000754094130",
                "youtube_url": "https://youtu.be/eR4LLSwLEBs",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrfr1J7kVelPD_Qhw4SfonOj",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment",
                        "contributors": [
                            "Benjamin Lee"
                        ],
                        "abstract": "Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.",
                        "time_start": "2020-10-29T14:00:00Z",
                        "time_end": "2020-10-29T14:15:00Z",
                        "uid": "f-info-1041"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays",
                        "contributors": [
                            "Patrick Reipschl\u00e4ger",
                            "Tamara Flemisch"
                        ],
                        "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts.\nBased on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.",
                        "time_start": "2020-10-29T14:15:00Z",
                        "time_end": "2020-10-29T14:30:00Z",
                        "uid": "f-info-1243"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "VRIA: A Web-based Framework for Creating Immersive Analytics Experiences",
                        "contributors": [
                            "Peter Butcher"
                        ],
                        "abstract": "We present VRIA, a Web-based framework for creating Immersive Analytics (IA) experiences in Virtual Reality. VRIA is built upon WebVR, A-Frame, React and D3.js, and offers a visualization creation workflow which enables users, of different levels of expertise, to rapidly develop Immersive Analytics experiences for the Web. The use of these open-standards Web-based technologies allows us to implement VR experiences in a browser and offers strong synergies with popular visualization libraries, through the HTML Document Object Model (DOM). This makes VRIA ubiquitous and platform-independent. Moreover, by using WebVR's progressive enhancement, the experiences VRIA creates are accessible on a plethora of devices. We elaborate on our motivation for focusing on open-standards Web technologies, present the VRIA creation workflow and detail the underlying mechanics of our framework. We also report on techniques and optimizations necessary for implementing Immersive Analytics experiences on the Web, discuss scalability implications of our framework, and present a series of use case applications to demonstrate the various features of VRIA. Finally, we discuss current limitations of our framework, the lessons learned from its development, and outline further extensions.",
                        "time_start": "2020-10-29T14:30:00Z",
                        "time_end": "2020-10-29T14:45:00Z",
                        "uid": "f-tvcg-2019080283"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics",
                        "contributors": [
                            "Barrett Ens"
                        ],
                        "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short `bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such `casual collaborative' scenarios will require engaging features to draw users' attention, with intuitive, `walk-up and use' interfaces. This paper presents Uplift, a novel prototype system to support `casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.",
                        "time_start": "2020-10-29T14:45:00Z",
                        "time_end": "2020-10-29T15:00:00Z",
                        "uid": "f-vast-1251"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Revisited: Comparison of Empirical Methods to Evaluate Visualizations Supporting Crafting and Assembly Purposes",
                        "contributors": [
                            "Katrin Angerbauer"
                        ],
                        "abstract": "Ubiquitous, situated, and physical visualizations offer entirely new possibilities for tasks contextualized in the real world, such as doctors inserting needles. During the development of situated visualizations, evaluating them is a core component. However, evaluating them is intrinsically tricky as the real scenarios are safety-critical or expensive to test. Thus, to overcome these issues, researchers and practitioners adapt classical approaches from ubiquitous computing and use surrogate empirical methods such as Augmented Reality (AR), Virtual Reality (VR) prototypes, or merely online demonstrations. The main assumption behind this approach is that meaningful insights can also be gained from different, usually cheaper and less cumbersome empirical methods. Nevertheless, recent efforts in the Human-Computer Interaction (HCI) have found evidence against this assumption, which would impede the use of surrogate empirical methods. Currently, these insights rely on a single investigation of four interactive objects. The goal of this work is to investigate if these prior findings also hold for situated visualizations. Therefore, we first created a scenario where situated visualizations support users in do-it-yourself (DIY) tasks such as crafting and assembly. We then set up five empirical study methods to evaluate the four tasks using an online survey, as well as VR, AR, laboratory, and in-situ studies. Based on those results, we conducted a new study with 60 participants. Our results show that investigating situated visualizations is not prone to the same dependency on the empirical method, as found in previous work. Thus, we argue that when investigating situated visualizations, researchers can conclude from all empirical study methods and comparisons between different methods.",
                        "time_start": "2020-10-29T15:00:00Z",
                        "time_end": "2020-10-29T15:15:00Z",
                        "uid": "f-info-1021"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Embodied Navigation in Immersive Abstract Data Visualization: Is Overview+Detail or Zooming Better for 3D Scatterplots?",
                        "contributors": [
                            "Yalong Yang"
                        ],
                        "abstract": "Abstract data has no natural scale and so interactive data visualizations must provide techniques to allow the user to choose their viewpoint and scale.  Such techniques are well established in desktop visualization tools. The two most common techniques are zoom+pan and overview+detail. However, how best to enable the analyst to navigate and view abstract data at different levels of scale in immersive environments has not previously been studied.  We report the findings of the first systematic study of immersive navigation techniques for 3D scatterplots. We tested four conditions that represent our best attempt to adapt standard 2D navigation techniques to data visualization in an immersive environment while still providing standard immersive navigation techniques through physical movement and teleportation. We compared room-sized visualization versus a zooming interface, each with and without an overview.  We find significant differences in participants' response times and accuracy for a number of standard visual analysis tasks. Both zoom and overview provide benefits over standard locomotion support alone (i.e., physical movement and pointer teleportation). However, which variation is superior, depends on the task. We obtain a more nuanced understanding of the results by analyzing them in terms of a time-cost model for the different components of navigation: way-finding, travel, number of travel steps, and context switching.",
                        "time_start": "2020-10-29T15:15:00Z",
                        "time_end": "2020-10-29T15:30:00Z",
                        "uid": "f-info-1169"
                    }
                ]
            },
            {
                "title": "Event Sequences",
                "session_id": "f-papers-event-seq",
                "chair": [
                    "Siming Chen"
                ],
                "organizers": [],
                "display_start": "2020-10-29T14:00:00Z",
                "time_start": "2020-10-29T14:00:00Z",
                "time_end": "2020-10-29T15:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "event-sequences",
                "discord_channel_id": "768679007218171934",
                "youtube_url": "https://youtu.be/fsZDqbRX-7Q",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrf2G9h2SsanxkJq2lBFr1-x",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports",
                        "contributors": [
                            "Jiang Wu"
                        ],
                        "abstract": "In this work, we propose a generic visual analytics framework to support tactic analysis based on data collected from racquet sports (such as tennis and badminton). The proposed approach models each rally in a game as a sequence of hits (i.e., events) until one athlete scores a point. Each hit can be described with a set of attributes, such as the positions of the ball and the techniques used to hit the ball (such as drive and volley in tennis). Thus, the mentioned sequence of hits can be viewed as a multivariate event sequence. By detecting and analyzing the multivariate subsequences that frequently occur in the rallies (namely, tactical patterns), athletes can gain insights into the playing styles adopted by their opponents, and therefore help them identify systematic weaknesses of the opponents and develop counter strategies in matches. To support such analysis effectively, we propose a steerable multivariate sequential pattern mining algorithm with adjustable weights over event attributes, such that the domain expert can obtain frequent tactical patterns according to the attributes specified by himself. We also propose a re-configurable glyph design to help users simultaneously analyze multiple attributes of the hits. The framework further supports comparative analysis of the tactical patterns, e.g., for different athletes or the same athlete playing under different conditions. By applying the framework on two datasets collected in tennis and badminton matches, we demonstrate that the system is generic and effective for tactic analysis in sports and can help identify signature techniques used by individual athletes. Finally, we discuss the strengths and limitations of the proposed approach based on the feedback from the domain experts.",
                        "time_start": "2020-10-29T14:00:00Z",
                        "time_end": "2020-10-29T14:15:00Z",
                        "uid": "f-vast-1010"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Constructing Spaces and Times for Tactical Analysis in Football",
                        "contributors": [
                            "Gennady Andrienko"
                        ],
                        "abstract": " A possible objective in analyzing trajectories of multiple simultaneously moving objects, such as football players during a game, is to extract and understand the general patterns of coordinated movement in different classes of situations as they develop. For achieving this objective, we propose an approach that includes a combination of query techniques for flexible selection of episodes of situation development, a method for dynamic aggregation of data from selected groups of episodes, and a data structure for representing the aggregates that enables their exploration and use in further analysis. The aggregation, which is meant to abstract general movement patterns, involves construction of new time-homomorphic reference systems owing to iterative application of aggregation operators to a sequence of data selections. As similar patterns may occur at different spatial locations, we also propose constructing new spatial reference systems for aligning and matching movements irrespective of their absolute locations. The approach was tested in application to tracking data from two Bundesliga games of the 2018/2019 season. It enabled detection of interesting and meaningful general patterns of team behaviors in three classes of situations defined by football experts. The experts found the approach and the underlying concepts worth implementing in tools for football analysts.",
                        "time_start": "2020-10-29T14:15:00Z",
                        "time_end": "2020-10-29T14:30:00Z",
                        "uid": "f-tvcg-2019060193"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "PassVizor: Toward Better Understanding of the Dynamics of Soccer Passes",
                        "contributors": [
                            "Xiao Xie"
                        ],
                        "abstract": "In soccer, passing is the most frequent interaction between players and plays a significant role in creating scoring chances. Experts are interested in analyzing players' passing behavior to learn passing tactics, i.e., how players build up an attack with passing. Various approaches have been proposed to facilitate the analysis of passing tactics. However, the dynamic changes of a team's employed tactics over a match have not been comprehensively investigated. To address the problem, we closely collaborate with domain experts and characterize requirements to analyze the dynamic changes of a team's passing tactics. To characterize the passing tactic employed for each attack, we propose a topic-based approach that provides a high-level abstraction of complex passing behaviors. Based on the model, we propose a glyph-based design to reveal the multi-variate information of passing tactics within different phases of attacks, including player identity, spatial context, and formation. We further design and develop PassVizor, a visual analytics system, to support the comprehensive analysis of passing dynamics. With the system, users can detect the changing patterns of passing tactics and examine the detailed passing process for evaluating passing tactics. We invite experts to conduct analysis with PassVizor and demonstrate the usability of the system through an expert interview.",
                        "time_start": "2020-10-29T14:30:00Z",
                        "time_end": "2020-10-29T14:45:00Z",
                        "uid": "f-vast-1229"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Once Upon A Time In Visualization: Understanding the Use of Textual Narratives for Causality",
                        "contributors": [
                            "Arjun Choudhry"
                        ],
                        "abstract": "Causality visualization can help people understand temporal chains of events, such as messages sent in a distributed system, cause and effect in a historical conflict, or interplay between political actors over time. However, as the scale and complexity of these event sequences grows, even these visualizations can become overwhelming to use. In this paper, we propose the use of textual narratives as a data-driven storytelling method to augment causality visualization. We first propose a design space for how textual narratives can be used to describe causal data. We then present results from a crowdsourced user study where participants were asked to recover causality information from two causality visualizations - causal graphs and Hasse diagrams - with and without an associated textual narrative. Finally, we describe CAUSEWORKS, a causality visualization system for understanding how specific interventions influence a causal model. The system incorporates an automatic textual narrative mechanism based on our design space. We validate CAUSEWORKS through interviews with experts who used the system for understanding complex events.",
                        "time_start": "2020-10-29T14:45:00Z",
                        "time_end": "2020-10-29T15:00:00Z",
                        "uid": "f-vast-1136"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visual Causality Analysis of Event Sequence Data",
                        "contributors": [
                            "Zhuochen Jin"
                        ],
                        "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.",
                        "time_start": "2020-10-29T15:00:00Z",
                        "time_end": "2020-10-29T15:15:00Z",
                        "uid": "f-vast-1152"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Sequence Braiding: Visual Overviews of Temporal Event Sequences and Attributes",
                        "contributors": [
                            "Sara DiBartolomeo"
                        ],
                        "abstract": "Temporal event sequence alignment has been used in many domains to visualize nuanced changes and interactions over time. Existing approaches align one or two sentinel events. Overview tasks require examining all alignments of interest using interaction and time or juxtaposition of many visualizations. Furthermore, any event attribute overviews are not closely tied to sequence visualizations. We present Sequence Braiding, a novel overview visualization for temporal event sequences and attributes using a layered directed acyclic network. Sequence Braiding visually aligns many temporal events and attribute groups simultaneously and supports arbitrary ordering, absence, and duplication of events. In a controlled experiment we compare Sequence Braiding and IDMVis on user task completion time, correctness, error, and confidence. Our results provide good evidence that users of Sequence Braiding can understand high-level patterns and trends faster and with similar error.",
                        "time_start": "2020-10-29T15:15:00Z",
                        "time_end": "2020-10-29T15:30:00Z",
                        "uid": "f-info-1377"
                    }
                ]
            },
            {
                "title": "Urban Spaces",
                "session_id": "f-papers-urban-space",
                "chair": [
                    "Gennady Andrienko"
                ],
                "organizers": [],
                "display_start": "2020-10-29T16:00:00Z",
                "time_start": "2020-10-29T16:00:00Z",
                "time_end": "2020-10-29T17:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "urban-spaces",
                "discord_channel_id": "768679039618908221",
                "youtube_url": "https://youtu.be/P0DR4uMkDww",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojre3-c_hOIsMY1DpKRgiSvzs",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Towards Better Bus Networks: A Visual Analytics Approach",
                        "contributors": [
                            "Di Weng"
                        ],
                        "abstract": "Bus routes are typically updated every 3--5 years to meet constantly changing travel demands. However, identifying deficient bus routes and finding their optimal replacements remain challenging due to the difficulties in analyzing a complex bus network and the large solution space comprising alternative routes. Most of the automated approaches cannot produce satisfactory results in real-world settings without laborious inspection and evaluation of the candidates. The limitations observed in these approaches motivate us to collaborate with domain experts and propose a visual analytics solution for the performance analysis and incremental planning of bus routes based on an existing bus network. Developing such a solution involves three major challenges, namely, a) the in-depth analysis of complex bus route networks, b) the interactive generation of improved route candidates, and c) the effective evaluation of alternative bus routes. For challenge a, we employ an overview-to-detail approach by dividing the analysis of a complex bus network into three levels to facilitate the efficient identification of deficient routes. For challenge b, we improve a route generation model and interpret the performance of the generation with tailored visualizations. For challenge c, we incorporate a conflict resolution strategy in the progressive decision-making process to assist users in evaluating the alternative routes and finding the most optimal one. The proposed system is evaluated with two usage scenarios based on real-world data and received positive feedback from the experts.",
                        "time_start": "2020-10-29T16:00:00Z",
                        "time_end": "2020-10-29T16:15:00Z",
                        "uid": "f-vast-1235"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Topology Density Map for Urban Data Visualization and Analysis",
                        "contributors": [
                            "Zezheng Feng"
                        ],
                        "abstract": "Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG, dividing the underlying territory into planar regions using a weighted Voronoi diagram, and calculating the scalar fields for every point. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.",
                        "time_start": "2020-10-29T16:15:00Z",
                        "time_end": "2020-10-29T16:30:00Z",
                        "uid": "f-vast-1274"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "UrbanMotion: Visual Analysis of Metropolitan-Scale Sparse Trajectories",
                        "contributors": [
                            "Lei Shi"
                        ],
                        "abstract": "Visualizing massive scale human movement in cities plays an important role in solving many of the problems that modern cities face (e.g., traffic optimization, business site configuration). In this work, we study a big mobile location dataset that covers millions of city residents, but is temporally sparse on the trajectory of individual user. Mapping sparse trajectories to illustrate population movement poses several challenges from both analysis and visualization perspectives. In the literature, there are a few techniques designed for sparse trajectory visualization; yet they do not consider trajectories collected from mobile apps that possess long-tailed sparsity with record intervals as long as hours. This work introduces UrbanMotion, a visual analytics system that extends the original wind map design by supporting map-matched local movements, multi-directional population flows, and population distributions. Effective methods are proposed to extract and aggregate population movements from dense parts of the trajectories leveraging their long-tailed sparsity. Both characteristic and anomalous patterns are discovered and visualized. We conducted three case studies, one comparative experiment, and collected expert feedback in the application domains of commuting analysis, event detection, and business site configuration. The study result demonstrates the significance and effectiveness of our system in helping to complete key analytics tasks for urban users.",
                        "time_start": "2020-10-29T16:30:00Z",
                        "time_end": "2020-10-29T16:45:00Z",
                        "uid": "f-tvcg-2019080284"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics",
                        "contributors": [
                            "Wei Zeng"
                        ],
                        "abstract": "Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions \u2013 rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran's I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.",
                        "time_start": "2020-10-29T16:45:00Z",
                        "time_end": "2020-10-29T17:00:00Z",
                        "uid": "f-vast-1045"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visual Cause Analytics for Traffic Congestion",
                        "contributors": [
                            "Hanbyul Yeon"
                        ],
                        "abstract": "Urban traffic congestion has become an important issue not only affecting our daily lives, but also limiting economic development. The primary cause of urban traffic congestion is that the number of vehicles is higher than the permissible limit of the road. Previous studies have focused on dispersing traffic volume by detecting urban traffic congestion zones and predicting future trends. However, to solve the fundamental problem, it is necessary to discover the cause of traffic congestion. Nevertheless, it is difficult to find a research which presents an approach to identify the causes of traffic congestion. In this paper, we propose a technique to analyze the cause of traffic congestion based on the traffic flow theory. We extract vehicle flows from traffic data, such as GPS trajectory and Vehicle Detector data. We detect vehicle flow changes utilizing the entropy from the information theory. Then, we build cumulative vehicle count curves (N-curve) that can quantify the flow of the vehicles in the traffic congestion area. The N-curves are classified into four different traffic congestion patterns by a convolutional neural network. Analyzing the causes and influence of traffic congestion is difficult and requires considerable experience and knowledge. Therefore, we present a visual analytics system that can efficiently perform a series of processes to analyze the cause and influence of traffic congestion. Through case studies, we have evaluated that our system can classify the causes of traffic congestion and can be used efficiently in road planning.",
                        "time_start": "2020-10-29T17:00:00Z",
                        "time_end": "2020-10-29T17:15:00Z",
                        "uid": "f-tvcg-20192940580"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "CrimAnalyzer: Understanding Crime Patterns in S\u00e3o Paulo",
                        "contributors": [
                            "Germain Garcia",
                            "Jaqueline Silveira",
                            "Jorge Poco",
                            "Afonso Paiva",
                            "Marcelo Batista Nery",
                            "Claudio T. Silva",
                            "Sergio Adorno",
                            "Luis Gustavo Nonato"
                        ],
                        "abstract": "S\u00e3o Paulo is the largest city in South America, with crime rates that reflect its size. The number and type of crimes vary considerably around the city, assuming different patterns depending on urban and social characteristics of each particular location. Previous works have mostly focused on the analysis of crimes with the intent of uncovering patterns associated to social factors, seasonality, and urban routine activities. Therefore, those studies and tools are more global in the sense that they are not designed to investigate specific regions of the city such as particular neighborhoods, avenues, or public areas. Tools able to explore specific locations of the city are essential for domain experts to accomplish their analysis in a bottom-up fashion revealing how urban features related to mobility, passersby behavior, and presence of public infrastructures (e.g., terminals of public transportation and schools) can influence the quantity and type of crimes. In this paper, we present CrimAnalyzer, a visual analytic tool that allows users to study the behavior of crimes in specific regions of a city. The system allows users to identify local hotspots and the pattern of crimes associated to them, while still showing how hotspots and corresponding crime patterns change over time. CrimAnalyzer has been developed from the needs of a team of experts in criminology and deals with three major challenges: i) flexibility to explore local regions and understand their crime patterns, ii) identification of spatial crime hotspots that might not be the most prevalent ones in terms of the number of crimes but that are important enough to be investigated, and iii) understand the dynamic of crime patterns over time. The effectiveness and usefulness of the proposed system are demonstrated by qualitative and quantitative comparisons as well as by case studies run by domain experts involving real data. The experiments show the capability of CrimAnalyzer in identifying crime-related phenomena.",
                        "time_start": "2020-10-29T17:15:00Z",
                        "time_end": "2020-10-29T17:30:00Z",
                        "uid": "f-tvcg-2018110405"
                    }
                ]
            },
            {
                "title": "Communities & Communication",
                "session_id": "f-papers-communities-communication",
                "chair": [
                    "Jeremy Boy"
                ],
                "organizers": [],
                "display_start": "2020-10-29T16:00:00Z",
                "time_start": "2020-10-29T16:00:00Z",
                "time_end": "2020-10-29T17:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "communities-communication",
                "discord_channel_id": "768679047516389406",
                "youtube_url": "https://youtu.be/tp_STZZA148",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrf8VVJhHIrA58By5qRKQZsb",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Chemicals in the Creek: designing a situated data physicalization of open government data with the community",
                        "contributors": [
                            "Laura Perovich"
                        ],
                        "abstract": "Over the last decade growing amounts of government data have been made available in an attempt to increase transparency and civic participation, but it is unclear if this data serves non-expert communities due to gaps in access and the technical knowledge needed to interpret this \u201copen\u201d data. We conducted a two-year design study focused on the creation of a community-based data display using the United States Environmental Protection Agency data on water permit violations by oil storage facilities on the Chelsea Creek in Massachusetts to explore whether situated data physicalization and Participatory Action Research could support meaningful engagement with open data. We selected this data as it is of interest to local groups and available online, yet remains largely invisible and inaccessible to the Chelsea community. The resulting installation, Chemicals in the Creek, responds to the call for community-engaged visualization processes and provides an application of situated methods of data representation. It proposes event-centered and power-aware modes of engagement using contextual and embodied data representations. The design of Chemicals in the Creek is grounded in interactive workshops and we analyze it through event observation, interviews, and community outcomes. We reflect on the role of community engaged research in the Information Visualization community relative to recent conversations on new approaches to design studies and evaluation.",
                        "time_start": "2020-10-29T16:00:00Z",
                        "time_end": "2020-10-29T16:15:00Z",
                        "uid": "f-info-1210"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Designing Narrative-Focused Role-Playing Games for Visualization Literacy in Young Children",
                        "contributors": [
                            "Elaine Huynh"
                        ],
                        "abstract": "Building on game design and education research, this paper introduces narrative-focused role-playing games as a way to promote visualization literacy in young children. Visualization literacy skills are vital in understanding the world around us and constructing meaningful visualizations, yet, how to better develop these skills at an early age remains largely overlooked and understudied. Only recently has the visualization community started to fill this gap, resulting in preliminary studies and development of educational tools for use in early education. We add to these efforts through the exploration of gamification to support learning, and identify an opportunity to apply role-playing game-based designs by leveraging the presence of narratives in data-related problems involving visualizations. We study the effects of including narrative elements on learning through a technology probe, grounded in a set of design considerations stemming from visualization, game design and education science. We create two versions of a game \u2013 one with narrative elements and one without \u2013 and evaluate our instances on 33 child participants between 11- to 13-years old using a between-subjects study design. Despite participants requiring double the amount of time to complete their game due to additional narrative elements, the inclusion of such elements were found to improve engagement without sacrificing learning; our results indicate no significant differences in development of graph-reading skills, but significant differences in engagement and overall enjoyment of the game. We report observations and qualitative feedback collected, and note areas for improvement and room for future work.",
                        "time_start": "2020-10-29T16:15:00Z",
                        "time_end": "2020-10-29T16:30:00Z",
                        "uid": "f-info-1366"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Sea of Genes: A Reflection on Visualising Metagenomic Data for Museums",
                        "contributors": [
                            "Keshav Dasu"
                        ],
                        "abstract": "We examine the process of designing an exhibit to communicate scientific findings from a complex dataset and unfamiliar domain to the public in a science museum. Our exhibit sought to communicate new lessons based on scientific findings from the domain of metagenomics. This multi-user exhibit had three goals: (1) to inform the public about microbial communities and their daily cycles; (2) to link microbes' activity to the concept of gene expression; (3) and to highlight scientists' use of gene expression data to understand the role of microbes. To address these three goals, we derived visualization designs with three corresponding stories, each corresponding to a goal. We present  three successive rounds of design and evaluation of our attempts to convey these goals. We could successfully present one story but had limited success with our second and third goals. This work presents a detailed account of an attempt to explain tightly coupled relationships through storytelling and animation in a multi-user, informal learning environment to a public with varying prior knowledge on the domain and identify lessons for future design.",
                        "time_start": "2020-10-29T16:30:00Z",
                        "time_end": "2020-10-29T16:45:00Z",
                        "uid": "f-scivis-1076"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Communicative Visualizations as a Learning Problem",
                        "contributors": [
                            "Eytan Adar"
                        ],
                        "abstract": "Significant research has provided robust task and evaluation languages for the analysis of exploratory visualizations. Unfortunately, these taxonomies fail when applied to communicative visualization. Instead, designers often resort to evaluating communicative visualizations from the cognitive efficiency perspective: \"can the recipient accurately decode my message/insight?\" However, designers are unlikely to be satisfied if the message went 'in one ear and out the other.' The consequence of this inconsistency is that it is difficult to design or select between competing options in a principled way. The problem we address is the fundamental mismatch between how designers want to describe their intent, and the language they have.  We argue that visualization designers can address this limitation through a learning lens: that the recipient is a student and the designer a teacher. By using learning objectives, designers can better define, assess, and compare communicative visualizations. We illustrate how the learning-based approach provides a framework for understanding a wide array of communicative goals.  To understand how the framework can be applied (and its limitations), we surveyed and interviewed members of the Data Visualization Society using their own visualizations as a probe. Through this study we identified the broad range of objectives in communicative visualizations and the prevalence of certain objective types.",
                        "time_start": "2020-10-29T16:45:00Z",
                        "time_end": "2020-10-29T17:00:00Z",
                        "uid": "f-info-1451"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Table Scraps: An Actionable Framework for Multi-Table Data Wrangling From An Artifact Study of Computational Journalism",
                        "contributors": [
                            "Stephen Kasica"
                        ],
                        "abstract": "For the many journalists who use data and computation to report the news, data wrangling is an integral part of their work.Despite an abundance of literature on data wrangling in the context of enterprise data analysis, little is known about the specific operations, processes, and pain points journalists encounter while performing this tedious, time-consuming task. To better understand the needs of this user group, we conduct a technical observation study of 50 public repositories of data and analysis code authored by 33 professional journalists at 26 news organizations. We develop two detailed and cross-cutting taxonomies of data wrangling in computational journalism, for actions and for processes. We observe the extensive use of multiple tables, a notable gap in previous wrangling analyses.  We develop a concise, actionable framework for general multi-table data wrangling that includes wrangling operations documented in our taxonomy that are without clear parallels in other work. This framework, the first to incorporate tablesas first-class objects, will support future interactive wrangling tools for both computational journalism and general-purpose use. We assess the generative and descriptive power of our framework through discussion of its relationship to our set of taxonomies.",
                        "time_start": "2020-10-29T17:00:00Z",
                        "time_end": "2020-10-29T17:15:00Z",
                        "uid": "f-info-1312"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Data Comics for Reporting Controlled User Studies in Human-Computer Interaction",
                        "contributors": [
                            "Zezhong Wang"
                        ],
                        "abstract": "Inspired by data comics, this paper introduces a novel format for reporting controlled studies in the domain of human-computer interaction (HCI). While many studies in HCI follow similar steps in explaining hypotheses, laying out a study design, and reporting results, many of these decisions are buried in blocks of dense scientific text. We propose leveraging data comics as study reports to provide an open and glanceable view of studies by tightly integrating text and images, illustrating design decisions and key insights visually, resulting in visual narratives that can be compelling to non-scientists and researchers alike. Use cases of data comics study reports range from illustrations for non-scientific audiences to graphical abstracts, study summaries, technical talks, textbooks, teaching, blogs, supplementary submission material, and inclusion in scientific articles. This paper provides examples of data comics study reports alongside a graphical repertoire of examples, embedded in a framework of guidelines for creating comics reports which was iterated upon and evaluated through a series of collaborative design sessions.",
                        "time_start": "2020-10-29T17:15:00Z",
                        "time_end": "2020-10-29T17:30:00Z",
                        "uid": "f-info-1288"
                    }
                ]
            },
            {
                "title": "Geospatial Data",
                "session_id": "f-papers-geospatial",
                "chair": [
                    "Aidan Slingsby"
                ],
                "organizers": [],
                "display_start": "2020-10-29T16:00:00Z",
                "time_start": "2020-10-29T16:00:00Z",
                "time_end": "2020-10-29T17:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "geospatial-data",
                "discord_channel_id": "768679054832041994",
                "youtube_url": "https://youtu.be/WwnVRMEYG5I",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrfas2UD91qC_dLcLeoUeFPh",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Cartographic Relief Shading with Neural Networks",
                        "contributors": [
                            "Magnus Heitzler"
                        ],
                        "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.",
                        "time_start": "2020-10-29T16:00:00Z",
                        "time_end": "2020-10-29T16:15:00Z",
                        "uid": "f-info-1130"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Simple Pipeline for Coherent Grid Maps",
                        "contributors": [
                            "Max Sondag"
                        ],
                        "abstract": "Grid maps are spatial arrangements of simple tiles (often squares or hexagons), each of which represents a spatial element. They are an established, effective way to show complex data per spatial element, using visual encodings within each tile ranging from simple coloring to nested small-multiples visualizations. An effective grid map is coherent with the underlying geographic space: the tiles maintain the contiguity, neighborhoods and identifiability of the corresponding spatial elements, while the grid map as a whole maintains the global shape of the input. Of particular importance are salient local features of the global shape which need to be represented by tiles assigned to the appropriate spatial elements. State-of-the-art techniques can adequately deal only with simple cases, such as close-to-uniform spatial distributions or global shapes that have few characteristic features. We introduce a simple fully-automated 3-step pipeline for computing coherent grid maps. Each step is a well-studied problem: shape decomposition based on salient features, tile-based Mosaic Cartograms, and point-set matching. Our pipeline is a seamless composition of existing techniques for these problems and results in high-quality grid maps. We provide an implementation, demonstrate the efficacy of our approach on various complex datasets, and compare it to the state-of-the-art.",
                        "time_start": "2020-10-29T16:15:00Z",
                        "time_end": "2020-10-29T16:30:00Z",
                        "uid": "f-info-1166"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Zoomless Maps: External Labeling Methods for the Interactive Exploration of Dense Point Sets at a Fixed Map Scale",
                        "contributors": [
                            "Sven Gedicke"
                        ],
                        "abstract": "Visualizing spatial data on small-screen devices such as smartphones and smartwatches poses new challenges in\ncomputational cartography. The current interfaces for map exploration require their users to zoom in and out frequently. Indeed,\nzooming and panning are tools suitable for choosing the map extent corresponding to an area of interest. They are not as suitable,\nhowever, for resolving the graphical clutter caused by a high feature density since zooming in to a large map scale leads to a loss\nof context. Therefore, in this paper, we present new external labeling methods that allow a user to navigate through dense sets of\npoints of interest while keeping the current map extent fixed. We provide a unified model, in which labels are placed at the boundary\nof the map and visually associated with the corresponding features via connecting lines, which are called leaders. Since the screen\nspace is limited, labeling all features at the same time is impractical. Therefore, at any time, we label a subset of the features. We offer\ninteraction techniques to change the current selection of features systematically and, thus, give the user access to all features. We\ndistinguish three methods, which allow the user either to slide the labels along the bottom side of the map or to browse the labels\nbased on pages or stacks. We present a generic algorithmic framework that provides us with the possibility of expressing the different\nvariants of interaction techniques as optimization problems in a unified way. We propose both exact algorithms and fast and simple\nheuristics that solve the optimization problems taking into account different criteria such as the ranking of the labels, the total leader\nlength as well as the distance between leaders. In experiments on real-world data we evaluate these algorithms and discuss the three\nvariants with respect to their strengths and weaknesses proving the flexibility of the presented algorithmic framework.",
                        "time_start": "2020-10-29T16:30:00Z",
                        "time_end": "2020-10-29T16:45:00Z",
                        "uid": "f-info-1290"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Tilt Map: Interactive Transitions Between Choropleth Map, Prism Map and Bar Chart in Immersive Environments",
                        "contributors": [
                            "Yalong Yang"
                        ],
                        "abstract": "We introduce Tilt Map, a novel interaction technique for intuitively transitioning between 2D and 3D map visualisations in immersive environments. Our focus is visualising data associated with areal features on maps, for example, population density by state. Tilt Map transitions from 2D choropleth maps to 3D prism maps to 2D bar charts to overcome the limitations of each. Our paper includes two user studies. The first study compares subjects' task performance interpreting population density data using 2D choropleth maps and 3D prism maps in virtual reality (VR). We observed greater task accuracy with prism maps, but faster response times with choropleth maps. The complementarity of these views inspired our hybrid Tilt Map design. Our second study compares Tilt Map to: a side-by-side arrangement of the various views; and interactive toggling between views. The results indicate benefits for Tilt Map in user preference; and accuracy (versus side-by-side) and time (versus toggle).",
                        "time_start": "2020-10-29T16:45:00Z",
                        "time_end": "2020-10-29T17:00:00Z",
                        "uid": "f-tvcg-2020010025"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Phoenixmap: An Abstract Approach to Visualize 2D Spatial Distributions",
                        "contributors": [
                            "Junhan Zhao"
                        ],
                        "abstract": "The multidimensional nature of spatial data poses a challenge for visualization. In this paper, we introduce Phoenixmap, a simple abstract visualization method to address the issue of visualizing multiple spatial distributions at once. The Phoenixmap approach starts by identifying the enclosed outline of the point collection, then assigns different widths to outline segments according to the segments' corresponding inside regions. Thus, one 2D distribution is represented as an outline with varied thicknesses. Phoenixmap is capable of overlaying multiple outlines and comparing them across categories of objects in a 2D space. We chose heatmap as a benchmark spatial visualization method and conducted user studies to compare performances among Phoenixmap, heatmap, and dot distribution map. Based on the analysis and participant feedback, we demonstrate that Phoenixmap1) allows users to perceive and compare spatial distribution data efficiently; 2) frees up graphics space with a concise form that can provide broad visualization design possibilities like overlapping; and 3) provides a good quantitative perceptual estimating capability given the proper legends. Finally, we discuss several possible applications of Phoenixmapand present one visualization of multiple species of birds' active regions in a nature preserve.",
                        "time_start": "2020-10-29T17:00:00Z",
                        "time_end": "2020-10-29T17:15:00Z",
                        "uid": "f-tvcg-2019030083"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "MetroSets: Visualizing Sets as Metro Maps",
                        "contributors": [
                            "Ben Jacobsen"
                        ],
                        "abstract": "We propose MetroSets, a new, flexible online tool for visualizing set systems using the metro map metaphor. We model a given set system as a hypergraph H = (V; S), consisting of a set V of vertices and a set S, which contains subsets of V called hyperedges. Our system then computes a metro map representation of H, where each hyperedge E in S corresponds to a metro line and each vertex corresponds to a metro station. Vertices that appear in two or more hyperedges are drawn as interchanges in the metro map, connecting the different sets. MetroSets is based on a modular 4-step pipeline which constructs and optimizes a path-based hypergraph support, which is then drawn and schematized using metro map layout algorithms. We propose and implement multiple algorithms for each step of the MetroSet pipeline and provide a functional prototype with easy-to-use preset configurations. Furthermore, using several real-world datasets, we perform an extensive quantitative evaluation of the impact of different pipeline stages on desirable properties of the generated maps, such as octolinearity, monotonicity, and edge uniformity.",
                        "time_start": "2020-10-29T17:15:00Z",
                        "time_end": "2020-10-29T17:30:00Z",
                        "uid": "f-info-1352"
                    }
                ]
            },
            {
                "title": "Intelligent Systems",
                "session_id": "f-papers-intelligent-sys",
                "chair": [
                    "Paolo Buono"
                ],
                "organizers": [],
                "display_start": "2020-10-29T16:00:00Z",
                "time_start": "2020-10-29T16:00:00Z",
                "time_end": "2020-10-29T17:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "intelligent-systems",
                "discord_channel_id": "768679061756444672",
                "youtube_url": "https://youtu.be/ffqeJAAc6ro",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrc3A0BC8c-klNx-shu3oM8z",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Interweaving Multimodal Interaction with Flexible Unit Visualizations for Data Exploration",
                        "contributors": [
                            "Arjun Srinivasan"
                        ],
                        "abstract": "Multimodal interfaces that combine direct manipulation and natural language have shown great promise for data visualization. Such multimodal interfaces allow people to stay in the flow of their visual exploration by leveraging the strengths of one modality to complement the weaknesses of others. In this work, we introduce an approach that interweaves multimodal interaction combining direct manipulation and natural language with flexible unit visualizations. We employ the proposed approach in a proof-of-concept system, DataBreeze. Coupling pen, touch, and speech-based multimodal interaction with flexible unit visualizations, DataBreeze allows people to create and interact with both systematically bound (e.g., scatterplots, unit column charts) and manually customized views, enabling a novel visual data exploration experience.  We describe our design process along with DataBreeze's interface and interactions, delineating specific aspects of the design that empower the synergistic use of multiple modalities. We also present a preliminary user study with DataBreeze, highlighting the data exploration patterns that participants employed. Finally, reflecting on our design process and preliminary user study, we discuss future research directions.",
                        "time_start": "2020-10-29T16:00:00Z",
                        "time_end": "2020-10-29T16:15:00Z",
                        "uid": "f-tvcg-2019080302"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Competing Models: Inferring Exploration Patterns and Information Relevance via Bayesian Model Selection",
                        "contributors": [
                            "Shayan Monadjemi"
                        ],
                        "abstract": "Analyzing interaction data provides an opportunity to learn about users, uncover their underlying goals, and create intelligent visualization systems. The first step for intelligent response in visualizations is to enable computers to infer user goals and strategies through observing their interactions with a system. Researchers have proposed multiple techniques to model users, however, their frameworks often depend on the visualization design, interaction space, and dataset. Due to these dependencies, many techniques do not provide a general algorithmic solution to user exploration modeling. In this paper, we construct a series of models based on the dataset and pose user exploration modeling as a Bayesian model selection problem where we maintain a belief over numerous competing models that could explain user interactions. Each of these competing models represent an exploration strategy the user could adopt during a session. The goal of our technique is to make high-level and in-depth inferences about the user by observing their low-level interactions. Although our proposed idea is applicable to various probabilistic model spaces, we demonstrate a specific instance of encoding exploration patterns as competing models to infer information relevance. We validate our technique's ability to infer exploration bias, predict future interactions, and summarize an analytic session using user study datasets. Our results indicate that depending on the application, our method outperforms established baselines for bias detection and future interaction prediction. Finally, we discuss future research directions based on our proposed modeling paradigm and suggest how practitioners can use this method to build intelligent visualization systems that understand users' goals and adapt to improve the exploration process.",
                        "time_start": "2020-10-29T16:15:00Z",
                        "time_end": "2020-10-29T16:30:00Z",
                        "uid": "f-vast-1198"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "II-20: Intelligent and pragmatic analytic categorization of image collections",
                        "contributors": [
                            "Jan Zah\u00e1lka"
                        ],
                        "abstract": "In this paper, we introduce II-20 (Image Insight 2020), a multimedia analytics approach for analytic categorization of image collections. Advanced visualizations for image collections exist, but they need tight integration with a machine model to support the task of analytic categorization. Directly employing computer vision and interactive learning techniques gravitates towards search. Analytic categorization, however, is not machine classification (the difference between the two is called the pragmatic gap): a human adds/redefines/deletes categories of relevance on the fly to build insight, whereas the machine classifier is rigid and non-adaptive. Analytic categorization that truly brings the user to insight requires a flexible machine model that allows dynamic sliding on the exploration-search axis, as well as semantic interactions: a human thinks about image data mostly in semantic terms. II-20 brings three major contributions to multimedia analytics on image collections and towards closing the pragmatic gap. Firstly, a new machine model that closely follows the user's interactions and dynamically models her categories of relevance. II-20's machine model, in addition to matching and exceeding the state of the art's ability to produce relevant suggestions, allows the user to dynamically slide on the exploration-search axis without any additional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris metaphor that synergizes with the model. It allows a well-trained model to analyze the collection by itself with minimal interaction from the user and complements the classic grid metaphor. Thirdly, the fast-forward interaction, allowing the user to harness the model to quickly expand (\"fast-forward'\") the categories of relevance, expands the multimedia analytics semantic interaction dictionary. Automated experiments show that II-20's machine model outperforms the existing state of the art and also demonstrate the Tetris metaphor's analytic quality. User studies further confirm that II-20 is an intuitive, efficient, and effective multimedia analytics tool.",
                        "time_start": "2020-10-29T16:30:00Z",
                        "time_end": "2020-10-29T16:45:00Z",
                        "uid": "f-vast-1109"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Interactive Steering of Hierarchical Clustering",
                        "contributors": [
                            "Weikai Yang"
                        ],
                        "abstract": "We present an interactive steering method to visually supervise constrained hierarchical clustering by utilizing both public knowledge (e.g., Wikipedia) and private knowledge from users. The novelty of our approach includes 1) automatically constructing constraints for hierarchical clustering using knowledge (knowledge-driven) and intrinsic data distribution (data-driven), and 2) enabling the interactive steering of clustering through a visual interface (user-driven). Our method first maps each data item to the most relevant items in a knowledge base. An initial constraint tree is then extracted using the ant colony optimization algorithm. The algorithm balances the tree width and depth and covers the data items with high confidence. Given the constraint tree, the data items are hierarchically clustered using evolutionary Bayesian rose tree. To clearly convey the hierarchical clustering results, an uncertainty-aware tree visualization has been developed to enable users to quickly locate the most uncertain sub-hierarchies and interactively improve them. The quantitative evaluation and case study demonstrate that the proposed approach facilitates the building of customized clustering trees in an efficient and effective manner.",
                        "time_start": "2020-10-29T16:45:00Z",
                        "time_end": "2020-10-29T17:00:00Z",
                        "uid": "f-tvcg-2019090341"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "The Making of Continuous Colormaps",
                        "contributors": [
                            "Pascal Nardini"
                        ],
                        "abstract": "Continuous colormaps are integral parts of many visualization techniques, such as heat-maps, surface plots, and flow visualization. Despite that the critiques of rainbow colormaps have been around and well-acknowledged for three decades, rainbow colormaps are still widely used today. One reason behind the resilience of rainbow colormaps is the lack of tools for users to create a continuous colormap that encodes semantics specific to the application concerned. In this paper, we present a web-based software system, CCC-Tool (short for Charting Continuous Colormaps) under the URL https://ccctool.com, for creating, editing, and analyzing such application-specific colormaps. We introduce the notion of \u201ccolormap specification (CMS)\u201d that maintains the essential semantics required for defining a color mapping scheme. We provide users with a set of advanced utilities for constructing CMS's with various levels of complexity, examining their quality attributes using different plots, and exporting them to external application software. We present two case studies, demonstrating that the CCC-Tool can help domain scientists as well as visualization experts in designing semantically-rich colormaps.",
                        "time_start": "2020-10-29T17:00:00Z",
                        "time_end": "2020-10-29T17:15:00Z",
                        "uid": "f-tvcg-2019010013"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Argus: Interactive a priori Power Analysis",
                        "contributors": [
                            "Xiaoyi Wang"
                        ],
                        "abstract": "A key challenge HCI researchers face when designing a controlled experiment is choosing the appropriate number of participants, or sample size. A prior power analysis examines the relationships among multiple parameters, including the complexity associated with human participants, e.g., order and fatigue effects, to calculate the statistical power of a given experiment design. We created Argus, a tool that supports interactive exploration of statistical power: Researchers specify experiment design scenarios with varying confounds and effect sizes. Argus then simulates data and visualizes statistical power across these scenarios, which lets researchers interactively weigh various trade-offs and make informed decisions about sample size. We describe the design and implementation of Argus, a usage scenario designing a visualization experiment, and a think-aloud study.",
                        "time_start": "2020-10-29T17:15:00Z",
                        "time_end": "2020-10-29T17:30:00Z",
                        "uid": "f-vast-1070"
                    }
                ]
            },
            {
                "title": "Topology & Scalar Fields",
                "session_id": "f-papers-topo-scalar-field",
                "chair": [
                    "Hamish Carr"
                ],
                "organizers": [],
                "display_start": "2020-10-29T18:00:00Z",
                "time_start": "2020-10-29T18:00:00Z",
                "time_end": "2020-10-29T19:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "topology-scalar-fields",
                "discord_channel_id": "768679092186120213",
                "youtube_url": "https://youtu.be/gSQglICXNd0",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrcHVv1JzEkaf4_xbxTU2h5U",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "TopoMap: A 0-dimensional Homology Preserving Projection of High-Dimensional Data",
                        "contributors": [
                            "Julien Tierny"
                        ],
                        "abstract": "Multidimensional Projection is a fundamental tool for high-dimensional data analytics and visualization. With very few exceptions, projection techniques are designed to map data from a high-dimensional space to a visual space so as to preserve some dissimilarity (similarity) measure, such as the Euclidean distance for example. In fact, although adopting distinct mathematical formulations designed to favor different aspects of the data, most multidimensional projection methods strive to preserve dissimilarity measures that encapsulate geometric properties such as distances or the proximity relation between data objects. However, geometric relations is not the only interesting property to be preserved in a projection. For instance, the analysis of particular structures such as clusters and outliers could be more reliably performed if the mapping process gives some guarantee as to topological invariants such as connected components and loops. This paper introduces TopoMap, a novel projection technique which provides topological guarantees during the mapping process. In particular, the proposed method performs the mapping from a high-dimensional space to a visual space, while preserving the 0-dimensional persistence diagram of the Rips filtration of the high-dimensional data, ensuring that the filtrations generate the same connected components when applied to the original as well as projected data. The presented case studies show that the topological guarantee provided by TopoMap not only brings confidence to the visual analytic process but also can be used to assist in the assessment of other projection methods.",
                        "time_start": "2020-10-29T18:00:00Z",
                        "time_end": "2020-10-29T18:15:00Z",
                        "uid": "f-scivis-1049"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Localized Topological Simplification of Scalar Data",
                        "contributors": [
                            "Jonas Lukasczyk"
                        ],
                        "abstract": "This paper describes a localized algorithm for the topological simplification of scalar data, an essential pre-processing step of topological data analysis (TDA). Given a scalar field f and a selection of extrema to preserve, the proposed localized topological simplification (LTS) derives a function g that is close to f and only exhibits the selected set of extrema. Specifically, sub- and superlevel set components associated with undesired extrema are first locally flattened and then correctly embedded into the global scalar field, such that these regions are guaranteed---from a combinatorial perspective---to no longer contain any undesired extrema. In contrast to previous global approaches, LTS only and independently processes regions of the domain that actually need to be simplified, which already results in a noticeable speedup. Moreover, due to the localized nature of the algorithm, LTS can utilize shared-memory parallelism to simplify regions simultaneously with a high parallel efficiency (70%). Hence, LTS significantly improves interactivity for the exploration of simplification parameters and their effect on subsequent topological analysis. For such exploration tasks, LTS brings the overall execution time of a plethora of TDA pipelines from minutes down to seconds, with an average observed speedup over state-of-the-art techniques of up to x36. Furthermore, in the special case where preserved extrema are selected based on topological persistence, an adapted version of LTS partially computes the persistence diagram and simultaneously simplifies features below a predefined persistence threshold. The effectiveness of LTS, its parallel efficiency, and its resulting benefits for TDA are demonstrated on several simulated and acquired datasets from different application domains, including physics, chemistry, and biomedical imaging.",
                        "time_start": "2020-10-29T18:15:00Z",
                        "time_end": "2020-10-29T18:30:00Z",
                        "uid": "f-scivis-1057"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Mode Surfaces of Symmetric Tensor Fields: Topological Analysis and Seamless Extraction",
                        "contributors": [
                            "Botong Qu"
                        ],
                        "abstract": "Mode surfaces are the generalization of degenerate curves and neutral surface, which constitute 3D symmetric tensor fields. Efficient analysis and visualization of mode surfaces can provide additional insights into not only tensor field topology, but also how these features transition into each other. Existing methods of extracting mode surfaces are time-consuming and can miss features in the surfaces. Moreover, the mode surfaces extracted from neighboring cells have gaps, which make their subsequent analysis difficult. In this paper, we provide novel analysis on the topological structures of mode surfaces, including a common parameterization of all mode surfaces of a tensor field using 2D asymmetric tensors. This allows us to not only better understand the structures in mode surfaces and their interactions with degenerate curves and neutral surfaces, but also develop an efficient algorithm to seamlessly extract mode surfaces, including neutral surfaces. The seamless mode surfaces enable efficient analysis of their geometric structures, such as the principal curvature directions. We apply our analysis and visualization to a number of solid mechanics data sets.",
                        "time_start": "2020-10-29T18:30:00Z",
                        "time_end": "2020-10-29T18:45:00Z",
                        "uid": "f-scivis-1066"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Suggestive Interface for Untangling Mathematical Knots",
                        "contributors": [
                            "Huan Liu"
                        ],
                        "abstract": "In this paper we present a user-friendly sketching-based suggestive interface for untangling mathematical knots with complicated structures. Rather than treating mathematical knots as if they were 3D ropes, our interface is designed to assist the user to interact with knots with the right sequence of mathematically legal moves. Our knot interface allows one to sketch and untangle knots by proposing the Reidemeister moves, and can guide the user to untangle mathematical knots to the fewest possible number of crossings by suggesting the moves needed. The system highlights parts of the knot where the Reidemeister moves are applicable, suggests the possible moves, and constrains the user's drawing to legal moves only. This ongoing suggestion is based on a Reidemeister move analyzer, that reads the evolving knot in its Gauss code and predicts the needed Reidemeister moves towards the fewest possible number of crossings. For our principal test case of mathematical knot diagrams, this for the first time permits us to visualize, analyze, and deform them in a mathematical visual interface. In addition, understanding of a fairly long mathematical deformation sequence in our interface can be aided by visual analysis and comparison over the identified \u201ckey moments\u201d where only critical changes occur in the sequence. Our knot interface allows users to track and trace mathematical knot deformation with a significantly reduced number of visual frames containing only the Reidemeister moves being applied. All these combine to allow a much cleaner exploratory interface for us to analyze and study mathematical knots and their dynamics in topological space.",
                        "time_start": "2020-10-29T18:45:00Z",
                        "time_end": "2020-10-29T19:00:00Z",
                        "uid": "f-scivis-1209"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Geometry-Driven Detection, Tracking and Visual Analysis of Viscous and Gravitational Fingers",
                        "contributors": [
                            "Jiayi Xu"
                        ],
                        "abstract": "Viscous and gravitational flow instabilities cause a displacement front to break up into finger-like fluids. The detection and evolutionary analysis of these fingering instabilities are critical in multiple scientific disciplines such as fluid mechanics and hydrogeology. However, previous detection methods of the viscous and gravitational fingers are based on density thresholding, which provides limited geometric information of the fingers. The geometric structures of fingers and their evolution are important yet little studied in the literature. In this work, we explore the geometric detection and evolution of the fingers in detail to elucidate the dynamics of the instability. We propose a ridge voxel detection method to guide the extraction of finger cores from three-dimensional (3D) scalar fields. After skeletonizing finger cores into skeletons, we design a spanning tree based approach to capture how fingers branch spatially from the finger skeletons. Finally, we devise a novel geometric-glyph augmented tracking graph to study how the fingers and their branches grow, merge, and split over time. Feedback from earth scientists demonstrates the usefulness of our approach to performing spatio-temporal geometric analyses of fingers.",
                        "time_start": "2020-10-29T19:00:00Z",
                        "time_end": "2020-10-29T19:15:00Z",
                        "uid": "f-tvcg-2020040124"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Efficient and Flexible Hierarchical Data Layouts for a Unified Encoding of Scalar Field Precision and Resolution",
                        "contributors": [
                            "Duong Hoang"
                        ],
                        "abstract": "To address the problem of ever-growing scientific data sizes making data movement a major hindrance to analysis, we introduce a novel encoding for scalar fields: a unified tree of resolution and precision, specifically constructed so that valid cutes correspond to sensible approximations of the original field in the precision-resolution space. Furthermore, we introduce a highly flexible encoding of such trees that forms a parameterized family of data hierarchies. We discuss how different parameter choices lead to different trade-offs in practice, and show how specific choices result in known data representation schemes such as zfp, IDX, and JPEG2000. Finally, we provide system-level details and empirical evidence on how such hierarchies facilitate common approximate queries with minimal data movement and time, using real-world data sets ranging from a few gigabytes to nearly a terabyte in size. Experiments suggest that our new strategy of combining reductions in resolution and precision is competitive with state-of-the-art compression techniques with respect to data quality, while being significantly more flexible, orders of magnitude faster, and requires significant fewer resources.",
                        "time_start": "2020-10-29T19:15:00Z",
                        "time_end": "2020-10-29T19:30:00Z",
                        "uid": "f-scivis-1141"
                    }
                ]
            },
            {
                "title": "Graphs",
                "session_id": "f-papers-graphs",
                "chair": [
                    "Joseph Cottam"
                ],
                "organizers": [],
                "display_start": "2020-10-29T18:00:00Z",
                "time_start": "2020-10-29T18:00:00Z",
                "time_end": "2020-10-29T19:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "graphs",
                "discord_channel_id": "768679099169374228",
                "youtube_url": "https://youtu.be/qpsBOtN2kcQ",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrd_kNqo4l6r61jfKq2wRENZ",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Responsive Matrix Cells: A Focus+Context Approach for Exploring and Editing Multivariate Graphs",
                        "contributors": [
                            "Tom Horak",
                            "Philip Berger"
                        ],
                        "abstract": "Matrix visualizations are a useful tool to provide a general overview of a graph's structure. For multivariate graphs, a remaining challenge is to cope with the attributes that are associated with nodes and edges. Addressing this challenge, we propose responsive matrix cells as a focus+context approach for embedding additional interactive views into a matrix. Responsive matrix cells are local zoomable regions of interest that provide auxiliary data exploration and editing facilities for multivariate graphs. They behave responsively by adapting their visual contents to the cell location, the available display space, and the user task. Responsive matrix cells enable users to reveal details about the graph, compare node and edge attributes, and edit data values directly in a matrix without resorting to external views or tools. We report the general design considerations for responsive matrix cells covering the visual and interactive means necessary to support a seamless data exploration and editing. Responsive matrix cells have been implemented in a web-based prototype. To demonstrate the utility of our approach, we report on insights from a preliminary user feedback session and describe a walk-through for the use case of analyzing a graph of soccer players.",
                        "time_start": "2020-10-29T18:00:00Z",
                        "time_end": "2020-10-29T18:15:00Z",
                        "uid": "f-info-1044"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Exemplar-based Layout Fine-tuning for Node-link Diagrams",
                        "contributors": [
                            "Jiacheng Pan"
                        ],
                        "abstract": "We design and evaluate a novel layout fine-tuning technique for node-link diagrams that facilitates exemplar-based adjustment of a group of substructures in batching mode. The key idea is to transfer user modifications on a local substructure to other substructures in the entire graph that are topologically similar to the exemplar. We first precompute a canonical representation for each substructure with node embedding techniques and then use it for on-the-fly substructure retrieval. We design and develop a light-weight interactive system to enable intuitive adjustment, modification transfer, and visual graph exploration. We also report some results of quantitative comparisons, three case studies, and a within-participant user study.",
                        "time_start": "2020-10-29T18:15:00Z",
                        "time_end": "2020-10-29T18:30:00Z",
                        "uid": "f-info-1069"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by Dimensionality Reduction",
                        "contributors": [
                            "Minfeng Zhu"
                        ],
                        "abstract": "Efficient layout of large-scale graphs remains a challenging problem: the force-directed and dimensionality reduction-based methods suffer from high overhead for graph distance and gradient computation. In this paper, we present a new graph layout algorithm, called DRGraph, that enhances the nonlinear dimensionality reduction process with three schemes: approximating graph distances by means of a sparse distance matrix, estimating the gradient by using the negative sampling technique, and accelerating the optimization process through a multi-level layout scheme. DRGraph achieves a linear complexity for the computation and memory consumption, and scales up to large-scale graphs with millions of nodes. Experimental results and comparisons with state-of-the-art graph layout methods demonstrate that DRGraph can generate visually comparable layouts with a faster running time and a lower memory requirement.",
                        "time_start": "2020-10-29T18:30:00Z",
                        "time_end": "2020-10-29T18:45:00Z",
                        "uid": "f-info-1075"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Scalability of Network Visualisation from a Cognitive Load Perspective",
                        "contributors": [
                            "Vahan Yoghourdjian"
                        ],
                        "abstract": "Node-link diagrams are widely used to visualise networks. However, even the best network layout algorithms ultimately result in 'hairball' visualisations when the graph reaches a certain degree of complexity, requiring simplification through aggregation or interaction (such as filtering) to remain usable. Until now, there has been little data to indicate at what level of complexity node-link diagrams become ineffective or how visual complexity affects cognitive load. To this end, we conducted a controlled study to understand workload limits for a task that requires a detailed understanding of the network topology\u2014finding the shortest path between two nodes. We tested performance on graphs with 25 to 175 nodes with varying density. We collected performance measures (accuracy andresponse time), subjective feedback, and physiological measures (EEG, pupil dilation, and heart rate variability). To the best of our knowledge this is the first network visualisation study to include physiological measures. Our results show that people have significant difficulty finding the shortest path in high density node-link diagrams with more than 50 nodes and even low density graphs with more than 100 nodes. From our collected EEG data we observe functional differences in brain activity between hard and easy tasks. We found that cognitive load increased up to certain level of difficulty after which it decreased, likely because participants had given up. We also explored the effects of global network layout features such as size or number of crossings, and features of the shortest path such as length or straightness on task difficulty. We found that global features generally had a greater impact than those of the shortest path.",
                        "time_start": "2020-10-29T18:45:00Z",
                        "time_end": "2020-10-29T19:00:00Z",
                        "uid": "f-info-1394"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Visual Analytics Framework for Contrastive Network Analysis",
                        "contributors": [
                            "Takanori Fujiwara"
                        ],
                        "abstract": "A common network analysis task is comparison of two networks to identify unique characteristics in one network with respect to the other. For example, when comparing protein interaction networks derived from normal and cancer tissues, one essential task is to discover protein-protein interactions unique to cancer tissues. However, this task is challenging when the networks contain complex structural (and semantic) relations. To address this problem, we design ContraNA, a visual analytics framework leveraging both the power of machine learning for uncovering unique characteristics in networks and also the effectiveness of visualization for understanding such uniqueness. The basis of ContraNA is cNRL, which integrates two machine learning schemes, network representation learning (NRL) and contrastive learning (CL), to generate a low-dimensional embedding that reveals the uniqueness of one network when compared to another. ContraNA provides an interactive visualization interface to help analyze the uniqueness by relating embedding results and network structures as well as explaining the learned features by cNRL. We demonstrate the usefulness of ContraNA with two case studies using real-world datasets. We also evaluate through a controlled user study with 12 participants on network comparison tasks. The results show that participants were able to both effectively identify unique characteristics from complex networks and interpret the results obtained from cNRL.",
                        "time_start": "2020-10-29T19:00:00Z",
                        "time_end": "2020-10-29T19:15:00Z",
                        "uid": "f-vast-1135"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles",
                        "contributors": [
                            "Arpit Narechania"
                        ],
                        "abstract": "Modern automobiles have evolved from just being mechanical machines to having full-fledged electronics systems that enhance vehicle dynamics and driver experience. However, these complex hardware and software systems, if not properly designed, can experience failures that can compromise the safety of the vehicle, its occupants, and the surrounding environment. For example, a system to activate the brakes to avoid a collision saves lives when it functions properly, but could lead to tragic outcomes if the brakes were applied in a way that's inconsistent with the design. Broadly speaking, the analysis performed to minimize such risks falls into a systems engineering domain called Functional Safety. In this paper, we present SafetyLens, a visual data analysis tool to assist engineers and analysts in analyzing automotive Functional Safety datasets. SafetyLens combines techniques including network exploration and visual comparison to help analysts perform domain-specific tasks. This paper presents the design study with domain experts that resulted in the design guidelines, the tool, and user feedback.",
                        "time_start": "2020-10-29T19:15:00Z",
                        "time_end": "2020-10-29T19:30:00Z",
                        "uid": "f-info-1207"
                    }
                ]
            },
            {
                "title": "Automate & Recommend",
                "session_id": "f-papers-automate-rec",
                "chair": [
                    "Kanit Wongsuphasawat"
                ],
                "organizers": [],
                "display_start": "2020-10-29T18:00:00Z",
                "time_start": "2020-10-29T18:00:00Z",
                "time_end": "2020-10-29T19:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "automate-recommend",
                "discord_channel_id": "768679106543878194",
                "youtube_url": "https://youtu.be/eDdWp1Lo9jY",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrc0pcWRJPYBbIujRVr4y4TR",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Retrieve-Then-Adapt: Example-based Automatic Generation for Proportion-related Infographics",
                        "contributors": [
                            "Chunyao Qian",
                            "Shizhao Sun"
                        ],
                        "abstract": "Infographic is a data visualization technique which combines graphic and textual descriptions in an aesthetic and effective manner. Creating infographics is a difficult and time-consuming process which often requires significant attempts and adjustments even for experienced designers, not to mention novice users with limited design expertise. Recently, a few approaches have been proposed to automate the creation process by applying predefined blueprints to user information. However, predefined blueprints are often hard to create, hence limited in volume and diversity. In contrast, good infogrpahics have been created by professionals and accumulated on the Internet rapidly. These online examples often represent a wide variety of design styles, and serve as exemplars or inspiration to people who like to create their own infographics. Based on these observations, we propose to generate infographics by automatically imitating examples. We present a two-stage approach, namely retrieve-then-adapt. In the retrieval stage, we index online examples by their visual elements. For a given user information, we transform it to a concrete query by sampling from a learned distribution about visual elements, and then find appropriate examples in our example library based on the similarity between example indexes and the query. For a retrieved example, we generate an initial drafts by replacing its content with user information. However, in many cases, user information cannot be perfectly fitted to retrieved examples. Therefore, we further introduce an adaption stage. Specifically, we propose a MCMC-like approach and leverage recursive neural networks to help adjust the initial draft and improve its visual appearance iteratively, until a satisfactory result is obtained. We implement our approach on widely-used proportion-related infographics, and demonstrate its effectiveness by sample results and expert reviews.",
                        "time_start": "2020-10-29T18:00:00Z",
                        "time_end": "2020-10-29T18:15:00Z",
                        "uid": "f-info-1160"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Calliope: A System for Automatic Visual Data Story Generation from a Spreadsheet",
                        "contributors": [
                            "Danqing Shi"
                        ],
                        "abstract": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.",
                        "time_start": "2020-10-29T18:15:00Z",
                        "time_end": "2020-10-29T18:30:00Z",
                        "uid": "f-info-1150"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "MobileVisFixer: Tailoring Web Visualizations for Mobile Phones Leveraging an Explainable Reinforcement Learning Framework",
                        "contributors": [
                            "Aoyu Wu"
                        ],
                        "abstract": "We contribute MobileVisFixer, a new method to make visualizations more mobile-friendly. Although mobile devices have become the primary means of accessing information on the web, many existing visualizations are not optimized for small screens and can lead to a frustrating user experience. Currently, practitioners and researchers have to engage in a tedious and time-consuming process to ensure that their designs scale to screens of different sizes, and existing toolkits and libraries provide little support in diagnosing and repairing issues. To address this challenge, MobileVisFixer automates a mobile-friendly visualization re-design process with a novel reinforcement learning framework. To inform the design of MobileVisFixer, we first collected and analyzed SVG-based visualizations on the web, and identified five common mobile-friendly issues.  MobileVisFixer addresses four of these issues on single-view Cartesian visualizations with linear or discrete scales by a Markov Decision Process model that is both generalizable across various visualizations and fully explainable.  MobileVisFixer deconstructs charts into declarative formats, and uses a greedy heuristic based on Policy Gradient methods to find solutions to this difficult, multi-criteria optimization problem in reasonable time. In addition, MobileVisFixer can be easily extended with the incorporation of optimization algorithms for data visualizations.  Quantitative evaluation on two real-world datasets demonstrates the effectiveness and generalizability of our method.",
                        "time_start": "2020-10-29T18:30:00Z",
                        "time_end": "2020-10-29T18:45:00Z",
                        "uid": "f-info-1067"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Palettailor:\u00a0Discriminable\u00a0Colorization\u00a0for\u00a0Categorical\u00a0Data",
                        "contributors": [
                            "Kecheng Lu"
                        ],
                        "abstract": "We present an integrated approach for creating and assigning color palettes to different visualizations such as multi-class scatterplots, line, and bar charts. Other approaches separate the creation of colors from their assignment. In contrast, our approach takes data characteristics already into account to produce color palettes, which are then assigned in a way that fosters better visual discrimination of classes. To do so, we use a customized optimization based on simulated annealing to maximize the combination of three carefully designed color scoring functions: point distinctness, name difference, and color discrimination. We compared our approach to state-of-the-art palettes with a controlled user study for scatterplots and line charts and performed a case study. Our results show that Palettailor, as a fully-automated approach, generates color palettes with a higher discrimination quality than existing approaches. The efficiency of our optimization allows us also to incorporate user modifications into the color selection process.",
                        "time_start": "2020-10-29T18:45:00Z",
                        "time_end": "2020-10-29T19:00:00Z",
                        "uid": "f-info-1326"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Gemini: A Grammar and Recommender System for Animated Transitions in Statistical Graphics",
                        "contributors": [
                            "Younghoon Kim"
                        ],
                        "abstract": "Animated transitions help viewers follow changes between related visualizations. Specifying effective animations demands significant effort: authors must select the elements and properties to animate, provide transition parameters, and coordinate the timing of stages. To facilitate this process, we present Gemini, a declarative grammar and recommendation system for animated transitions between single-view statistical graphics. Gemini specifications define transition \u201csteps\u201d in terms of high-level visual components (marks, axes, legends) and composition rules to synchronize and concatenate steps. With this grammar, Gemini can recommend animation designs to augment and accelerate designers' work. Gemini enumerates staged animation designs for given start and end states, and ranks those designs using a cost function informed by prior perceptual studies. To evaluate Gemini, we conduct both a formative study on Mechanical Turk to assess and tune our ranking function, and a summative study in which 8 experienced visualization developers implement animations in D3 that we then compare to Gemini's suggestions. We find that most designs (9/11) are exactly replicable in Gemini, with many (8/11) achievable via edits to suggestions, and that Gemini suggestions avoid multiple participant errors.",
                        "time_start": "2020-10-29T19:00:00Z",
                        "time_end": "2020-10-29T19:15:00Z",
                        "uid": "f-info-1111"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "VizCommender: Computing Text-Based Similarity in Visualization Repositories for Content-Based Recommendations",
                        "contributors": [
                            "Michael Oppermann"
                        ],
                        "abstract": "Cloud-based visualization services have made visual analytics accessible to a much wider audience than ever before. Systems such as Tableau have started to amass increasingly large repositories of analytical knowledge in the form of interactive visualization workbooks. When shared, these collections can form a visual analytic knowledge base. However, as the size of a collection increases, so does the difficulty in finding relevant information. Content-based recommendation (CBR) systems could help analysts in finding and managing workbooks relevant to their interests. Toward this goal, we focus on text-based content that is representative of the subject matter of visualizations rather than the visual encodings and style. We discuss the challenges associated with creating a CBR based on visualization specifications and explore more concretely how to implement the relevance measures required using Tableau workbook specifications as the source of content data. We also demonstrate what information can be extracted from these visualization specifications and how various natural language processing techniques can be used to compute similarity between workbooks as one way to measure relevance. We report on a crowd-sourced user study to determine if our similarity measure mimics human judgement. Finally, we choose latent Dirichlet allocation (LDA) as a specific model and instantiate it in a proof-of-concept recommender tool to demonstrate the basic function of our similarity measure.",
                        "time_start": "2020-10-29T19:15:00Z",
                        "time_end": "2020-10-29T19:30:00Z",
                        "uid": "f-vast-1247"
                    }
                ]
            },
            {
                "title": "Deep Learning for Spatial Data",
                "session_id": "f-papers-deep-learn-spatial",
                "chair": [
                    "Hanqi Guo"
                ],
                "organizers": [],
                "display_start": "2020-10-29T18:00:00Z",
                "time_start": "2020-10-29T18:00:00Z",
                "time_end": "2020-10-29T19:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "deep-learning-for-spatial-data",
                "discord_channel_id": "768679113577725983",
                "youtube_url": "https://youtu.be/suPUjN59rkc",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrdI5LIf3D6hMDjDzLjJU5Ht",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Deep Volumetric Ambient Occlusion",
                        "contributors": [
                            "Dominik Engel"
                        ],
                        "abstract": "We present a novel deep learning based technique for volumetric ambient occlusion in the context of direct volume rendering.\nOur proposed Deep Volumetric Ambient Occlusion (DVAO) approach can predict per-voxel ambient occlusion in volumetric data sets, while\nconsidering global information provided through the transfer function. The proposed neural network only needs to be executed upon\nchange of this global information, and thus supports real-time volume interaction. Accordingly, we demonstrate DVAO's ability to predict\nvolumetric ambient occlusion, such that it can be applied interactively within direct volume rendering. To achieve the best possible\nresults, we propose and analyze a variety of transfer function representations and injection strategies for deep neural networks. Based\non the obtained results we also give recommendations applicable in similar volume learning scenarios. Lastly, we show that DVAO\ngeneralizes to a variety of modalities, despite being trained on computed tomography data only.",
                        "time_start": "2020-10-29T18:00:00Z",
                        "time_end": "2020-10-29T18:15:00Z",
                        "uid": "f-scivis-1033"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Fluid Flow Data Set for Machine Learning and its Application to Neural Flow Map Interpolation",
                        "contributors": [
                            "Jakob Jakob"
                        ],
                        "abstract": "In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart - the application of deep learning to visualization problems - requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.",
                        "time_start": "2020-10-29T18:15:00Z",
                        "time_end": "2020-10-29T18:30:00Z",
                        "uid": "f-scivis-1180"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data",
                        "contributors": [
                            "Jun Han"
                        ],
                        "abstract": "We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training. Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).",
                        "time_start": "2020-10-29T18:30:00Z",
                        "time_end": "2020-10-29T18:45:00Z",
                        "uid": "f-scivis-1043"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data",
                        "contributors": [
                            "Yifan Wang"
                        ],
                        "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructure in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP -- a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small / micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro-)cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.",
                        "time_start": "2020-10-29T18:45:00Z",
                        "time_end": "2020-10-29T19:00:00Z",
                        "uid": "f-scivis-1095"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images",
                        "contributors": [
                            "Suphanut Jamonnak",
                            "Xinyi Huang"
                        ],
                        "abstract": "Existing interactive visualization tools for deep learning are mostly applied to the training, debugging, and refinement of neural network models working on natural images. While deep learning methods also gain its population in scientific domains, visual analysis of classification behavior of multiple structural attributes contained in many scientific images, however, has not been well supported by existing visualization tools. In this paper, we present an interactive system for domain scientists to visually study the multiple attributes learning models applied to x-ray scattering images. It allows domain scientists to interactively explore this important type of scientific images in embedded spaces that are defined on the model prediction output, the actual labels, and the discovered feature space of neural networks. Users are allowed to flexibly select instance images, their clusters, and compare them with the specified visual representation of attributes. The exploration is guided by the manifestation of model performance related to mutual relationships among attributes, which often affect the learning accuracy and effectiveness. The system thus supports domain scientists to improve the training dataset and model, find questionable attributes labels, and identify outlier images or spurious data clusters. Case studies and scientists feedback demonstrate its functionalities and usefulness.",
                        "time_start": "2020-10-29T19:00:00Z",
                        "time_end": "2020-10-29T19:15:00Z",
                        "uid": "f-scivis-1063"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Volumetric Isosurface Rendering with Deep Learning-Based Super-Resolution",
                        "contributors": [
                            "Sebastian Weiss"
                        ],
                        "abstract": "Rendering an accurate image of an isosurface in a volumetric field typically requires large numbers of data samples. Reducing this number lies at the core of research in volume rendering. With the advent of deep learning networks, a number of architectures have been proposed recently to infer missing samples in multi-dimensional fields, for applications such as image super-resolution. In this paper, we investigate the use of such architectures for learning the upscaling of a low-resolution sampling of an isosurface to a higher resolution, with reconstruction of spatial detail and shading. We introduce a fully convolutional neural network, to learn a latent representation generating smooth, edge-aware depth and normal fields as well as ambient occlusions from a low-resolution depth and normal field. By adding a frame-to-frame motion loss into the learning stage, upscaling can consider temporal variations and achieves improved frame-to-frame coherence. We assess the quality of inferred results and compare it to bi-linear and -cubic upscaling. We do this for isosurfaces which were never seen during training, and investigate the improvements when the network can train on the same or similar isosurfaces. We discuss remote visualization and foveated rendering as potential applications.",
                        "time_start": "2020-10-29T19:15:00Z",
                        "time_end": "2020-10-29T19:30:00Z",
                        "uid": "f-tvcg-2019080255"
                    }
                ]
            },
            {
                "title": "Decision Making & Reasoning",
                "session_id": "f-papers-decision-reason",
                "chair": [
                    "Jordan Crouser"
                ],
                "organizers": [],
                "display_start": "2020-10-30T14:00:00Z",
                "time_start": "2020-10-30T14:00:00Z",
                "time_end": "2020-10-30T15:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "decision-making-reasoning",
                "discord_channel_id": "767141461757788192",
                "youtube_url": "https://youtu.be/YWhjyAu0hZs",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrd4CGDueJvT-6Zmo7k8eRIX",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs",
                        "contributors": [
                            "Dylan Cashman"
                        ],
                        "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.",
                        "time_start": "2020-10-30T14:00:00Z",
                        "time_end": "2020-10-30T14:15:00Z",
                        "uid": "f-vast-1122"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "An Examination of Grouping and Spatial Organization Tasks for High-Dimensional Data Exploration",
                        "contributors": [
                            "John Wenskovitch"
                        ],
                        "abstract": "How do analysts think about grouping and spatial operations?  This overarching question incorporates a number of points for investigation, including understanding how analysts begin to explore a dataset, the types of grouping/spatial structures created and the operations performed on them, the relationship between grouping and spatial structures, the decisions analysts make when exploring individual observations, and the role of external information.  This work contributes the design and results of such a study, in which a group of participants are asked to organize the data contained within an unfamiliar quantitative dataset.  We identify several overarching approaches taken by participants to design their organizational space, discuss the interactions performed by the participants, and propose design recommendations to improve the usability of future high-dimensional data exploration tools that make use of grouping (clustering) and spatial (dimension reduction) operations.",
                        "time_start": "2020-10-30T14:15:00Z",
                        "time_end": "2020-10-30T14:30:00Z",
                        "uid": "f-vast-1124"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Boba: Authoring and Visualizing Multiverse Analyses",
                        "contributors": [
                            "Yang Liu"
                        ],
                        "abstract": "Multiverse analysis is an approach to data analysis in which all ``reasonable'' analytic decisions are evaluated in parallel and interpreted collectively, in order to foster robustness and transparency.\nHowever, specifying a multiverse is demanding because analysts must manage myriad variants from a cross-product of analytic decisions, and the results require nuanced interpretation.\nWe contribute Boba: an integrated domain-specific language (DSL) and visual analysis system for authoring and reviewing multiverse analyses.\nWith the Boba DSL, analysts write the shared portion of analysis code only once, alongside local variations defining alternative decisions, from which the compiler generates a multiplex of scripts representing all possible analysis paths.\nThe Boba Visualizer provides linked views of model results and the multiverse decision space to enable rapid, systematic assessment of consequential decisions and robustness, including sampling uncertainty and model fit.\nWe demonstrate Boba's utility through two data analysis case studies, and reflect on challenges and design opportunities for multiverse analysis software.",
                        "time_start": "2020-10-30T14:30:00Z",
                        "time_end": "2020-10-30T14:45:00Z",
                        "uid": "f-vast-1131"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Supporting the Problem-Solving Loop: Designing Highly Interactive Optimisation Systems",
                        "contributors": [
                            "Jie Liu"
                        ],
                        "abstract": "Efficient optimisation algorithms have become important tools for finding high-quality solutions to hard, real-world problems such as production scheduling, timetabling, or vehicle routing. These algorithms are typically \u201cblack boxes\u201d that work on mathematical models of the problem to solve. However, many problems are difficult to fully specify, and require a \u201chuman in the loop\u201d who collaborates with the algorithm by refining the model and guiding the search to produce acceptable solutions. Recently, the Problem-Solving Loop was introduced as a high-level model of such interactive optimisation. Here, we present and evaluate nine recommendations for the design of interactive visualisation tools supporting the Problem-Solving Loop. They range from the choice of visual representation for solutions and constraints to the use of a solution gallery to support exploration of alternate solutions. We first examined the applicability of the recommendations by investigating how well they had been supported in previous interactive optimisation tools. We then evaluated the recommendations in the context of the vehicle routing problem with time windows (VRPTW). To do so we built a sophisticated interactive visual system for solving VRPTW that was informed by the recommendations. Ten participants then used this system to solve a variety of routing problems. We report on participant comments and interaction patterns with the tool. These showed the tool was regarded as highly usable and the results generally supported the usefulness of the underlying recommendations.",
                        "time_start": "2020-10-30T14:45:00Z",
                        "time_end": "2020-10-30T15:00:00Z",
                        "uid": "f-vast-1149"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Integrating Prior Knowledge in Mixed Initiative Social Network Clustering",
                        "contributors": [
                            "Alexis Pister"
                        ],
                        "abstract": "We propose a new approach\u2014called PK-clustering\u2014to help social scientists create meaningful clusters in social networks.Many clustering algorithms exist but most social scientists find them difficult to understand, and tools do not provide any guidance tochoose algorithms, or to evaluate results taking into account theprior knowledgeof the scientists. Our work introduces a new clusteringapproach and a visual analytics user interface that address this issue. It is based on a process that 1) captures the prior knowledgeof the scientists as a set of incomplete clusters, 2) runs multiple clustering algorithms (similarly toclustering ensemblemethods),3) visualizes the results of all the algorithms ranked and summarized by how well each algorithm matches the prior knowledge, 4)evaluates the consensus between user-selected algorithms and 5) allows users to review details and iteratively update the acquiredknowledge. We describe our approach using an initial functional prototype, then provide two examples of use and early feedback fromsocial scientists. We believe our clustering approach offers a novel constructive method to iteratively build knowledge while avoidingbeing overly influenced by the results of often randomly selected black-box clustering algorithms.",
                        "time_start": "2020-10-30T15:00:00Z",
                        "time_end": "2020-10-30T15:15:00Z",
                        "uid": "f-vast-1094"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Investigating Visual Analysis of Differentially Private Data",
                        "contributors": [
                            "Dan Zhang"
                        ],
                        "abstract": "Differential Privacy is an emerging privacy model with increasing popularity in many domains. It functions by adding carefullycalibrated noise to data that blurs information about individuals while preserving overall statistics about the population. Theoretically,it is possible to produce robust privacy-preserving visualizations by plotting differentially private data.   However,  noise-induceddata perturbations can alter visual patterns and impact the utility of private visualization.  We still know little about challenges andopportunities for visual data exploration and analysis using private visualizations. As a first step towards filling this gap, we conducted acrowdsourced experiment, measuring participants' performance under three levels of privacy (high, low, non-private) for combinationsof eight analysis tasks and four visualization types (bar chart, pie chart, line chart, scatter plot). Our findings show that for participants'accuracy for summary tasks (e.g., find clusters in data) was higher that value tasks (e.g., retrieve a certain value). We also found thatunder DP, pie chart and line chart offer similar or better accuracy than bar chart. In this work, we contribute the results of our empiricalstudy, investigating the task-based effectiveness of basic private visualizations, a dichotomous model for defining and measuring usersuccess in performing visual analysis tasks under DP, and a set of distribution metrics for tuning the injection to improve the utility ofprivate visualizations",
                        "time_start": "2020-10-30T15:15:00Z",
                        "time_end": "2020-10-30T15:30:00Z",
                        "uid": "f-info-1306"
                    }
                ]
            },
            {
                "title": "Cognition & Emotion",
                "session_id": "f-papers-cog-emotion",
                "chair": [
                    "Evanthia Dimara"
                ],
                "organizers": [],
                "display_start": "2020-10-30T14:00:00Z",
                "time_start": "2020-10-30T14:00:00Z",
                "time_end": "2020-10-30T15:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "cognition-emotion",
                "discord_channel_id": "767141470082826321",
                "youtube_url": "https://youtu.be/KHoBVvConGk",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrcwpVBk1am6RcV8a62raPoI",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "A Bayesian cognition approach for belief updating of correlation judgment through uncertainty visualizations",
                        "contributors": [
                            "Alireza Karduni"
                        ],
                        "abstract": "Understanding correlation judgement is important to designing effective visualizations of bivariate data. Prior work on correlation perception has not considered how factors including prior beliefs and uncertainty representation impact such judgements. The present work focuses on the impact of uncertainty communication when judging bivariate visualizations. Specifically, we model how users update their beliefs about variable relationships after seeing a scatterplot with and without uncertainty representation. To model and evaluate the belief updating, we present three studies. Study 1 focuses on a proposed ''Line + Cone'' visual elicitation method for capturing users' beliefs in an accurate and intuitive fashion. The findings reveal that our proposed method of belief solicitation reduces complexity and accurately captures the users' uncertainty about a range of bivariate relationships. Study 2 leverages the ``Line + Cone'' elicitation method to measure belief updating on the relationship between different sets of variables when seeing correlation visualization with and without uncertainty representation. We compare changes in users beliefs to the predictions of Bayesian cognitive models which provide normative benchmarks for how users should update their prior beliefs about a relationship in light of observed data. The findings from Study 2 revealed that one of the visualization conditions with uncertainty communication led to users being slightly more confident about their judgement compared to visualization without uncertainty information. Study 3 builds on findings from Study 2 and explores differences in belief update when the bivariate visualization is congruent or incongruent with users' prior belief. Our results highlight the effects of incorporating uncertainty representation, and the potential of measuring belief updating on correlation judgement with Bayesian cognitive models.",
                        "time_start": "2020-10-30T14:00:00Z",
                        "time_end": "2020-10-30T14:15:00Z",
                        "uid": "f-info-1297"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Bayesian-Assisted Inference from Visualized Data",
                        "contributors": [
                            "Yea-Seul Kim"
                        ],
                        "abstract": "A Bayesian view of data interpretation suggests that a visualization user should update their existing beliefs about a parameter's value in accordance with the amount of information about the parameter value captured by the new observations. Extending recent work applying Bayesian models to understand and evaluate belief updating from visualizations, we show how the predictions of Bayesian inference can be used to guide more rational belief updating. We design a Bayesian inference-assisted uncertainty analogy that numerically relates uncertainty in observed data to the user's subjective uncertainty, and a posterior visualization that prescribes how a user should update their beliefs given their prior beliefs and the observed data. In a pre-registered experiment on 4,800 people, we find that when a newly observed data sample is relatively small (N=158), both techniques reliably improve people's Bayesian updating on average compared to the current best practice of visualizing uncertainty in the observed data. For large data samples (N=5208), where people's updated beliefs tend to deviate more strongly from the prescriptions of a Bayesian model, we find evidence that the effectiveness of the two forms of Bayesian assistance may depend on people's proclivity toward trusting the source of the data. We discuss how our results provide insight into individual processes of belief updating and subjective uncertainty, and how understanding these aspects of interpretation paves the way for more sophisticated interactive visualizations for analysis and communication.",
                        "time_start": "2020-10-30T14:15:00Z",
                        "time_end": "2020-10-30T14:30:00Z",
                        "uid": "f-info-1280"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Towards Modeling Visualization Processes as Dynamic Bayesian Networks",
                        "contributors": [
                            "Christian Heine"
                        ],
                        "abstract": "Visualization designs typically need to be evaluated with user studies because their suitability for a particular task is hard to predict. What the field of visualization is currently lacking are theories and models that can be used to explain why certain designs work and others do not. This paper outlines a general framework for modeling visualization processes that can be the first step towards such a theory. It surveys related research in mathematical and computational psychology and argues for the use of dynamic Bayesian networks to describe these time-dependent, probabilistic processes. It is discussed how these models could be used to aid in design evaluation. The development of concrete models will be a long process. Thus, the paper outlines a research program sketching how to develop prototypes and their extensions from existing models and empirical and observational studies.",
                        "time_start": "2020-10-30T14:30:00Z",
                        "time_end": "2020-10-30T14:45:00Z",
                        "uid": "f-info-1215"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Characterizing the Quality of Insight by Interactions: A Case Study",
                        "contributors": [
                            "Chen He"
                        ],
                        "abstract": "Understanding the quality of insight has become increasingly important with the trend of allowing users to post comments during visual exploration, yet approaches for qualifying insight are rare. This paper presents a case study to investigate the possibility of characterizing the quality of insight via the interactions performed. To do this, we devised the interaction of a visualization tool-MediSyn-for insight generation. MediSyn supports five types of interactions: selecting, connecting, elaborating, exploring, and sharing. We evaluated MediSyn with 14 participants by allowing them to freely explore the data and generate insights. We then extracted seven interaction patterns from their interaction logs and correlated the patterns to four aspects of insight quality. The results show the possibility of qualifying insights via interactions. Among other findings, exploration actions can lead to unexpected insights; the drill-down pattern tends to increase the domain values of insights. A qualitative analysis shows that using domain knowledge to guide exploration can positively affect the domain value of derived insights. We discuss the study's implications, lessons learned, and future research opportunities.",
                        "time_start": "2020-10-30T14:45:00Z",
                        "time_end": "2020-10-30T15:00:00Z",
                        "uid": "f-tvcg-20202977634"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Insight Beyond Numbers: The Impact of Qualitative Factors on Visual Data Analysis",
                        "contributors": [
                            "Benjamin Karer"
                        ],
                        "abstract": "As of today, data analysis focuses primarily on the findings to be made inside the data and concentrates less on how those findings relate to the domain of investigation. Contemporary visualization as a field of research shows a strong tendency to adopt this data-centrism. Despite their decisive influence on the analysis result, qualitative aspects of the analysis process such as the structure, soundness, and complexity of the applied reasoning strategy are rarely discussed explicitly. We argue that if the purpose of visualization is the provision of domain insight rather than the depiction of data analysis results, a holistic perspective requires a qualitative component to to be added to the discussion of quantitative and human factors. To support this point, we demonstrate how considerations of qualitative factors in visual analysis can be applied to obtain explanations and possible solutions for a number of practical limitations inherent to the data-centric perspective on analysis. Based on this discussion of what we call qualitative visual analysis, we develop an inside-outside principle of nested levels of context that can serve as a conceptual basis for the development of visualization systems that optimally support the emergence of insight during analysis.",
                        "time_start": "2020-10-30T15:00:00Z",
                        "time_end": "2020-10-30T15:15:00Z",
                        "uid": "f-vast-1300"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
                        "contributors": [
                            "Haipeng Zeng"
                        ],
                        "abstract": "Analyzing students' emotions from classroom videos can help both teachers and parents quickly know the engagement of students in class. The availability of high-definition cameras creates opportunities to record class scenes. However, watching videos is time-consuming, and it is challenging to gain a quick overview of the emotion distribution and find abnormal emotions. In this paper, we propose EmotionCues, a visual analytics system to easily analyze classroom videos from the perspective of emotion summary and detailed analysis, which integrates emotion recognition algorithms with visualizations. It consists of three coordinated views: a summary view depicting the overall emotions and their dynamic evolution, a character view presenting the detailed emotion status of an individual, and a video view enhancing the video analysis with further details. Considering the possible inaccuracy of emotion recognition, we also explore several factors affecting the emotion analysis, such as face size and occlusion. They provide hints for inferring the possible inaccuracy and the corresponding reasons. Two use cases and interviews with end users and domain experts are conducted to show that the proposed system could be useful and effective for analyzing emotions in the classroom videos.",
                        "time_start": "2020-10-30T15:15:00Z",
                        "time_end": "2020-10-30T15:30:00Z",
                        "uid": "f-tvcg-2019090334"
                    }
                ]
            },
            {
                "title": "Sampling",
                "session_id": "f-papers-sampling",
                "chair": [
                    "Katerina Vrotsou"
                ],
                "organizers": [],
                "display_start": "2020-10-30T14:00:00Z",
                "time_start": "2020-10-30T14:00:00Z",
                "time_end": "2020-10-30T15:15:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "sampling",
                "discord_channel_id": "767141478027624549",
                "youtube_url": "https://youtu.be/N46isApUQsY",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrcX-wmz0_H1fBVje9pbPNLW",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Preserving Minority Structures in Graph Sampling",
                        "contributors": [
                            "Haojin Jiang",
                            "Yitao Wu"
                        ],
                        "abstract": "Sampling is a widely used graph reduction technique to accelerate graph computations and simplify graph visualizations. By comprehensively analyzing the literature on graph sampling, we assume that existing algorithms cannot effectively preserve minority structures that are rare and small in a graph but are very important in graph analysis. In this work, we initially conduct a pilot user study to investigate representative minority structures that are most appealing to human viewers. We then perform an experimental study to evaluate the performance of existing graph sampling algorithms regarding minority structure preservation. Results confirm our assumption and suggest key points for designing a new graph sampling approach named mino-centric graph sampling (MCGS). In this approach, a triangle-based algorithm and a cut-point-based algorithm are proposed to efficiently identify minority structures. A set of importance assessment criteria are designed to guide the preservation of important minority structures. Three optimization objectives are introduced into a greedy strategy to balance the preservation between minority and majority structures and suppress the generation of new minority structures. A series of experiments and case studies are conducted to evaluate the effectiveness of the proposed MCGS.",
                        "time_start": "2020-10-30T14:00:00Z",
                        "time_end": "2020-10-30T14:15:00Z",
                        "uid": "f-vast-1082"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Context-aware Sampling of Large Networks via Graph Representation Learning",
                        "contributors": [
                            "Chen Shi"
                        ],
                        "abstract": "Numerous sampling strategies have been proposed to simplify large-scale networks for highly readable visualizations. It is of great challenge to preserve contextual structures formed by nodes and edges with tight relationships in a sampled graph, because they are easily overlooked during the process of sampling due to their irregular distribution and immunity to scale. In this paper, a new graph sampling method is proposed oriented to the preservation of contextual structures. We first utilize a graph representation learning (GRL) model to transform nodes into vectors so that the contextual structures in a network can be effectively extracted and organized. Then, we propose a multi-objective blue noise sampling model to select a subset of nodes in the vectorized space to preserve contextual structures with the retention of relative data densities and relative cluster densities in addition to those significant topology features, such as bridging nodes and graph connections. We also design a visual interface that supports conduct context-aware sampling, compare results with various sampling strategies, and deeply explore large networks. Case studies and quantitative comparisons of sampling results based on real-world datasets have demonstrated the effectiveness of our method in the abstraction and exploration of large networks.",
                        "time_start": "2020-10-30T14:15:00Z",
                        "time_end": "2020-10-30T14:30:00Z",
                        "uid": "f-info-1259"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Evaluation of Sampling Methods for Scatterplots",
                        "contributors": [
                            "Juan Yuan"
                        ],
                        "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201dgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.",
                        "time_start": "2020-10-30T14:30:00Z",
                        "time_end": "2020-10-30T14:45:00Z",
                        "uid": "f-vast-1062"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visual Abstraction of Geographical Point Data with Spatial Autocorrelations",
                        "contributors": [
                            "Xinlong Zhang"
                        ],
                        "abstract": "Scatterplots are always employed to visualize geographical point datasets, which often suffer from an overdraw problem due to the increase of data sizes. A variety of sampling strategies have been proposed to reduce overdraw and visual clutter with the spatial densities of points taken into account. However, informative attributes associated with the points also play significant roles in the exploration of geographical datasets. In this paper, we propose an attribute-based abstraction method to simplify the cluttered visualization of large-scale geographical points. Spatial autocorrelations are utilized to measure the attribute relationships of points in local areas, and a novel attribute-based sampling model is designed to generate a subset of points to preserve both density and attribute characteristics of original geographical points. A set of visual designs and user-friendly interactions are implemented, enabling users to capture the spatial distribution of geographical points and get deeper insights into the attribute features across local areas. Case studies and quantitative comparisons based on the real-world datasets further demonstrate the effectiveness of our method in the abstraction and exploration of large-scale geographical point datasets.",
                        "time_start": "2020-10-30T14:45:00Z",
                        "time_end": "2020-10-30T15:00:00Z",
                        "uid": "f-vast-1200"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "STULL: Unbiased Online Sampling for Visual Exploration of Large Spatiotemporal Data",
                        "contributors": [
                            "Guizhen Wang"
                        ],
                        "abstract": "Online sampling-supported visual analytics is increasingly important,\nas it allows users to explore large datasets with acceptable approximate answers at interactive rates.\nHowever, existing online spatiotemporal sampling techniques are often biased, as most researchers have primarily focused on reducing computational latency. \nBiased sampling approaches select data with unequal probabilities and produce results that do not match the exact data distribution, leading end users to incorrect interpretations. \nIn this paper, we propose a novel approach to perform unbiased online sampling of large spatiotemporal data.\nThe proposed approach ensures the same probability of selection to every point that qualifies the specifications of a user's multidimensional query.\nTo achieve unbiased sampling for accurate representative interactive visualizations, we design a novel data index and an associated sample retrieval plan. \nOur proposed sampling approach is suitable for a wide variety of visual analytics tasks, e.g., tasks that run aggregate queries of spatiotemporal data. \nExtensive experiments confirm the superiority of our approach over a state-of-the-art spatial online sampling technique, demonstrating that within the same computational time,\ndata samples generated in our approach are at least 50\\% more accurate in representing the actual spatial distribution of the data and enable approximate visualizations to present closer visual appearances to the exact ones.",
                        "time_start": "2020-10-30T15:00:00Z",
                        "time_end": "2020-10-30T15:15:00Z",
                        "uid": "f-vast-1204"
                    }
                ]
            },
            {
                "title": "Health & Disease",
                "session_id": "f-papers-health-disease",
                "chair": [
                    "David Gotz"
                ],
                "organizers": [],
                "display_start": "2020-10-30T14:00:00Z",
                "time_start": "2020-10-30T14:00:00Z",
                "time_end": "2020-10-30T15:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "health-disease",
                "discord_channel_id": "767141486600519690",
                "youtube_url": "https://youtu.be/tB6-lkVz_R8",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojreqiUZelu5Cj5DCW2ISnepn",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Humane Visual AI: Telling the stories behind a medical condition",
                        "contributors": [
                            "Wonyoung So"
                        ],
                        "abstract": "A biological understanding is key for managing medical conditions, yet psychological and social aspects matter too. The main problem is that these aspects are hard to quantify and inherently difficult to communicate. To quantify psychological aspects, this work mined around half a million Reddit posts in the sub-communities specialised in 14 medical conditions, and it did so with a new deep-learning framework. In so doing, it was able to associate mentions of medical conditions with those of emotions and various expressions of symptoms. To then quantify social aspects, this work designed a probabilistic approach that mines open prescription data from the National Health Service in England to compute the prevalence of drug prescriptions, and to relate such a prevalence to census data (putting health in context). To finally visually communicate each medical condition's biological, psychological, and social aspects through storytelling, we designed a narrative-style layered Martini Glass visualization. In a user study involving 52 participants, after interacting with our visualization, a considerable number of them changed their mind on previously held opinions: 10% gave more importance to the psychological aspects of medical conditions, and 27% were more favourable of the use of social media data, suggesting the importance of persuasive elements in interactive visualizations.",
                        "time_start": "2020-10-30T14:00:00Z",
                        "time_end": "2020-10-30T14:15:00Z",
                        "uid": "f-info-1396"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "QualDash: Adaptable Generation of Visualisation Dashboards for Healthcare Quality Improvement",
                        "contributors": [
                            "Mai Elshehaly"
                        ],
                        "abstract": "Adapting dashboard design to different contexts of use is an open question in visualisation research. Dashboard designers often seek to strike a balance between dashboard adaptability and ease-of-use, and in hospitals challenges arise from the vast diversity of key metrics, data models and users involved at different organizational levels. In this design study, we present QualDash, a dashboard generation engine that allows for the dynamic configuration and deployment of visualisation dashboards for healthcare quality improvement (QI). We present a rigorous task analysis based on interviews with healthcare professionals, a co-design workshop and a series of one-on-one meetings with front line analysts. From these activities we define a metric card metaphor as a unit of visual analysis in healthcare QI, using this concept as a building block for generating highly adaptable dashboards, and leading to the design of a Metric Specification Structure (MSS). Each MSS is a JSON structure which enables dashboard authors to concisely configure unit-specific variants of a metric card, while offloading common patterns that are shared across cards to be preset by the engine. We reflect on deploying and iterating the design of QualDash in cardiology wards and pediatric intensive care units of five NHS hospitals. Finally, we report evaluation results that demonstrate the adaptability, ease-of-use and usefulness of QualDash in a real-world scenario.",
                        "time_start": "2020-10-30T14:15:00Z",
                        "time_end": "2020-10-30T14:30:00Z",
                        "uid": "f-info-1373"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visualization of Human Spine Biomechanics for Spinal Surgery",
                        "contributors": [
                            "Pepe Eulzer"
                        ],
                        "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, {\\color{blue} abstraction} and focus and context to display simulation outcomes in a dedicated tool. By linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. In a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.",
                        "time_start": "2020-10-30T14:30:00Z",
                        "time_end": "2020-10-30T14:45:00Z",
                        "uid": "f-scivis-1077"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
                        "contributors": [
                            "Tom Baumgartl",
                            "Tatiana von Landesberger "
                        ],
                        "abstract": "Pathogen outbreaks (i.e., outbreaks of bacteria and viruses) in hospitals can cause high mortality rates and increase costs for hospitals signi\ufb01cantly. An outbreak is generally noticed when the number of infected patients rises above an endemic level or the usual prevalence of a pathogen in a de\ufb01ned population. Reconstructing transmission pathways back to the source of an outbreak \u2013 the patient zero or index patient \u2013 requires the analysis of microbiological data and patient contacts. This is often manually completed by infection control experts. We present a novel visual analytics approach to support the analysis of transmission pathways, patient contacts, progression of the outbreak, and patient timelines during hospitalization. Infection control experts applied our solution to a real outbreak of Klebsiella pneumoniae in a large German hospital. Using our system, our experts were able to scale the analysis of transmission pathways to longer time intervals (i.e., several years of data instead of days) and across a larger number of wards. Also, the system is able to reduce the analysis time from days to hours. In our \ufb01nal study, feedback from twenty-\ufb01ve experts from seven German hospitals provides evidence that our solution brings signi\ufb01cant bene\ufb01ts for analyzing outbreaks.",
                        "time_start": "2020-10-30T14:45:00Z",
                        "time_end": "2020-10-30T15:00:00Z",
                        "uid": "f-vast-1184"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "DPVis: Visual Analytics with Hidden Markov Models for Disease Progression Pathways",
                        "contributors": [
                            "Bum Chul Kwon"
                        ],
                        "abstract": "Clinical researchers use disease progression models to understand patient status and characterize progression patterns from longitudinal health records. One approach for disease progression modeling is to describe patient status using a small number of states that represent distinctive distributions over a set of observed measures. Hidden Markov models (HMMs) and its variants are a class of models that both discover these states and make inferences of health states for patients. Despite the advantages of using the algorithms for discovering interesting patterns, it still remains challenging for medical experts to interpret model outputs, understand complex modeling parameters, and clinically make sense of the patterns. To tackle these problems, we conducted a design study with clinical scientists, statisticians, and visualization experts, with the goal to investigate disease progression pathways of chronic diseases, namely type 1 diabetes (T1D), Huntington's disease, Parkinson's disease, and chronic obstructive pulmonary disease (COPD). As a result, we introduce DPVis which seamlessly integrates model parameters and outcomes of HMMs into interpretable and interactive visualizations. In this study, we demonstrate that DPVis is successful in evaluating disease progression models, visually summarizing disease states, interactively exploring disease progression patterns, and building, analyzing, and comparing clinically relevant patient subgroups.",
                        "time_start": "2020-10-30T15:00:00Z",
                        "time_end": "2020-10-30T15:15:00Z",
                        "uid": "f-tvcg-2019070243"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Modeling in the Time of COVID-19: Statistical and Rule-based Mesoscale Models",
                        "contributors": [
                            "Ngan Nguyen"
                        ],
                        "abstract": "We present a new technique for the rapid modeling and construction of scientifically accurate mesoscale biological models. The resulting 3D models are based on a few 2D microscopy scans and the latest knowledge available about the biological entity, represented as a set of geometric relationships. Our new visual-programming technique is based on statistical and rule-based modeling approaches that are rapid to author, fast to construct, and easy to revise. From a few 2D microscopy scans, we determine the statistical properties of various structural aspects, such as the outer membrane shape, the spatial properties, and the distribution characteristics of the macromolecular elements on the membrane. This information is utilized in the construction of the 3D model. Once all the imaging evidence is incorporated into the model, additional information can be incorporated by interactively defining the rules that spatially characterize the rest of the biological entity, such as mutual interactions among macromolecules, and their distances and orientations relative to other structures. These rules are defined through an intuitive 3D interactive visualization as a visual-programming feedback loop. We demonstrate the applicability of our approach on a use case of the modeling procedure of the SARS-CoV-2 virion ultrastructure. This atomistic model, which we present here, can steer biological research to new promising directions in our efforts to fight the spread of the virus.",
                        "time_start": "2020-10-30T15:15:00Z",
                        "time_end": "2020-10-30T15:30:00Z",
                        "uid": "f-scivis-1175"
                    }
                ]
            },
            {
                "title": "Finance & Blockchain",
                "session_id": "f-papers-finance-blockchain",
                "chair": [
                    "J\u00f6rn Kohlhammer"
                ],
                "organizers": [],
                "display_start": "2020-10-30T16:00:00Z",
                "time_start": "2020-10-30T16:00:00Z",
                "time_end": "2020-10-30T17:15:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "finance-blockchain",
                "discord_channel_id": "767141503503826994",
                "youtube_url": "https://youtu.be/scTPYya5Rek",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrcoKbbmJ3pa3Olgn25gfk-1",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Group",
                        "contributors": [
                            "Yating Lin"
                        ],
                        "abstract": "Tax evasion is a serious economic problem for many countries, as it can undermine the government's tax system and lead to an unfair business competition environment. Recent research has applied data analytics techniques to analyze and detect tax evasion behaviors of individual taxpayers. However, they failed to support the analysis and exploration of the uprising related party transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where a group of taxpayers is involved. In this paper, we present TaxThemis, an interactive visual analytics system to help tax of\ufb01cers mine and explore suspicious tax evasion groups through analyzing heterogeneous tax-related data. A taxpayer network is constructed and fused with the trade network to detect suspicious RPTTE groups. Rich visualizations are designed to facilitate the exploration and investigation of suspicious transactions between related taxpayers with pro\ufb01t and topological data analysis. Speci\ufb01cally, we propose a calendar heatmap with a carefully-designed encoding scheme to intuitively show the evidence of transferring revenue through related party transactions. We demonstrate the usefulness and effectiveness of TaxThemis through two case studies on real-world tax-related data, and interviews with domain experts.",
                        "time_start": "2020-10-30T16:00:00Z",
                        "time_end": "2020-10-30T16:15:00Z",
                        "uid": "f-vast-1039"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "iConViz: Interactive Visual Exploration of the Default Contagion Risk of Networked-Guarantee Loans",
                        "contributors": [
                            "Zhibin Niu"
                        ],
                        "abstract": "Groups of enterprises can serve as guarantees for one another and form complex networks when obtaining loans from commercial banks. During economic slowdowns, corporate default may spread like a virus and lead to large-scale defaults or even systemic financial crises. To help financial regulatory authorities and banks manage the risk associated with networked loans, we identified the default contagion risk, a pivotal issue in developing preventive measures, and established iConViz, an interactive visual analysis tool that facilitates the closed-loop analysis process. A novel financial metric, the contagion effect, was formulated to quantify the infectious consequences of guarantee chains in this type of network. Based on this metric, we designed and implement a series of novel and coordinated views that address the analysis of financial problems. Experts evaluated the system using real-world financial data. The proposed approach grants practitioners the ability to avoid previous ad hoc analysis methodologies and extend coverage of the conventional Capital Accord to the banking industry.",
                        "time_start": "2020-10-30T16:15:00Z",
                        "time_end": "2020-10-30T16:30:00Z",
                        "uid": "f-vast-1006"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visualization of Blockchain Data: A Systematic Review",
                        "contributors": [
                            "Natkamon Tovanich"
                        ],
                        "abstract": "We present a systematic review of visual analytics tools used for the analysis of blockchains-related data. The blockchain concept has recently received considerable attention and spurred applications in a variety of domains. We systematically and quantitatively assessed 76 analytics tools that have been proposed in research as well as online by professionals and blockchain enthusiasts. Our classification of these tools distinguishes (1) target blockchains, (2) blockchain data, (3) target audiences, (4) task domains, and (5) visualization types. Furthermore, we look at which aspects of blockchain data have already been explored and point out areas that deserve more investigation in the future.",
                        "time_start": "2020-10-30T16:30:00Z",
                        "time_end": "2020-10-30T16:45:00Z",
                        "uid": "f-tvcg-20192963018"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "SilkViser: A Visual Explorer of Blockchain-based Cryptocurrency Transaction Data",
                        "contributors": [
                            "Shuirun Wei",
                            "Yunpeng Chen"
                        ],
                        "abstract": "Many blockchain-based cryptocurrencies provide users with online blockchain explorers for viewing online transaction data. However, traditional blockchain explorers mostly present transaction information in textual and tabular forms. Such forms make understanding cryptocurrency transaction mechanisms difficult for novice users (NUsers). They are also insufficiently informative for experienced users (EUsers) to recognize advanced transaction information. This study introduces a new online cryptocurrency transaction data viewing tool called SilkViser. Guided by detailed scenario and requirement analyses, we create a series of appreciating visualization designs, such as paper ledger-inspired block and blockchain visualizations and ancient copper coin-inspired transaction visualizations, to help users understand cryptocurrency transaction mechanisms and recognize advanced transaction information. We also provide a set of lightweight interactions to facilitate easy and free data exploration. Moreover, a controlled user study is conducted to quantitatively evaluate the usability and effectiveness of SilkViser. Results indicate that SilkViser can satisfy the requirements of NUsers and EUsers. Our visualization designs can compensate for the inexperience of NUsers in data viewing and attract potential users to participate in cryptocurrency transactions.",
                        "time_start": "2020-10-30T16:45:00Z",
                        "time_end": "2020-10-30T17:00:00Z",
                        "uid": "f-vast-1185"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "AgentVis: Visual Analysis of Agent Behavior with Hierarchical Glyphs",
                        "contributors": [
                            "Dylan Rees"
                        ],
                        "abstract": "Glyphs representing complex behavior provide a useful and common means of visualizing multivariate data. However, due to their complex shape, overlapping and occlusion of glyphs is a common and prominent limitation. This limits the number of discreet data tuples that can be displayed in a given image. Using a real-world application, glyphs are used to depict agent behavior in a call center. However, many call centers feature thousands of agents. A standard approach representing thousands of agents with glyphs does not scale. To accommodate the visualization incorporating thousands of glyphs we develop clustering of overlapping glyphs into a single parent glyph. This hierarchical glyph represents the mean value of all child agent glyphs, removing overlap and reducing visual clutter. Multi-variate clustering techniques are explored and developed in collaboration with domain experts in the call center industry. We implement dynamic control of glyph clusters according to zoom level and customized distance metrics, to utilize image space with reduced overplotting and cluttering. We demonstrate our technique with examples and a usage scenario using real-world call-center data to visualize thousands of call center agents, revealing insight into their behavior and reporting feedback from expert call-center analysts.",
                        "time_start": "2020-10-30T17:00:00Z",
                        "time_end": "2020-10-30T17:15:00Z",
                        "uid": "f-tvcg-2019090336"
                    }
                ]
            },
            {
                "title": "Multidimensional Data",
                "session_id": "f-papers-multidim-data",
                "chair": [
                    "Paul Rosen"
                ],
                "organizers": [],
                "display_start": "2020-10-30T16:00:00Z",
                "time_start": "2020-10-30T16:00:00Z",
                "time_end": "2020-10-30T17:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "multidimensional-data",
                "discord_channel_id": "767141510629556264",
                "youtube_url": "https://youtu.be/sSU11LA30qE",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrfc-CFGp4FKZivxadXcISkn",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "SMAP: A Joint Dimensionality Reduction Scheme for Secure Multi-Party Visualization",
                        "contributors": [
                            "Jiazhi Xia"
                        ],
                        "abstract": "Nowadays, as data becomes increasingly complex and distributed, data analyses often involve several related datasets that are stored on different servers and probably owned by different stakeholders. While there is an emerging need to provide these stakeholders with a full picture of their data under a global context, conventional visual analytical methods, such as dimensionality reduction, could expose data privacy when multi-party datasets are fused into a single site to build point-level relationships. In this paper, we reformulate the conventional t-SNE method from the single-site mode into a secure distributed infrastructure. We present a secure multi-party scheme for joint t-SNE computation, which can minimize the risk of data leakage. Aggregated visualization can be optionally employed to hide disclosure of point-level relationships. We build a prototype system based on our method, SMAP, to support the organization, computation, and exploration of secure joint embedding. We demonstrate the effectiveness of our approach with three case studies, one of which is based on the deployment of our system in real-world applications.",
                        "time_start": "2020-10-30T16:00:00Z",
                        "time_end": "2020-10-30T16:15:00Z",
                        "uid": "f-vast-1064"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Implicit Multidimensional Projection of Local Subspaces",
                        "contributors": [
                            "Rongzheng Bian"
                        ],
                        "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.",
                        "time_start": "2020-10-30T16:15:00Z",
                        "time_end": "2020-10-30T16:30:00Z",
                        "uid": "f-info-1330"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Multi-Perspective, Simultaneous Embedding",
                        "contributors": [
                            "Vahan Huroyan"
                        ],
                        "abstract": "We describe MPSE: a Multi-Perspective Simultaneous Embedding method for visualizing high-dimensional data, based on multiple pairwise distances between the data points. Specifically, MPSE computes positions for the points in 3D and provides different views into the data by means of 2D projections (planes) that preserve each of the given distance matrices. We consider two versions of the problem: fixed projections and variable projections. MPSE with fixed projections takes as input a set of pairwise distance matrices defined on the data points, along with the same number of projections and embeds the points in 3D so that the pairwise distances are preserved in the given projections. MPSE with variable projections takes as input a set of pairwise distance matrices and embeds the points in 3D while also computing the appropriate projections that preserve the pairwise distances. The proposed approach can be useful in multiple scenarios: from creating simultaneous embedding of multiple graphs on the same set of vertices, to reconstructing a 3D object from multiple 2D snapshots, to analyzing data from multiple points of view. We provide a functional prototype of MPSE that is based on an adaptive and stochastic generalization of multi-dimensional scaling to multiple distances and multiple variable projections. We provide an extensive quantitative evaluation with datasets of different sizes and using different number of projections, as well as several examples that  illustrate the quality of the resulting solutions.",
                        "time_start": "2020-10-30T16:30:00Z",
                        "time_end": "2020-10-30T16:45:00Z",
                        "uid": "f-info-1382"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visual Analysis of Large Multivariate Scattered Data using Clustering and Probabilistic Summaries",
                        "contributors": [
                            "Tobias Rapp"
                        ],
                        "abstract": "Rapidly growing data sizes of scientific simulations pose significant challenges for interactive visualization and analysis techniques. In this work, we propose a compact probabilistic representation to interactively visualize large scattered datasets. In contrast to previous approaches that represent blocks of volumetric data using probability distributions, we model clusters of arbitrarily structured multivariate data. In detail, we discuss how to efficiently represent and store a high-dimensional distribution for each cluster. We observe that it suffices to consider low-dimensional marginal distributions for two or three data dimensions at a time to employ common visual analysis techniques. Based on this observation, we represent high-dimensional distributions by combinations of low-dimensional Gaussian mixture models. We discuss the application of common interactive visual analysis techniques to this representation. In particular, we investigate several frequency-based views, such as density plots in 1D and 2D, density-based parallel coordinates, and a time histogram. We visualize the uncertainty introduced by the representation, discuss a level-of-detail mechanism, and explicitly visualize outliers. Furthermore, we propose a spatial visualization by splatting anisotropic 3D Gaussians for which we derive a closed-form solution. Lastly, we describe the application of brushing and linking to this clustered representation. Our evaluation on several large, real-world datasets demonstrates the scaling of our approach.",
                        "time_start": "2020-10-30T16:45:00Z",
                        "time_end": "2020-10-30T17:00:00Z",
                        "uid": "f-scivis-1115"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
                        "contributors": [
                            "Angelos Chatzimparmpas"
                        ],
                        "abstract": "t-Distributed Stochastic Neighbor Embedding (t-SNE) for the visualization of multidimensional data has proven to be a popular approach, with successful applications in a wide range of domains. Despite their usefulness, t-SNE projections canbe hard to interpret or even misleading, which hurts the trustworthiness of the results. Understanding the details of t-SNE itself and the reasons behind specific patterns in its output may be a daunting task, especially for non-experts in dimensionality reduction. In this work, we present t-viSNE, an interactive tool for the visual exploration of t-SNE projections that enables analysts to inspect different aspects of their accuracy and meaning, such as the effects of hyper-parameters, distance and neighborhood preservation, densities and costs of specific neighborhoods, and the correlations between dimensions and visual patterns. We propose a coherent, accessible, and well-integrated collection of different views for the visualization of t-SNE projections. The applicability and usability of t-viSNE are demonstrated through hypothetical usage scenarios with real data sets. Finally, we present the results of a user study where the tool's effectiveness was evaluated. By bringing to light information that would normally be lost after running t-SNE, we hope to support analysts in using t-SNE and making its results better understandable.",
                        "time_start": "2020-10-30T17:00:00Z",
                        "time_end": "2020-10-30T17:15:00Z",
                        "uid": "f-tvcg-2019070227"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Data-Driven Space-Filling Curves",
                        "contributors": [
                            "Liang Zhou"
                        ],
                        "abstract": "We propose a data-driven space-filling curve method for 2D and 3D visualization. Our flexible data-driven space-filling curve traverses the data elements in the spatial domain in a way that the resulting linearization better preserves features in space compared to existing methods. We achieve such data coherency by calculating a Hamiltonian path that approximately minimizes an objective function that describes the similarity of data values and location coherency in a neighborhood. Our extended variant even supports multiscale data via quadtrees and octrees. Our method is useful in many areas of visualization, including multivariate or comparative visualization, ensemble visualization of 2D and 3D data on regular grids, or multiscale visual analysis of particle simulations. The effectiveness of our method is evaluated with numerical comparisons to existing techniques and through examples of ensemble and multivariate datasets.",
                        "time_start": "2020-10-30T17:15:00Z",
                        "time_end": "2020-10-30T17:30:00Z",
                        "uid": "f-scivis-1140"
                    }
                ]
            },
            {
                "title": "Theory",
                "session_id": "f-papers-theory",
                "chair": [
                    "Tamara Munzner"
                ],
                "organizers": [],
                "display_start": "2020-10-30T16:00:00Z",
                "time_start": "2020-10-30T16:00:00Z",
                "time_end": "2020-10-30T17:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "theory",
                "discord_channel_id": "767141518209056799",
                "youtube_url": "https://youtu.be/5cWWPBE_DTQ",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrfHvVRIRUAt8WueOFXR0VrC",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Why Visualize? Untangling a Large Network of Arguments",
                        "contributors": [
                            "Dirk Streeb"
                        ],
                        "abstract": "Visualization has been deemed a useful technique by researchers and practitioners, alike, leaving a trail of arguments behind that reason why visualization works. In addition, examples of misleading usages of visualizations in information communication have occasionally been pointed out. Thus, to contribute to the fundamental understanding of our discipline, we require a comprehensive collection of arguments on \u201cwhy visualize?\u201d (or \u201cwhy not?\u201d), untangling the rationale behind positive and negative viewpoints. In this paper, we report a theoretical study to understand the underlying reasons of various arguments; their relationships (e.g., built-on, and conflict); and their respective dependencies on tasks, users, and data. We curated an argumentative network based on a collection of arguments from various fields, including information visualization, cognitive science, psychology, statistics, philosophy, and others. Our work proposes several categorizations for the arguments, and makes their relations explicit. We contribute the first comprehensive and systematic theoretical study of the arguments on visualization. Thereby, we provide a roadmap towards building a foundation for visualization theory and empirical research as well as for practical application in the critique and design of visualizations. In addition, we provide our argumentation network and argument collection online at https://whyvis.dbvis.de, supported by an interactive visualization.",
                        "time_start": "2020-10-30T16:00:00Z",
                        "time_end": "2020-10-30T16:15:00Z",
                        "uid": "f-tvcg-2019030100"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Introducing Layers of Meaning (LoM): A Framework to Reduce Semantic Distance of Visualization In Humanistic Research",
                        "contributors": [
                            "Houda Lamqaddam"
                        ],
                        "abstract": "Information visualization (infovis) is a powerful tool for exploring rich datasets. Within the context of humanistic research, rich qualitative data and domain specificities make traditional infovis approaches appear reductive and disconnected, leading to low adoption. In this paper, we use a multi-step approach to scrutinize the relationship between infovis and the humanities and suggest new directions for it. We first look into infovis from the humanistic perspective by exploring the humanistic literature around infovis. We then validate and expand upon those findings though a co-design workshop with humanist researchers and infovis experts. Then, we translate our findings into guidelines for designers and conduct a design critique exercise to explore their effect on humanist researchers perception. Based on these steps, we propose a framework to reduce the semantic distance between humanist researchers and visualizations of their research material, by grounding infovis tools in time and space, physicality, terminology, nuance, and provenance.",
                        "time_start": "2020-10-30T16:15:00Z",
                        "time_end": "2020-10-30T16:30:00Z",
                        "uid": "f-info-1098"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Data Visceralization: Enabling Deeper Understanding of Data Using Virtual Reality",
                        "contributors": [
                            "Benjamin Lee"
                        ],
                        "abstract": "A fundamental part of data visualization is transforming data to map abstract information onto visual attributes. While this abstraction is a powerful basis for data visualization, the connection between the representation and the original underlying data (i.e., what the quantities and measurements actually correspond with in reality) can be lost. On the other hand, virtual reality (VR) is being increasingly used to represent real and abstract models as natural experiences to users. In this work, we explore the potential of using VR to help restore the basic understanding of units and measures that are often abstracted away in data visualization in an approach we call data visceralization. By building VR prototypes as design probes, we identify key themes and factors for data visceralization. We do this first through a critical reflection by the authors, then by involving external participants. We find that data visceralization is an engaging way of understanding the qualitative aspects of physical measures and their real-life form, which complements analytical and quantitative understanding commonly gained from data visualization. However, data visceralization is most effective when there is a one-to-one mapping between data and representation, with transformations such as scaling affecting this understanding. We conclude with a discussion of future directions for data visceralization.",
                        "time_start": "2020-10-30T16:30:00Z",
                        "time_end": "2020-10-30T16:45:00Z",
                        "uid": "f-info-1084"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Insights From Experiments with Rigor in an EvoBio Design Study",
                        "contributors": [
                            "Jen Rogers"
                        ],
                        "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work reporting on design studies, informed by a handful of theoretical frameworks, and\napplied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed\na new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration\nwith evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.",
                        "time_start": "2020-10-30T16:45:00Z",
                        "time_end": "2020-10-30T17:00:00Z",
                        "uid": "f-info-1177"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Design Space of Vision Science Methods for Visualization Research",
                        "contributors": [
                            "Madison Elliott"
                        ],
                        "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.",
                        "time_start": "2020-10-30T17:00:00Z",
                        "time_end": "2020-10-30T17:15:00Z",
                        "uid": "f-info-1362"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Structured Review of Data Management Technology for Interactive Visualization and Analysis",
                        "contributors": [
                            "Carlos Scheidegger"
                        ],
                        "abstract": "In the last two decades, interactive visualization and analysis have become a central tool in data-driven decision making. Concurrently to the contributions in data visualization, research in data management has produced technology that directly benefits interactive analysis. Here, we contribute a systematic review of 30 years of work in this adjacent field, and highlight techniques and principles we believe to be underappreciated in visualization work. We structure our review along two axes. First, we use task taxonomies from the visualization literature to structure the space of interactions in usual systems. Second, we created a categorization of data management work that strikes a balance between specificity and generality.\nConcretely, we contribute a characterization of 131 research papers along these two axes. We find that five notions in data management venues fit interactive visualization systems well: materialized views, approximate query processing, user modeling and query prediction, multi-query optimization, lineage techniques, and indexing techniques.\nIn addition, we find a preponderance of work in materialized views and approximate query processing, most targeting a limited subset of the interaction tasks in the taxonomy we used. This suggests natural avenues of future research both in visualization and data management.\nOur categorization both changes how we visualization researchers design and build our systems, and highlights where future work is necessary.",
                        "time_start": "2020-10-30T17:15:00Z",
                        "time_end": "2020-10-30T17:30:00Z",
                        "uid": "f-info-1208"
                    }
                ]
            },
            {
                "title": "Volume Visualization",
                "session_id": "f-papers-volvis",
                "chair": [
                    "Hank Childs"
                ],
                "organizers": [],
                "display_start": "2020-10-30T16:00:00Z",
                "time_start": "2020-10-30T16:00:00Z",
                "time_end": "2020-10-30T17:30:00Z",
                "discord_category": "vis full papers",
                "discord_channel": "volume-visualization",
                "discord_channel_id": "767141525422473266",
                "youtube_url": "https://youtu.be/s6CZzP6zT7s",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojreiqI4tcG5gWd8WhnspzYYi",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Advanced Rendering of Line Data with Ambient Occlusion and Transparency",
                        "contributors": [
                            "David Gro\u00df"
                        ],
                        "abstract": "3D Lines are a widespread rendering primitive for the visualization of data from research fields like fluid dynamics or fiber tractography. Global illumination effects and transparent rendering improve the perception of three-dimensional features and decrease occlusion within the data set, thus enabling better understanding of complex line data. We present an efficient approach for high quality GPU-based rendering of line data with ambient occlusion and transparency effects. Our approach builds on GPU-based raycasting of rounded cones, which are geometric primitives similar to truncated cones, but with spherical endcaps. Object space ambient occlusion is provided by an efficient voxel cone tracing approach. Our core contribution is a new fragment visibility sorting strategy that allows for interactive visualization of line data sets with millions of line segments. We improve performance further by exploiting hierarchical opacity maps.",
                        "time_start": "2020-10-30T16:00:00Z",
                        "time_end": "2020-10-30T16:15:00Z",
                        "uid": "f-scivis-1120"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Comparison of Rendering Techniques for 3D Line Sets with Transparency",
                        "contributors": [
                            "Michael Kern"
                        ],
                        "abstract": " This paper presents a comprehensive study of rendering techniques for 3D line sets with transparency. The rendering of transparent lines is widely used for visualizing trajectories of tracer particles in flow fields. Transparency is then used to fade out lines deemed unimportant, based on, for instance, geometric properties or attributes defined along with them. Accurate blending of transparent lines requires rendering the lines in back-to-front or front-to-back order, yet enforcing this order for space-filling 3D line sets with extremely high-depth complexity becomes challenging. In this paper, we study CPU and GPU rendering techniques for transparent 3D line sets. We compare accurate and approximate techniques using optimized implementations and several benchmark data sets. We discuss the effects of data size and transparency on quality, performance, and memory consumption. Based on our study, we propose two improvements to per-pixel fragment lists and multi-layer alpha blending. The first improves the rendering speed via an improved GPU sorting operation, and the second improves rendering quality via transparency-based bucketing.",
                        "time_start": "2020-10-30T16:15:00Z",
                        "time_end": "2020-10-30T16:30:00Z",
                        "uid": "f-tvcg-2019080294"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Discrete Probabilistic Approach to Dense Flow Visualization",
                        "contributors": [
                            "Daniel Preu\u00df"
                        ],
                        "abstract": "Dense \ufb02ow visualization is a popular visualization paradigm. Traditionally, the various models and methods in this area use a continuous formulation, resting upon the solid foundation of functional analysis. In this work, we examine a discrete formulation of dense \ufb02ow visualization. From probability theory, we derive a similarity matrix that measures the similarity between different points in the \ufb02ow domain, leading to the discovery of a whole new class of visualization models. Using this matrix, we propose a novel visualization approach consisting of the computation of spectral embeddings, i.e., characteristic domain maps, de\ufb01ned by particle mixture probabilities. These embeddings are scalar \ufb01elds that give insight into the mixing processes of the \ufb02ow on different scales. The approach of spectral embeddings is already well studied in image segmentation, and we see that spectral embeddings are connected to Fourier expansions and frequencies. We showcase the utility of our method using different 2D and 3D \ufb02ows. ",
                        "time_start": "2020-10-30T16:30:00Z",
                        "time_end": "2020-10-30T16:45:00Z",
                        "uid": "f-tvcg-2019090310"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Ray Tracing Structured AMR Data Using ExaBricks",
                        "contributors": [
                            "Stefan Zellmann"
                        ],
                        "abstract": "Structured Adaptive Mesh Refinement (Structured AMR) enables simulations to adapt the domain resolution to save computation and storage, and has become one of the dominant data representations used by scientific simulations; however, efficiently rendering such data remains a challenge.  We present an efficient\n  approach for volume- and iso-surface ray tracing of Structured AMR data on GPU-equipped workstations, using a combination of two different data structures. Together, these allow the ray tracing based renderer to quickly determine which segments along the ray need to be integrated at which frequency, while also providing quick access to all data values required for a smooth sample reconstruction kernel. Our method makes use of RTX ray tracing hardware for surface rendering, ray marching, space skipping, and adaptive sampling; and allows for interactive changes to the transfer function and implicit iso-surfacing thresholds.  We demonstrate that our method achieves high performance with little memory overhead, enabling interactive high quality rendering of complex AMR data sets on individual GPU workstations.",
                        "time_start": "2020-10-30T16:45:00Z",
                        "time_end": "2020-10-30T17:00:00Z",
                        "uid": "f-scivis-1059"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Homomorphic-Encrypted Volume Rendering",
                        "contributors": [
                            "Sebastian Mazza"
                        ],
                        "abstract": "Computationally-demanding tasks are typically calculated in dedicated data centers and real-time visualizations follow this trend as well. Some rendering tasks, however, require highest-level of confidentiality so that no other party, besides the owner, can read or see the sensitive data. We present a direct volume rendering approach that performs volume rendering directly on encrypted volume data by using the homomorphic Paillier encryption algorithm. This keeps the volume data and rendered image uninterpretable for the rendering server. Our volume rendering pipeline introduces novel approaches for encrypted-data compositing, interpolation, opacity modulation as well as simple transfer function design, where each of these routines maintains the highest-level of privacy.\nWe present performance and memory overhead analysis that is associated with our privacy-preserving scheme. Our approach is open and secure by design as opposed to secure through obscurity. The owner of the data only has to keep her secure key confidential to guarantee the privacy of her volume data and the rendered images. Our work is to our knowledge the first privacy-preserving remote volume-rendering approach which does not require trusting any involved server, even in case the server is compromised, no sensitive data will be leaked to a foreign party.",
                        "time_start": "2020-10-30T17:00:00Z",
                        "time_end": "2020-10-30T17:15:00Z",
                        "uid": "f-scivis-1132"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "The Mixture Graph - A Data Structure for Compressing, Rendering, and Querying Segmentation Histograms",
                        "contributors": [
                            "Khaled Al-Thelaya",
                            "Marco Agus",
                            "Jens Schneider"
                        ],
                        "abstract": "In this paper, we present a novel data structure, called the Mixture Graph. This data structure allows us to compress, render, and query segmentation histograms. Such histograms arise when building a mipmap of a volume containing segmentation IDs. Each voxel in the histogram mipmap contains a convex combination (mixture) of segmentation IDs. Each mixture represents the distribution of IDs in the respective voxel's children. Our method factorizes these mixtures into a series of linear interpolations between exactly two segmentation IDs. The result is represented as a directed acyclic graph (DAG) whose nodes are topologically ordered. Pruning replicate nodes in the tree followed by compression allows us to store the resulting data structure efficiently. During rendering, transfer functions are propagated from sources (leafs) through the DAG to allow for efficient, pre-filtered rendering at interactive frame rates. Assembly of histogram contributions across the footprint of a given volume allows us to efficiently query partial histograms, achieving up to 178x speed-up over naive parallelized range queries. Additionally, we apply the Mixture Graph to compute correctly pre-filtered volume lighting and to interactively explore segments based on shape, geometry, and orientation using multi-dimensional transfer functions.",
                        "time_start": "2020-10-30T17:15:00Z",
                        "time_end": "2020-10-30T17:30:00Z",
                        "uid": "f-scivis-1038"
                    }
                ]
            }
        ]
    },
    "s-papers": {
        "event": "VIS Short Papers",
        "long_name": "VIS Short Papers",
        "event_type": "Paper Presentations",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Vis and Scientific Computing",
                "session_id": "s-papers-vis-scicomp",
                "chair": [
                    "Christina Gillman"
                ],
                "organizers": [],
                "display_start": "2020-10-27T17:45:00Z",
                "time_start": "2020-10-27T17:45:00Z",
                "time_end": "2020-10-27T19:30:00Z",
                "discord_category": "vis short papers",
                "discord_channel": "vis-and-scientific-computing",
                "discord_channel_id": "767122243746005013",
                "youtube_url": "https://youtu.be/yxHYxo2rT8c",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrcjSXAchEUr-mKluf3Hxz-P",
                "time_slots": [
                    {
                        "type": null,
                        "title": "Short Papers and Posters Opening",
                        "contributors": [
                            "Short Papers Chairs",
                            "Posters Chairs"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T17:45:00Z",
                        "time_end": "2020-10-27T18:00:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Topological Analysis of Magnetic Reconnection in Kinetic Plasma Simulations",
                        "contributors": [
                            "Divya Banesh"
                        ],
                        "abstract": "Magnetic reconnection is a ubiquitous plasma process in which oppositely directed magnetic field lines break and rejoin, resulting in a change of the magnetic field topology. Reconnection generates magnetic islands: regions enclosed by magnetic field lines and separated by reconnection points. Proper identification of these features is important to understand particle acceleration and overall behavior of plasma. We present a contour-tree based visualization for robust and objective identification of islands and reconnection points in two-dimensional (2D) magnetic reconnection simulations. The application of this visualization to a simple simulation has revealed a physical phenomenon previously not reported, resulting in a more comprehensive understanding of magnetic reconnection.",
                        "time_start": "2020-10-27T18:00:00Z",
                        "time_end": "2020-10-27T18:10:00Z",
                        "uid": "s-short-1042"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Virtual Frame Buffer Abstraction for Parallel Rendering of Large Tiled Display Walls",
                        "contributors": [
                            "Mengjiao Han"
                        ],
                        "abstract": "We present dw2, a flexible and easy-to-use software infrastructure for interactive rendering of large tiled display walls. Our library represents the tiled display wall as a single virtual screen through a display \u201cservice\u201d, which renderers connect to and send image tiles to be displayed, either from an on-site or remote cluster. The display service can be easily configured to support a range of typical network and display hardware configurations; the client library provides a straightforward interface for easy integration into existing renderers. We evaluate the performance of our display wall service in different configurations using a CPU and GPU ray tracer, in both on-site and remote rendering scenarios using multiple display walls.",
                        "time_start": "2020-10-27T18:10:00Z",
                        "time_end": "2020-10-27T18:20:00Z",
                        "uid": "s-short-1051"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Uncertain Transport in Unsteady Flows",
                        "contributors": [
                            "Tobias Rapp"
                        ],
                        "abstract": "We study uncertainty in the dynamics of time-dependent flows by identifying barriers and enhancers to stochastic transport. This topological segmentation is closely related to the theory of Lagrangian coherent structures and is based on a recently introduced quantity, the diffusion barrier strength (DBS). The DBS is defined similar to the finite-time Lyapunov exponent (FTLE), but incorporates diffusion during flow integration. Height ridges of the DBS indicate stochastic transport barriers and enhancers, i.e. material surfaces that are minimally or maximally diffusive. To apply these concepts to real-world data, we represent uncertainty in a flow by a stochastic differential equation that consists of a deterministic and a stochastic component modeled by a Gaussian. With this formulation we identify barriers and enhancers to stochastic transport, without performing expensive Monte Carlo simulation and with a computational complexity comparable to FTLE. In addition, we propose a complementary visualization to convey the absolute scale of uncertainties in the Lagrangian frame of reference. This enables us to study uncertainty in real-world datasets, for example due to small deviations, data reduction, or estimated from multiple ensemble runs.",
                        "time_start": "2020-10-27T18:20:00Z",
                        "time_end": "2020-10-27T18:30:00Z",
                        "uid": "s-short-1052"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "High-quality real-time raycasting and raytracing of streamtubes with sparse voxel octrees",
                        "contributors": [
                            "Tim McGraw"
                        ],
                        "abstract": "Voxel-based rendering and signed distance function (SDF) raycasting have\nbeen active areas of graphics research recently because they simplify high-quality graphical effects like ambient occlusion and shadowing as compared to rasterization. Much work has centered around converting triangle meshes to sparse voxel octrees (SVOs) and developing memory efficient storage schemes but little exploration of the implications to scientific visualization has taken place. In this work we explore techniques for high-performance rendering of tubes, such as streamtubes used for visualizing vector fields and fiber tracts from diffusion tensor MRI. We first present our method for generating a voxelization of the tubes, and then describe several methods for rendering:\nraytracing a SVO storing straight tube segments in the leaves, and hybrid raytracing/raycasting a SVO storing curved tube segments. We discuss the tradeoffs inherent in these different representations and compare the rendering techniques and results. Compared to standard graphics pipeline approaches, like using the geometry shader, we achieve joining, capping, and smooth circular cross-sections with minimal additional effort.",
                        "time_start": "2020-10-27T18:30:00Z",
                        "time_end": "2020-10-27T18:40:00Z",
                        "uid": "s-short-1105"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "GPU-based Raycasting of Hermite Spline Tubes",
                        "contributors": [
                            "Benjamin Russig"
                        ],
                        "abstract": "Visualizing curve and trajectory data is a common task in many scientific fields including medicine and physics. Tubes are an effective visualization primitive for this sort of data, but they require highly specialized renderers to achieve high image quality at frame rates sufficient for interactive visualization. We present a rendering algorithm for Hermite spline tubes, i.e. tubes that result from Hermite splines interpolating the data, with support for varying-radii circular tube cross sections. Our approach employs raycasting and works directly on this continuous representation without the need for surface tessellation, made possible by an efficient ray-tube intersection routine suitable for execution on modern GPUs.",
                        "time_start": "2020-10-27T18:40:00Z",
                        "time_end": "2020-10-27T18:50:00Z",
                        "uid": "s-short-1125"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Implicit Ray Casting of the Parallel Vectors Operator",
                        "contributors": [
                            "Ramon Witschi"
                        ],
                        "abstract": "Feature extraction is an essential aspect of scientific data analysis, as it allows for a data reduction onto relevant structures. The extraction of such features from scalar and vector fields, however, can be computationally expensive and numerically challenging. In this paper, we concentrate on 3D line features in vector fields that are defined by the parallel vectors operator. Common examples are vortex corelines and hyperbolic trajectories, i.e., lines around which particles are rotating, or from which particles are repelled and attracted locally the strongest. In our work, we use a GPU volume rendering framework to calculate the lines on-the-fly via a parallel vectors implementation in the volume rendering kernels. We achieve real-time performance for the feature curve extraction, which enables interactive filtering and parameter adjustment.",
                        "time_start": "2020-10-27T18:50:00Z",
                        "time_end": "2020-10-27T19:00:00Z",
                        "uid": "s-short-1142"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "GPU Parallel Computation of Morse-Smale Complexes",
                        "contributors": [
                            "Varshini Subhash"
                        ],
                        "abstract": "The Morse-Smale complex is a well studied topological structure that represents the gradient flow behavior of a scalar function. It supports multi-scale topological analysis and visualization of large scientific data. Its computation poses significant algorithmic challenges when considering large scale data and increased feature complexity. Several parallel algorithms have been proposed towards the fast computation of the 3D Morse-Smale complex. The non-trivial structure of the saddle-saddle connections are not amenable to parallel computation. This paper describes a fine grained parallel method for computing the Morse-Smale complex that is implemented on a GPU. The saddle-saddle reachability is first determined via a transformation into a sequence of vector operations followed by the path traversal, which is achieved via a sequence of matrix operations. Computational experiments show that the method achieves up to 7x speedup over current shared memory implementations.",
                        "time_start": "2020-10-27T19:00:00Z",
                        "time_end": "2020-10-27T19:10:00Z",
                        "uid": "s-short-1172"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Relationship-aware Multivariate Sampling Strategy for Scientific Simulation Data",
                        "contributors": [
                            "Subhashis Hazarika"
                        ],
                        "abstract": "With the increasing computational power of current supercomputers, the size of data produced by scientific simulations is rapidly growing. To reduce the storage footprint and facilitate scalable post-hoc analyses of such scientific data sets, various data reduction/summarization methods have been proposed over the years. Different flavors of sampling algorithms exist to sample the high-resolution scientific data, while preserving important data properties required for subsequent analyses. However, most of these sampling algorithms are designed for univariate data and cater to post-hoc analyses of single variables. In this work, we propose a multivariate sampling strategy which preserves the original variable relationships and enables different multivariate analyses directly on the sampled data. Our proposed strategy utilizes principal component analysis to capture the variance of multivariate data and can be built on top of any existing state-of-the-art sampling algorithms for single variables. In addition, we also propose variants of different data partitioning schemes (regular and irregular) to efficiently model the local multivariate relationships. Using two real-world multivariate data sets, we demonstrate the efficacy of our proposed multivariate sampling strategy with respect to its data reduction capabilities as well as the ease of performing efficient post-hoc multivariate analyses.",
                        "time_start": "2020-10-27T19:10:00Z",
                        "time_end": "2020-10-27T19:20:00Z",
                        "uid": "s-short-1176"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "PRAGMA: Interactively Constructing Functional Brain Parcellations",
                        "contributors": [
                            "Roza G. Bayrak",
                            "Nhung Hoang"
                        ],
                        "abstract": "A prominent goal of neuroimaging studies is mapping the human brain, in order to identify and delineate functionally-meaningful regions and elucidate their roles in cognitive behaviors. These brain regions are typically represented by atlases that capture general trends over large populations. Despite being indispensable to neuroimaging experts, population-level atlases do not capture individual differences in functional organization. In this work, we present an interactive visualization method, PRAGMA, that allows domain experts to derive scan-specific parcellations from established atlases. PRAGMA features a user-driven, hierarchical clustering scheme for defining temporally correlated parcels in varying granularity. The visualization design supports the user in making decisions on how to perform clustering, namely when to expand, collapse, or merge parcels. This is accomplished through a set of linked and coordinated views for understanding the user's current hierarchy, assessing intra-cluster variation, and relating parcellations to an established atlas. We assess the effectiveness of PRAGMA through a user study with four neuroimaging domain experts, where our results show that PRAGMA shows the potential to enable exploration of individualized and state-specific brain parcellations and to offer interesting insights into functional brain networks.",
                        "time_start": "2020-10-27T19:20:00Z",
                        "time_end": "2020-10-27T19:30:00Z",
                        "uid": "s-short-1166"
                    }
                ]
            },
            {
                "title": "Geospatial, Finance, and Health",
                "session_id": "s-papers-geo-fin-health",
                "chair": [
                    "Bahador Saket"
                ],
                "organizers": [],
                "display_start": "2020-10-28T14:00:00Z",
                "time_start": "2020-10-28T14:00:00Z",
                "time_end": "2020-10-28T15:30:00Z",
                "discord_category": "vis short papers",
                "discord_channel": "geospatial-finance-and-health",
                "discord_channel_id": "767561672457846824",
                "youtube_url": "https://youtu.be/Z70iLcKkNo0",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrdb-N8j4bujNsO7NkoARDre",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "A Review of Geospatial Content in IEEE Visualization Publications",
                        "contributors": [
                            "Laura Tateosian",
                            "Alex Yoshizumi"
                        ],
                        "abstract": "Geospatial analysis is crucial for addressing many of the world's most pressing challenges. Given this, there is immense value in improving and expanding the visualization techniques used to communicate geospatial data. In this work, we explore this important intersection \u2013 between geospatial analytics and visualization \u2013 by examining a set of recent IEEE VIS Conference papers (a selection from 2017-2019) to assess the inclusion of geospatial data and geospatial analyses within these papers. After removing the papers with no geospatial data, we organize the remaining literature into geospatial data domain categories and provide insight into how these categories relate to VIS Conference paper types. We also contextualize our results by investigating the use of geospatial terms in IEEE Visualization publications over the last 30 years. Our work provides an understanding of the quantity and role of geospatial subject matter in recent IEEE VIS publications and supplies a foundation for future meta-analytical work around geospatial analytics and geovisualization that may shed light on opportunities for innovation.",
                        "time_start": "2020-10-28T14:00:00Z",
                        "time_end": "2020-10-28T14:10:00Z",
                        "uid": "s-short-1015"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "COVIs: Supporting Temporal Visual Analysis of Covid-19 Events Usable in Data-Driven Journalism",
                        "contributors": [
                            "Roger Leite"
                        ],
                        "abstract": "Figure 1: COVis's coordinated multiple view environment: (A) Control Panel: allows the user to change the graphs scale (linear / log), metric (absolute / per million cohabitants), and dimensions (cases / deaths). (B) Line charts: present four different line charts that are coordinated to support exploration of multiple narratives. Respectively, the charts display the relation between: (B1) time x cases/deaths, (B2) time x tests, (B3) total cases/deaths x last week cases/deaths, and (B4) time x cases/deaths projection length. (C) Events Panel: displays information and source references concerning main events occurred in certain time periods. (D) Events Time Chart: chart presenting the policy changes of a country over time. (E) Country Cards: show information concerning the analysed group of countries, allowing the exclusion and inclusion of different countries into the analysis.",
                        "time_start": "2020-10-28T14:10:00Z",
                        "time_end": "2020-10-28T14:20:00Z",
                        "uid": "s-short-1017"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "TradAO: A Visual Analytics System for Trading Algorithm Optimization",
                        "contributors": [
                            "Ka Wing Tsang"
                        ],
                        "abstract": "With the wide applications of algorithmic trading, it has become critical for traders to build a winning trading algorithm to beat the market. However, due to the lack of efficient tools, traders mainly rely on their memory to manually compare the algorithm instances of a trading algorithm and further select the best trading algorithm instance for the real trading deployment. We work closely with industry practitioners to discover and consolidate user requirements and develop an interactive visual analytics system for trading algorithm optimization. Structured expert interviews are conducted to evaluate TradAO and a representative case study is documented for illustrating the system effectiveness. To the best of our knowledge, previous financial data visual analyses have mainly aimed to assist investment managers in investment portfolio analysis but have neglected the need of traders in developing trading algorithms for portfolio execution. TradAO is the first visual analytics system that assists users in comprehensively exploring the performances of a trading algorithm with different parameter settings.",
                        "time_start": "2020-10-28T14:20:00Z",
                        "time_end": "2020-10-28T14:30:00Z",
                        "uid": "s-short-1062"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Study of Opacity Ranges for Transparent Overlays in 3D Landscapes",
                        "contributors": [
                            "Jan Hombeck"
                        ],
                        "abstract": "When visualizing data in a realistically rendered 3D virtual environment, it is often important to represent not only the 3D scene but also overlaid information about additional, abstract data. These overlays must be usefully visible, i.e. be readable enough to convey the information they represent, but remain unobtrusive to avoid cluttering the view. We take a step toward establishing guidelines for designing such overlays by studying the relationship between three different patterns (filled, striped and dotted patterns), two pattern densities, the presence or not of a solid outline, two types of background (blank and with trees), and the opacity of the overlay. For each combination of factors, participants set the faintest and the strongest acceptable opacity values. Results from this first study suggest that i) ranges of acceptable opacities are around 20-70%, that ii) ranges can be extended by 5% by using an outline, and that iii) ranges shift based on features like pattern and density.",
                        "time_start": "2020-10-28T14:30:00Z",
                        "time_end": "2020-10-28T14:40:00Z",
                        "uid": "s-short-1136"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Co-visualization of air temperature and urban data for visual exploration",
                        "contributors": [
                            "Jacques Gautier"
                        ],
                        "abstract": "Urban climate data remain complex to analyze regarding their spatial distribution. The co-visualization of simulated air temperature into urban models could help experts to analyze horizontal and vertical spatial distributions. We design a co-visualization framework enabling simulated air temperature data exploration, based on the graphic representation of three types of geometric proxies, and their co-visualization with a 3D urban model with various possible rendering styles. Through this framework, we aim at allowing meteorological researchers to visually analyze and interpret the relationships between simulated air temperature data and urban morphology.",
                        "time_start": "2020-10-28T14:40:00Z",
                        "time_end": "2020-10-28T14:50:00Z",
                        "uid": "s-short-1137"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Visual Analytics Approach to Scheduling Customized Shuttle Buses via Perceiving Passengers' Travel Demands'",
                        "contributors": [
                            "Qiangqiang Liu"
                        ],
                        "abstract": "Shuttle buses have been a popular means to move commuters sharing similar origins and destinations during periods of high travel demand. However, planning and deploying reasonable, customized service bus systems becomes challenging when the commute demand is rather dynamic. It is difficult, if not impossible to form a reliable, unbiased estimation of user needs in such a case using traditional modeling methods. We propose a visual analytics approach to facili- tating assessment of actual, varying travel demands and planning of night customized shuttle systems. A preliminary case study verifies the efficacy of our approach.",
                        "time_start": "2020-10-28T14:50:00Z",
                        "time_end": "2020-10-28T15:00:00Z",
                        "uid": "s-short-1140"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Designing for Ambiguity: Visual Analytics in Avalanche Forecasting",
                        "contributors": [
                            "Stan Nowak"
                        ],
                        "abstract": "Ambiguity, an information state where multiple interpretations are plausible, is a common challenge in visual analytics (VA) systems. We discuss lessons learned from a case study designing VA tools for Canadian avalanche forecasters. Avalanche forecasting is a complex and collaborative risk-based decision-making and analysis domain, demanding experience and knowledge-based interpretation of human reported and uncertain data. Differences in reporting practices, organizational contexts, and the particularities of individual reports result in a variety of potential interpretations that have to be negotiated as part of the forecaster's sensemaking processes. We describe our preliminary research using glyphs to support sensemaking under ambiguity. Ambiguity is not unique to public avalanche forecasting. There are many other domains where the way data are measured and reported vary in ways not accounted explicitly in the data, and require analysts to negotiate multiple potential meanings. We argue that ambiguity is under-served by visualization research and would benefit from more explicit VA support.",
                        "time_start": "2020-10-28T15:00:00Z",
                        "time_end": "2020-10-28T15:10:00Z",
                        "uid": "s-short-1162"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Visual Analytics Based Decision Making Environment for COVID-19 Modeling and Visualization",
                        "contributors": [
                            "Shehzad Afzal"
                        ],
                        "abstract": "Public health officials dealing with pandemics like COVID-19 have to evaluate and prepare response plans. This planning phase requires not only looking into the spatiotemporal dynamics and impact of the pandemic using simulation models, but they also need to plan and ensure the availability of resources under different spread scenarios. To this end, we have developed a visual analytics environment that enables public health officials to model, simulate, and explore the spread of COVID-19 by supplying county-level information such as population, demographics, and hospital beds. This environment facilitates users to explore spatiotemporal model simulation data relevant to COVID-19 through a geospatial map with linked statistical views, apply different decision measures at different points in time, and understand their potential impact. Users can drill-down to county-level details such as the number of sicknesses, deaths, needs for hospitalization, and variations in these statistics over time. We demonstrate the usefulness of this environment through a use case study and also provide feedback from domain experts. We also provide details about future extensions and potential applications of this work.",
                        "time_start": "2020-10-28T15:10:00Z",
                        "time_end": "2020-10-28T15:20:00Z",
                        "uid": "s-short-1230"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Mapping the Global South: Equal-Area Projections for Choropleth Maps",
                        "contributors": [
                            "Gabriela Molina Leon"
                        ],
                        "abstract": "Choropleth maps are among the most common visualization techniques used to present geographical data. These maps require an equal-area projection but there are no clear criteria to select one. We collaborated with social scientists researching on the Global South, interested in using choropleth maps, to investigate their design choices according to their research tasks. We conducted a survey with 20 researchers where we asked them to design world choropleth maps. The results suggest that the design choices of projection, map center, scale, and color scheme, were influenced by the personal research goals and the tasks. The map projection was considered the most important choice and the Equal Earth projection was the most common projection used. Our study takes the first step on investigating projection choices for world choropleth maps in applied visualization research.",
                        "time_start": "2020-10-28T15:20:00Z",
                        "time_end": "2020-10-28T15:30:00Z",
                        "uid": "s-short-1236"
                    }
                ]
            },
            {
                "title": "Systems, Libraries, and Algorithms",
                "session_id": "s-papers-sys-lib-alg",
                "chair": [
                    "Alper Sarikaya"
                ],
                "organizers": [],
                "display_start": "2020-10-28T16:00:00Z",
                "time_start": "2020-10-28T16:00:00Z",
                "time_end": "2020-10-28T17:20:00Z",
                "discord_category": "vis short papers",
                "discord_channel": "systems-libraries-and-algorithms",
                "discord_channel_id": "767561719966466049",
                "youtube_url": "https://youtu.be/bi6FfsWV_9k",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrcjcKMpBpR0xk_VP5WP-cMc",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Accelerating Force-Directed Graph Drawing with RT Cores",
                        "contributors": [
                            "Stefan Zellmann"
                        ],
                        "abstract": "Graph drawing with spring embedders employs a V \u00d7 V computation phase over the graph's vertex set to compute repulsive forces. Here, the efficacy of forces diminishes with distance: a vertex can effectively only influence other vertices in a certain radius around its position. Therefore, the algorithm lends itself to an implementation using search data structures to reduce the runtime complexity. NVIDIA RT cores implement hierarchical tree traversal in hardware. We show how to map the problem of finding graph layouts with force-directed methods to a ray tracing problem that can subsequently be implemented with dedicated ray tracing hardware. With that, we observe speedups of 4\u00d7 to 13\u00d7 over a CUDA software implementation.",
                        "time_start": "2020-10-28T16:00:00Z",
                        "time_end": "2020-10-28T16:10:00Z",
                        "uid": "s-short-1027"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Fast and Flexible Overlap Detection for Chart Labeling with Occupancy Bitmaps",
                        "contributors": [
                            "Chanwut Kittivorawong"
                        ],
                        "abstract": "Legible labels should not overlap with other marks in a chart. The state-of-the-art labeling algorithm detects overlaps using a set of points to approximate each mark's shape. This approach is inefficient for large marks or many marks as it requires too many points to detect overlaps. In response, we present a novel label placement algorithm, which leverages occupancy bitmaps to accelerate overlap detection. To create an occupancy bitmap, we rasterize all marks onto a bitmap based on the area they occupy in the chart.\nIt supports rasterizations of marks with any shape. With the bitmap, we can efficiently place labels without overlapping existing marks, regardless of the number and geometric complexity of the marks. Our algorithm offers significant performance improvements over the state-of-the-art approach while placing a similar number of labels.",
                        "time_start": "2020-10-28T16:10:00Z",
                        "time_end": "2020-10-28T16:20:00Z",
                        "uid": "s-short-1028"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "FAVR - Accelerating Direct Volume Rendering for Virtual Reality Systems",
                        "contributors": [
                            "Andre Waschk"
                        ],
                        "abstract": "In recent years, virtual reality (VR) has become readily available using robust and affordable head-mounted display (HMD) systems. Several VR-based scientific visualization solutions were proposed recently, but direct volume rendering (DVR) was  considered only in a handful of VR-applications. We attribute this to the high computational demand of DVR and the limited rendering budget available for VR systems. For a heavily fragment-bound method such as DVR, it is challenging to achieve the very high update rates essential for VR. We propose an acceleration technique designed to take advantage of the specific characteristics of HMD systems. We utilize an adaptive rendering approach based on the lens distortion of HMDs and the visual perception of the human eye. Our implementation reduces the rendering cost of DVR while providing an experience indistinguishable to standard rendering techniques.",
                        "time_start": "2020-10-28T16:20:00Z",
                        "time_end": "2020-10-28T16:30:00Z",
                        "uid": "s-short-1108"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "DRUIDJS - A JavaScript Library for Dimensionality Reduction",
                        "contributors": [
                            "Rene Cutura"
                        ],
                        "abstract": "Dimensionality reduction (DR) is a widely used technique for visualization. Nowadays, many of these visualizations are developed for the web, most commonly using JavaScript as the underlying programming language. So far, only  few DR methods have a JavaScript implementation though, necessitating developers to write wrappers around implementations in other languages. In addition, those DR methods that exist in JavaScript libraries, such as PCA, t-SNE and UMAP, do not offer consistent programming interfaces, hampering the quick integration of different methods. Toward a coherent and comprehensive DR programming framework, we developed an open source JavaScript library named DruidJS. Our library contains implementations of ten different DR algorithms, as well as the required linear algebra techniques, tools, and utilities.",
                        "time_start": "2020-10-28T16:30:00Z",
                        "time_end": "2020-10-28T16:40:00Z",
                        "uid": "s-short-1132"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Trrack: A Library for Provenance Tracking in Web-Based Visualizations",
                        "contributors": [
                            "Zachary Cutler"
                        ],
                        "abstract": "Provenance tracking is widely acknowledged as an important feature of visualization systems. By tracking provenance data, visualization designers can provide a wide variety of functionality, ranging from action recovery (undo/redo), reproducibility, collaboration and sharing, to logging in support of quantitative and longitudinal evaluation. However, there is currently no widely used library that can provide that functionality. As a consequence, visualization designers either develop ad-hoc solutions that are rarely comprehensive, or don't track provenance at all. In this paper, we introduce a web-based software library \u2013 Trrack \u2013 that is designed for easy integration in existing or future visualization systems. Trrack supports a wide range of use cases, from simple action recovery, to capturing intent and reasoning, and can be used to share states with collaborators and store provenance on a server. Trrack also includes an optional provenance visualization component that supports annotation of states and aggregation of events.",
                        "time_start": "2020-10-28T16:40:00Z",
                        "time_end": "2020-10-28T16:50:00Z",
                        "uid": "s-short-1156"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Just TYPEical: Visualizing Common Function Type Signatures in R",
                        "contributors": [
                            "Cameron Moy",
                            "Julia Belyakova"
                        ],
                        "abstract": "Data-driven approaches to programming language design are uncommon. Despite the availability of large code repositories, distilling semantically-rich information from programs remains difficult. Important dimensions, like run-time type data, are inscrutable without the appropriate tools. We contribute a task abstraction and interactive visualization, TYPEical, for programming language designers who are exploring and analyzing type information from execution traces. Our approach aids user understanding of function type signatures across many executions. Insights derived from our visualization are aimed at informing language design decisions \u2014 specifically of a new gradual type system being developed for the R programming language. A copy of this paper, along with all the supplemental material, is available at osf.io/mc6zt",
                        "time_start": "2020-10-28T16:50:00Z",
                        "time_end": "2020-10-28T17:00:00Z",
                        "uid": "s-short-1212"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Loch Prospector: Metadata Visualization for Lakes of Open Data",
                        "contributors": [
                            "Neha Makhija"
                        ],
                        "abstract": "Data lakes are an emerging storage paradigm that promotes data availability over integration.  A prime example are repositories of Open Data which show great promise for transparent data science. Due to the lack of proper integration, Data Lakes may not have a common consistent schema and traditional data management techniques fall short with these repositories. Much recent research has tried to address the new challenges associated with these data lakes. Researchers in this area are mainly interested in the structural properties of the data for developing new algorithms, yet typical Open Data portals offer limited functionality in that respect and instead focus on data semantics. \n\nWe propose Loch Prospector, a visualization to assist data management researchers in exploring and understanding the most crucial structural aspects of Open Data \u2014 in particular, metadata attributes \u2014 and the associated task abstraction for their work. Our visualization enables researchers to navigate the contents of data lakes effectively and easily accomplish what were previously laborious tasks. A copy of this paper with all supplemental material is available at osf.io/zkxv9",
                        "time_start": "2020-10-28T17:00:00Z",
                        "time_end": "2020-10-28T17:10:00Z",
                        "uid": "s-short-1224"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Encodable: Configurable Grammar for Visualization Components",
                        "contributors": [
                            "Krist Wongsuphasawat"
                        ],
                        "abstract": "There are so many libraries of visualization components nowadays with their APIs often different from one another. Could these components be more similar, both in terms of the APIs and common functionalities? For someone who is developing a new visualization component, how should the API look like? This work drew inspiration from visualization grammar, decoupled the grammar from its rendering engine and adapted it into a configurable grammar for individual components called Encodable. Encodable helps component authors define grammar for their components, and parse encoding specifications from users into utility functions for the implementation. This paper explains the grammar design and demonstrates how to build components with it.",
                        "time_start": "2020-10-28T17:10:00Z",
                        "time_end": "2020-10-28T17:20:00Z",
                        "uid": "s-short-1133"
                    }
                ]
            },
            {
                "title": "Interaction and Animation",
                "session_id": "s-papers-interact-animate",
                "chair": [
                    "Emily Wall"
                ],
                "organizers": [],
                "display_start": "2020-10-28T18:00:00Z",
                "time_start": "2020-10-28T18:00:00Z",
                "time_end": "2020-10-28T19:10:00Z",
                "discord_category": "vis short papers",
                "discord_channel": "interaction-and-animation",
                "discord_channel_id": "767561765004771329",
                "youtube_url": "https://youtu.be/L32stm1dpmk",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrf_a7iRwQ0h-ALci7Fc1Z1F",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Facilitating Exploration with Interaction Snapshots under High Latency",
                        "contributors": [
                            "Yifan Wu"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T18:00:00Z",
                        "time_end": "2020-10-28T18:10:00Z",
                        "uid": "s-short-1016"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Improving Engagement of Animated Visualization withVisual Foreshadowing",
                        "contributors": [
                            "Wenchao Li"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T18:10:00Z",
                        "time_end": "2020-10-28T18:20:00Z",
                        "uid": "s-short-1037"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Representing Real-Time Multi-User Collaboration in Visualizations",
                        "contributors": [
                            "Rupayan Neogy"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T18:20:00Z",
                        "time_end": "2020-10-28T18:30:00Z",
                        "uid": "s-short-1086"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Narrative Transitions in Data Videos",
                        "contributors": [
                            "Junxiu Tang"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T18:30:00Z",
                        "time_end": "2020-10-28T18:40:00Z",
                        "uid": "s-short-1198"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visualizing information on watch faces: A survey with smartwatch users",
                        "contributors": [
                            "Alaul Islam"
                        ],
                        "abstract": "People increasingly wear smartwatches that can track a wide variety of data.  However, it is currently unknown which data people consume and how it is visualized.  To better ground research on smartwatch visualization, it is important to understand the current use of these representation types on smartwatches, and to identify missed visualization opportunities.  We present the findings of a survey with 237 smartwatch wearers, and assess the types of data and representations commonly displayed on watch faces. We found a predominant display of health & fitness data, with icons accompanied by text being the most frequent representation type. Combining these results with a further analysis of online searches of watch faces and the data tracked on smartwatches that are not commonly visualized, we discuss opportunities for visualization research. Supplementary material is available at https://osf.io/nwy2r/.",
                        "time_start": "2020-10-28T18:40:00Z",
                        "time_end": "2020-10-28T18:50:00Z",
                        "uid": "s-short-1199"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Evaluating Animated Transitions between Contiguous Visualizations for Streaming Big Data",
                        "contributors": [
                            "Jo\u00e3o Moreira"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T18:50:00Z",
                        "time_end": "2020-10-28T19:00:00Z",
                        "uid": "s-short-1208"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Gaze-Driven Links for Magazine Style Narrative Visualizations",
                        "contributors": [
                            "Sebastien Lalle"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T19:00:00Z",
                        "time_end": "2020-10-28T19:10:00Z",
                        "uid": "s-short-1235"
                    }
                ]
            },
            {
                "title": "Theory, Cognition, and Sensemaking",
                "session_id": "s-papers-theory-cog-sense",
                "chair": [
                    "Cindy Xiong"
                ],
                "organizers": [],
                "display_start": "2020-10-29T14:00:00Z",
                "time_start": "2020-10-29T14:00:00Z",
                "time_end": "2020-10-29T15:30:00Z",
                "discord_category": "vis short papers",
                "discord_channel": "theory-cognition-and-sensemaking",
                "discord_channel_id": "768679015111196682",
                "youtube_url": "https://youtu.be/yrvRz_rT4JI",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrcGDhMCxza2F3a8S1M_JzXH",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Characterizing Automated Data Insights",
                        "contributors": [
                            "Terrance Law"
                        ],
                        "abstract": "Many researchers have explored tools that aim to recommend data insights to users. These tools automatically communicate a rich diversity of data insights and offer such insights for many different purposes. However, there is a lack of structured understanding concerning what researchers of these tools mean by \u201cinsight\u201d and what tasks in the analysis workflow these tools aim to support. We conducted a systematic review of existing systems that seek to recommend data insights. Grounded in the review, we propose 12 types of automated insights and four purposes of automating insights. We further discuss the design opportunities emerged from our analysis.",
                        "time_start": "2020-10-29T14:00:00Z",
                        "time_end": "2020-10-29T14:10:00Z",
                        "uid": "s-short-1009"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Design Judgment in Data Visualization Practice",
                        "contributors": [
                            "Paul Parsons"
                        ],
                        "abstract": "Data visualization is becoming an increasingly popular field of design practice. Although many studies have highlighted the knowledge required for effective data visualization design, their focus has largely been on formal knowledge and logical decision-making processes that can be abstracted and codified. Less attention has been paid to the more situated and personal ways of knowing that are prevalent in all design activity. In this study, we conducted semi-structured interviews with data visualization practitioners during which they were asked to describe the practical and situated aspects of their design processes. Using a philosophical framework of design judgment from Nelson and Stolterman [22], we analyzed the transcripts to describe the volume and complex layering of design judgments that are used by data visualization practitioners as they describe and interrogate their work. We identify aspects of data visualization practice that require further investigation beyond notions of rational, model- or principle-directed decision-making processes.",
                        "time_start": "2020-10-29T14:10:00Z",
                        "time_end": "2020-10-29T14:20:00Z",
                        "uid": "s-short-1024"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "What are Data Insights to Professional Visualization Users?",
                        "contributors": [
                            "Terrance Law"
                        ],
                        "abstract": "While many visualization researchers have attempted to define data insights, little is known about how visualization users perceive them. We interviewed 23 professional users of end-user visualization platforms (e.g., Tableau and Power BI) about their experiences with data insights. We report on seven characteristics of data insights based on interviewees' descriptions. Grounded in these characteristics, we propose practical implications for creating tools that aim to automatically communicate data insights to users.",
                        "time_start": "2020-10-29T14:20:00Z",
                        "time_end": "2020-10-29T14:30:00Z",
                        "uid": "s-short-1116"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Didactic Methodology for Crafting Information Visualizations",
                        "contributors": [
                            "Mandy Keck"
                        ],
                        "abstract": "Finding or creating the right information visualization solution that meets design goals can be a very challenging task, not only for students but also for visualization experts. \nIn this paper, we introduce a didactic methodology for designing interactive visualizations through hands-on activities. Our approach can assist students and anyone interested in crafting, ranking, and improving new visualization solutions. The suggested approach follows divergent and convergent thinking that motivates designing several low-fidelity prototypes, discussing pros and cons of each in groups, and improving the final solution by incorporating visual and perceptual principles. The methodology is described with teaching course examples for undergraduate and graduate students. We also list observations made when applying the methodology online and offline along with gathered student feedback.",
                        "time_start": "2020-10-29T14:30:00Z",
                        "time_end": "2020-10-29T14:40:00Z",
                        "uid": "s-short-1131"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "CrowdTrace: Visualizing Provenance in Distributed Sensemaking",
                        "contributors": [
                            "Tianyi Li"
                        ],
                        "abstract": "Capturing analytic provenance is important for refining sensemaking analysis. However, understanding this provenance can be difficult. First, making sense of the reasoning in intermediate steps is time-consuming. Especially in distributed sensemaking, the provenance is less cohesive because each analyst only sees a small portion of the data without an understanding of the overall collaboration workflow. Second, analysis errors from one step can propagate to later steps. Furthermore, in exploratory sensemaking, it is difficult to define what an error is since there are no correct answers to reference. In this paper, we explore provenance analysis for distributed sensemaking in the context of crowdsourcing, where distributed analysis contributions are captured in microtasks. We propose crowd auditing as a way to help individual analysts visualize and trace provenance to debug distributed sensemaking. To evaluate this concept, we implemented a crowd auditing tool, CrowdTrace. Our user study-based evaluation demonstrates that CrowdTrace offers an effective mechanism to audit and refine multi-step crowd sensemaking.",
                        "time_start": "2020-10-29T14:40:00Z",
                        "time_end": "2020-10-29T14:50:00Z",
                        "uid": "s-short-1138"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Let's Gamble: How a Poor Visualization Can Elicit Risky Behavior",
                        "contributors": [
                            "Melanie Bancilhon"
                        ],
                        "abstract": "Data visualizations are standard tools for assessing and communicating risks. However, it is not always clear which designs are optimal or how encoding choices might influence risk perception and decision-making. In this paper, we report the findings of a large-scale gambling game that immersed participants in an environment where their actions impacted their bonuses. Participants chose to either enter a lottery or receive guaranteed monetary gains based on five visualization designs. By measuring risk perception and observing decision-making, we present suggestive evidence that people were more likely to gamble when presented area proportioned triangle and circle designs. Using our results, we model risk perception and discuss how our findings can improve visualization selection.",
                        "time_start": "2020-10-29T14:50:00Z",
                        "time_end": "2020-10-29T15:00:00Z",
                        "uid": "s-short-1189"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Exploring How Personality Models Information Visualization Preferences",
                        "contributors": [
                            "Tom\u00e1s Alves"
                        ],
                        "abstract": "Recent research on information visualization has shown how individual differences act as a mediator on how users interact with visualization systems. We focus our exploratory study on whether personality has an effect on user preferences regarding idioms used for hierarchy, evolution over time, and comparison contexts. Specifically, we leverage all personality variables from the Five-Factor Model and the three dimensions from Locus of Control (LoC) with correlation and clustering approaches. The correlation-based method suggested that Neuroticism, Openness to Experience, Agreeableness, several facets from each trait, and the External dimensions from LoC mediate how much individuals prefer certain idioms. In addition, our results from the cluster-based analysis showed that Neuroticism, Extraversion, Conscientiousness, and all dimensions from LoC have an effect on preferences for idioms in hierarchy and evolution contexts. Our results support the incorporation of in-depth personality synergies with InfoVis into the design pipeline of visualization systems.",
                        "time_start": "2020-10-29T15:00:00Z",
                        "time_end": "2020-10-29T15:10:00Z",
                        "uid": "s-short-1204"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Why Shouldn't All Charts Be Scatter Plots? Beyond Precision-Driven Visualizations",
                        "contributors": [
                            "Enrico Bertini"
                        ],
                        "abstract": "A central tenet of information visualization research and practice is the notion of visual variable effectiveness, or the perceptual precision at which values are decoded given visual channels of encoding. Formative work from Cleveland \\& McGill has shown  that position along a common axis is the most effective visual variable for comparing individual values. One natural conclusion is that any chart that is not a dot plot or scatterplot is deficient and should be avoided. In this paper we refute a caricature of this \"scatterplots only\" argument as a way to call for new perspectives on how information visualization is researched, taught, and evaluated.",
                        "time_start": "2020-10-29T15:10:00Z",
                        "time_end": "2020-10-29T15:20:00Z",
                        "uid": "s-short-1229"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Data Visualization Practitioners' Perspectives on Chartjunk",
                        "contributors": [
                            "Paul Parsons"
                        ],
                        "abstract": "Chartjunk is a popular yet contentious topic. In recent years it has seen increasing research interest, with studies investigating phenomena such as memorability, engagement, preference, and performance. These studies have shown that extreme minimalism is not always best, and that visual embellishments can be useful depending on the context. While more knowledge is being developed regarding the effects of embellishments on users, less attention has been given to the perspectives of practitioners regarding how they design with embellishments. We conducted semi-structured interviews with 20 data visualization practitioners, investigating how they understand chartjunk and the factors that influence how and when they make use of embellishments. Our analysis uncovers a broad and pluralistic understanding of chartjunk among practitioners, and foregrounds a variety of personal and situated factors that influence the use of chartjunk beyond mere context. We highlight the personal nature of design practice, and discuss the need for more practice-led visualization research to better understand the ways in which concepts like chartjunk are interpreted and used by practitioners.",
                        "time_start": "2020-10-29T15:20:00Z",
                        "time_end": "2020-10-29T15:30:00Z",
                        "uid": "s-short-1231"
                    }
                ]
            },
            {
                "title": "Text and Communication",
                "session_id": "s-papers-text-comm",
                "chair": [
                    "Mennatallah El-Assady"
                ],
                "organizers": [],
                "display_start": "2020-10-29T16:00:00Z",
                "time_start": "2020-10-29T16:00:00Z",
                "time_end": "2020-10-29T17:10:00Z",
                "discord_category": "vis short papers",
                "discord_channel": "text-and-communication",
                "discord_channel_id": "768679069025566720",
                "youtube_url": "https://youtu.be/7QHpbEd_z7I",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrfVsHJJeATMyxO5UVIOkOiF",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Sentifiers: Interpreting Vague Intent Modifiers in Visual Analysis using Word Co-occurrence and Sentiment Analysis",
                        "contributors": [
                            "Vidya Setlur"
                        ],
                        "abstract": "Natural language interaction with data visualization tools often involves the use of vague subjective modifiers in utterances such as \u201cshow me the sectors that are performing\u201d and \u201cwhere is a good neighborhood to buy a house?.\u201d Interpreting these modifiers is often difficult for these tools because their meanings lack clear semantics and are in part defined by context and personal user preferences. This paper presents a system called Sentifiers that makes a first step in better understanding these vague predicates. The algorithm employs word co-occurrence and sentiment analysis to determine which data attributes and filters ranges to associate with the vague predicates. The provenance results from the algorithm are exposed to the user as interactive text that can be repaired and refined. We conduct a qualitative evaluation of the Sentifiers that indicates the usefulness of the interface as well as opportunities for better supporting subjective utterances in visual analysis tasks through natural language.",
                        "time_start": "2020-10-29T16:00:00Z",
                        "time_end": "2020-10-29T16:10:00Z",
                        "uid": "s-short-1003"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Data Visualization for Transgender Voice Training",
                        "contributors": [
                            "Alex A. Ahmed"
                        ],
                        "abstract": "Social and political factors structure how data are collected, analyzed, and visually presented. Visualization researchers are increasingly discussing these issues, and calling for feminist approaches that allow not only for the interrogation of unspoken assumptions and power imbalances in how visualizations are designed and deployed, but also for the creation of new systems by and for marginalized groups. This paper describes the motivation, ideation, and prototyping of a novel visualization tool to support voice training for transgender people. We centered our design around user needs and lived experiences, in order to support self-determination of gender expression. In so doing, we make novel inroads into how to visualize speech data, and open possibilities for future work in the design of visualizations that prioritize user agency within social and political contexts.",
                        "time_start": "2020-10-29T16:10:00Z",
                        "time_end": "2020-10-29T16:20:00Z",
                        "uid": "s-short-1020"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "GlassViz: Visualizing Automatically-Extracted Entry Points for Exploring Scientific Corpora in Problem-Driven Visualization Research",
                        "contributors": [
                            "Alejandro Benito-Samtps"
                        ],
                        "abstract": "In this paper, we report the development of a model and a proof-of-concept visual text analytics (VTA) tool to enhance document discovery in a problem-driven visualization research (PDVR) context. The proposed model captures the cognitive model followed by\ndomain and visualization experts by analyzing the interdisciplinary communication channel as represented by keywords found in two disjoint collections of research papers. High distributional inter-collection similarities are employed to build informative keyword associations that serve as entry points to drive the exploration of a large document corpus. Our approach is demonstrated in the context of research on visualization for the digital humanities.",
                        "time_start": "2020-10-29T16:20:00Z",
                        "time_end": "2020-10-29T16:30:00Z",
                        "uid": "s-short-1041"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Knowing what to look for: A Fact-Evidence Reasoning Framework for Decoding Communicative Visualization",
                        "contributors": [
                            "Sahaj Vaidya",
                            "Aritra Dasgupta"
                        ],
                        "abstract": "Despite the widespread use of communicative charts as a medium for scientific communication, we lack a systematic understanding of how well the charts fulfill the goals of effective visual communication. Existing research mostly focuses on the means, i.e. the encoding principles, and not the end, i.e. the key takeaway of a chart. To address this gap, we start from the first principles and aim to answer the fundamental question: how can we describe the message of a scientific chart? We contribute a fact-evidence reasoning framework (FaEvR) by augmenting the conventional visualization pipeline with the stages of gathering and associating evidence for decoding the facts presented in a chart. We apply the resulting classification scheme of fact and evidence on a collection of 500 charts collected from publications in multiple science domains. We demonstrate the practical applications of FaEvR in calibrating task complexity and detecting barriers towards chart interpretability.",
                        "time_start": "2020-10-29T16:30:00Z",
                        "time_end": "2020-10-29T16:40:00Z",
                        "uid": "s-short-1070"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "MeetCues: Supporting Online Meetings Experience",
                        "contributors": [
                            "Marios Constantinides"
                        ],
                        "abstract": "The remote work ecosystem is transforming patterns of communication between teams and individuals located at distance. Particularly, the absence of certain subtle cues in current communication tools may hinder an online's meeting outcome by negatively impacting attendees' overall experience and, often, make them feeling disconnected. The problem here might be due to the fact that current tools fall short in capturing it. To partly address this, we developed an online platform-MeetCues-with the aim of supporting online communication during meetings. MeetCues is a companion platform for a commercial communication tool with interactive and visual UI features that support back-channels of communications. It allows attendees to be more engaged during a meeting, and reflect in real-time or post-meeting. We evaluated our platform in a diverse set of five, real-world corporate meetings, and we found that, not only people were more engaged and aware during their meetings, but they also felt more connected. These findings suggest promise in the design of new communications tools, and reinforce the role of InfoVis in augmenting and enriching online meetings.",
                        "time_start": "2020-10-29T16:40:00Z",
                        "time_end": "2020-10-29T16:50:00Z",
                        "uid": "s-short-1165"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "DebateVis: Visualizing Political Debates for Non-Expert Users",
                        "contributors": [
                            "Laura South"
                        ],
                        "abstract": "Political debates provide an important opportunity for voters to observe candidate behavior, learn about issues, and make voting decisions. However, debates are generally broadcast late at night and last more than ninety minutes, so watching debates live can be inconvenient, if not impossible, for many potential viewers. Even voters who do watch debates may find themselves overwhelmed by a deluge of information in a substantive, issue-driven debate. Media outlets produce short summaries of debates, but these are not always effective as a method of deeply comprehending the policies candidates propose or the debate techniques they employ. In this paper we contribute reflections and results of an 18-month design study through an interdisciplinary collaboration with journalism and political science researchers. We characterize task and data abstractions for visualizing political debate transcripts for the casual user, and present a novel tool (DebateVis) to help non-expert users explore and analyze debate transcripts.",
                        "time_start": "2020-10-29T16:50:00Z",
                        "time_end": "2020-10-29T17:00:00Z",
                        "uid": "s-short-1169"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "ClaimViz: Visual Analytics for Identifying and Verifying Factual Claims",
                        "contributors": [
                            "Md Main Uddin Rony"
                        ],
                        "abstract": "Verifying a factual claim made by public figures, aka fact-checking, is a common task of the journalists in the newsrooms. One critical challenge that fact-checkers face is- they have to swift through a large amount of text to find claims that are check-worthy. While\nthere exist some computational methods for automating the factchecking process, little research has been done on how a system should combine such techniques with visualizations to assist factcheckers. ClaimViz is a visual analytic system that integrates natural language processing and machine learning methods with interactive\nvisualizations to facilitate the fact-checking process. The design of ClaimViz is based on analyzing the requirements of real factcheckers and our case studies demonstrate how the system can help users to effectively spot and verify claims.",
                        "time_start": "2020-10-29T17:00:00Z",
                        "time_end": "2020-10-29T17:10:00Z",
                        "uid": "s-short-1181"
                    }
                ]
            },
            {
                "title": "Visualizing Machine Learning",
                "session_id": "s-papers-vis-ml",
                "chair": [
                    "Josua Krause"
                ],
                "organizers": [],
                "display_start": "2020-10-30T14:00:00Z",
                "time_start": "2020-10-30T14:00:00Z",
                "time_end": "2020-10-30T15:30:00Z",
                "discord_category": "vis short papers",
                "discord_channel": "visualizing-machine-learning",
                "discord_channel_id": "767141494657777674",
                "youtube_url": "https://youtu.be/1GJWd-xXfAk",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojreO3ZPdUAzC1iK46Zv2Rv3S",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Visually Analyzing and Steering Zero Shot Learning",
                        "contributors": [
                            "Saroj Sahoo"
                        ],
                        "abstract": "We propose a visual analytics system to help a user analyze and steer zero-shot learning models. Zero-shot learning has emerged as a viable scenario for categorizing data that consists of no labeled examples, and thus a promising approach to minimize data annotation from humans. However, it is challenging to understand where zero-shot learning fails, the cause of such failures, and how a user can modify the model to prevent such failures. Our visualization system is designed to help users diagnose and understand mispredictions in such models, so that they may gain insight on the behavior of a model when applied to data associated with categories not seen during training. Through usage scenarios, we highlight how our system can help a user improve performance in zero-shot learning.",
                        "time_start": "2020-10-30T14:00:00Z",
                        "time_end": "2020-10-30T14:10:00Z",
                        "uid": "s-short-1106"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "DRIL: Descriptive Rules by Interactive Learning",
                        "contributors": [
                            "Eli Brown"
                        ],
                        "abstract": "Analyzing data is increasingly a part of jobs across industry, science and government, but data stakeholders are not necessarily experts in analytics. The human-in-the-loop (HIL) approach includes semantic interaction tools, which leverage machine learning behind the scenes to assist users with their tasks without engaging them directly with algorithms.  One widely applicable model for how humans understand data is descriptive rules, which can characterize important attributes and simultaneously their crucial values or ranges. In this paper, we introduce an approach to help with data understanding via interactively and automatically generated rules. Our approach makes discerning the behavior of groups of interesting data efficient and simple by bridging the gap between machine learning methods for rule learning and the user experience of sensemaking through visual exploration. We have evaluated our approach with machine learning experiments to confirm an existing rule learning algorithm performs well in this interactive context even with a small amount of user input, and created a prototype system, DRIL (Descriptive Rules by Interactive Learning), to demonstrate its capability through a case study.",
                        "time_start": "2020-10-30T14:10:00Z",
                        "time_end": "2020-10-30T14:20:00Z",
                        "uid": "s-short-1147"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Vortex Boundary Identification using Convolutional Neural Network",
                        "contributors": [
                            "Marzieh Berenjkoub"
                        ],
                        "abstract": "Feature extraction is an integral component of scientific visualization, and specifically in situations in which features are difficult to formalize, deep learning has great potential to aid in data analysis. In this paper, we develop a deep neural network that is capable of finding vortex boundaries. \nFor training data generation, we employ a parametric flow model that generates thousands of vector field patches with known ground truth. \nCompared to previous methods, our approach does not require the manual setting of a threshold in order to generate the training data or to extract the vortices.\nAfter supervised learning, we apply the method to numerical fluid flow simulations, demonstrating its applicability in practice. Our results show that the vortices extracted using the proposed method can capture more accurate behavior of the vortices in the flow.",
                        "time_start": "2020-10-30T14:20:00Z",
                        "time_end": "2020-10-30T14:30:00Z",
                        "uid": "s-short-1152"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "How Does Visualization Help People Learn Deep Learning? Evaluating GAN Lab with Observational Study and Log Analysis",
                        "contributors": [
                            "Minsuk Kahng"
                        ],
                        "abstract": "While a rapidly growing number of people want to learn artificial intelligence (AI) and deep learning, the increasing complexity of such models poses significant learning barriers. Recently, interactive visualizations, such as TensorFlow Playground and GAN Lab, have demonstrated success in lowering these barriers. However, there has been little work in evaluating these tools with human subjects. This paper presents two studies on evaluating GAN Lab, an interactive tool designed to help people learn how Generated Adversarial Networks (GANs) work. First, through an observational study, we investigate how the tool is used and what users learn from their usage. Second, we conduct a log analysis of the deployed tool to investigate how its visitors engage with GAN Lab. Based on the studies and our experience in developing and successfully deploying the tool, we provide design considerations and discuss further evaluation challenges for interactive educational tools for deep learning.",
                        "time_start": "2020-10-30T14:30:00Z",
                        "time_end": "2020-10-30T14:40:00Z",
                        "uid": "s-short-1167"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks",
                        "contributors": [
                            "Haekyu Park"
                        ],
                        "abstract": "Deep neural networks (DNNs) are now commonly used in many domains. However, they  are  vulnerable  to adversarial attacks: carefully-crafted perturbations on data inputs that can fool a model into making incorrect predictions. Despite significant research on developing DNN attack and defense techniques, people still lack an understanding of how such attacks penetrate a model's internals. We present Bluff, an interactive system for visualizing, characterizing, and deciphering adversarial attacks on vision-based neural networks. Bluff allows people to flexibly visualize and compare the activation pathways for benign and attacked images, revealing mechanisms that adversarial attacks employ to inflict harm on a model. Bluff is open-sourced and runs in modern web browsers.",
                        "time_start": "2020-10-30T14:40:00Z",
                        "time_end": "2020-10-30T14:50:00Z",
                        "uid": "s-short-1200"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visually Analyzing Contextualized Embeddings",
                        "contributors": [
                            "Matthew Berger"
                        ],
                        "abstract": "In this paper we introduce a method for visually analyzing contextualized embeddings produced by deep neural network-based language models. Our approach is inspired by linguistic probes for natural language processing, where tasks are designed to probe language models for linguistic structure, such as parts-of-speech and named entities. These approaches are largely confirmatory, however, only enabling a user to test for information known a priori. In this work, we eschew supervised probing tasks, and advocate for unsupervised probes, coupled with visual exploration techniques, to assess what is learned by language models. Specifically, we cluster contextualized embeddings produced from a large text corpus, and introduce a visualization design based on this clustering and textual structure - cluster co-occurrences, cluster spans, and cluster-word membership - to help elicit the functionality of, and relationship between, individual clusters. User feedback highlights the benefits of our design in discovering different types of linguistic structures.",
                        "time_start": "2020-10-30T14:50:00Z",
                        "time_end": "2020-10-30T15:00:00Z",
                        "uid": "s-short-1205"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Explainable Spatial Clustering: Leveraging Spatial Data in Radiation Oncology",
                        "contributors": [
                            "Andrew Wentzel"
                        ],
                        "abstract": "Advances in data collection in radiation therapy have led to an abundance of opportunities for applying data mining and machine learning techniques to promote new data-driven insights. In light of these advances, supporting collaboration between machine learning experts and clinicians is important for facilitating better development and adoption of these models. Although many medical use-cases rely on spatial data, where understanding and visualizing the underlying structure of the data is important, little is known about the interpretability of spatial clustering results by clinical audiences. In this work, we reflect on the design of visualizations for explaining novel approaches to clustering complex anatomical data from head and neck cancer patients. These visualizations were developed, through participatory design, for clinical audiences during a multi-year collaboration with radiation oncologists and statisticians. We distill this collaboration into a set of lessons learned for creating visual and explainable spatial clustering for clinical users.",
                        "time_start": "2020-10-30T15:00:00Z",
                        "time_end": "2020-10-30T15:10:00Z",
                        "uid": "s-short-1207"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "ProtoViewer: Visual Interpretation and Diagnostics of DNNs via Prototype Factorization",
                        "contributors": [
                            "Junhan Zhao"
                        ],
                        "abstract": "In recent years deep neural networks (DNNs) are increasingly used in a variety of application domains for their state-of-the-art performance in many challenging machine learning tasks. However their lack of interpretability could cause trustability and fairness issues and also makes model diagnostics a difficult task. In this paper we present a novel visual analytics framework to interpret and diagnose DNNs. Our approach utilizes ProtoFac to factorize the latent representations in DNNs into weighted combinations of prototypes, which are exemplar cases (e.g.representative image patches) from the original data. The visual interface uses the factorized prototypes to summarize and explain the model behaviour as well as support comparisons across subsets of data such that the users can form a hypothesis about the model's failure on certain subsets. The method is model-agnostic and provides global explanation of the model behaviour. Furthermore, the system selects prototypes and weights that faithfully represents the model under analysis by mimicking its latent representation and predictions. Example usage scenarios on two DNN architectures and two datasets illustrates the effectiveness and general applicability of the proposed approach.",
                        "time_start": "2020-10-30T15:10:00Z",
                        "time_end": "2020-10-30T15:20:00Z",
                        "uid": "s-short-1226"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "InstanceFlow: Visualizing the Evolution of Classifier Confusion on the Instance Level",
                        "contributors": [
                            "Michael P\u00fchringer",
                            "Michael P\u00fchringer"
                        ],
                        "abstract": "Classification is one of the most important supervised machine learning tasks. During the training of a classification model, the training instances are fed to the model multiple times (during multiple epochs) in order to iteratively increase the classification performance. The increasing complexity of models has led to a growing demand for model interpretability through visualizations. Existing approaches mostly focus on the visual analysis of the final model performance after training and are often limited to aggregate performance measures. In this paper we introduce InstanceFlow, a novel dual-view visualization tool that allows users to analyze the learning behavior of classifiers over time on the instance-level. A Sankey diagram visualizes the flow of instances throughout epochs, with on-demand detailed glyphs and traces for individual instances. A tabular view allows users to locate interesting instances by ranking and filtering. In this way, InstanceFlow bridges the gap between classlevel and instance-level performance evaluation while enabling users to perform a full temporal analysis of the training process.",
                        "time_start": "2020-10-30T15:20:00Z",
                        "time_end": "2020-10-30T15:30:00Z",
                        "uid": "s-short-1227"
                    }
                ]
            }
        ]
    },
    "p-academia": {
        "event": "Why Should I Stay in Academia? Bridging Generations of Researchers in Visualization",
        "long_name": "Why Should I Stay in Academia? Bridging Generations of Researchers in Visualization",
        "event_type": "Panel",
        "event_description": "The question \"(Why) should I stay in academia\" emerges often in the minds of researchers and in discussions in our community. In general, junior researchers might be more affected, but this is a choice that all levels of academic seniority face. Our panel targets an open discussion on the active choice of \u201cstaying or leaving\u201d and its reasons, on the positives and negatives of both possible paths (academia vs. industry), and on how we, as a community, can foster a healthier environment to support and equip our members---whether they decide to stay in academia, or not. Our choice of panelists (Silvia Miksch, Johanna Beyer, Chuck Hansen, Jan Reininghaus, Bei Wang and Jack van Wijk) pays attention to the inclusion of researchers from different academic ages and with different paths, to reflect different career and life challenges. We have ensured gender balance among our panelists, as female researchers often face different (and additional) challenges, and we have included a panelist from industry, to represent both sides of the coin. We anticipate that our panel will engage a large group of researchers at different career stages in a lively discussion to shape our community.",
        "event_url": "http://www.renataraidou.com/panel-ieee-vis-2020",
        "sessions": [
            {
                "title": "Why Should I Stay in Academia? Bridging Generations of Researchers in Visualization",
                "session_id": "p-academia",
                "chair": [
                    "Ingrid Hotz",
                    "Renata Raidou"
                ],
                "organizers": [
                    "Renata Raidou",
                    "Ingrid Hotz",
                    "Josh Levine",
                    "Dan Archambault",
                    "Micha Behrisch"
                ],
                "display_start": "2020-10-27T18:00:00Z",
                "time_start": "2020-10-27T18:00:00Z",
                "time_end": "2020-10-27T19:30:00Z",
                "discord_category": "why should i stay in academia bridging generations of researchers in visualization",
                "discord_channel": "general",
                "discord_channel_id": "767121147137228870",
                "youtube_url": "https://youtu.be/ddK9GlcqC4g",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "https://youtu.be/4Ci7KoLzFZ4",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Panel",
                        "title": "Why Should I Stay in Academia? Bridging Generations of Researchers in Visualization",
                        "contributors": [
                            "Silvia Miksch",
                            "Chuck Hansen",
                            "Jack van Wijk",
                            "Bei Wang",
                            "Jan Reininghaus",
                            "Johanna Beyer "
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T18:00:00Z",
                        "time_end": "2020-10-27T19:30:00Z"
                    }
                ]
            }
        ]
    },
    "l-cosmo": {
        "event": "Opportunities and Challenges in Cosmology Visualization",
        "long_name": "Opportunities and Challenges in Cosmology Visualization",
        "event_type": "Application Spotlight",
        "event_description": "Cosmologists use massive computer simulations to verify that models of the formation of the universe match observational data from telescopes. The massive data from these simulations create a number of opportunities for visualization, such as: 1) The Epoch of Reionization, which starts when stars first appeared after the big bang and radiation from these stars started ionizing hydrogen. This problem is similar to volume rendering but different enough that current volume rendering APIs cannot be used directly. How do we change graphics APIs to solve the reionization problem? 2) The data generated by these simulations is so large that only a few timesteps can be saved by the simulations. In post-hoc visualization, a significant amount of data wrangling is required to convert data from simulation format to something that visualization tools understand, requiring several iterations until the results are useful to the science team. Creating customizable workflows to achieve automatic visualization is an interesting research/application problem. In addition, how do we create pipelines for insitu visualization given the restrictions on memory and resources of simulation codes? 3) Validation is important in the field of cosmology, and some cosmologists compare Lagrangian cosmology simulations to Eulerian ones. How do we compare visualizations from these two different topologies? Can we assume that a point cloud volume rendering is equivalent to a grid based volume rendering?",
        "event_url": "https://sites.google.com/view/viscomsospotlight/",
        "sessions": [
            {
                "title": "Opportunities and Challenges in Cosmology Visualization",
                "session_id": "l-cosmo",
                "chair": [
                    "Jim Ahrens",
                    "Jesus Pulido",
                    "Pascal Grosset",
                    "Silvio Rizzi"
                ],
                "organizers": [
                    "Jesus Pulido",
                    "Pascal Grosset",
                    "Silvio Rizzi",
                    "Jim Ahrens"
                ],
                "display_start": "2020-10-27T18:00:00Z",
                "time_start": "2020-10-27T18:00:00Z",
                "time_end": "2020-10-27T19:42:00Z",
                "discord_category": "opportunities and challenges in cosmology visualization",
                "discord_channel": "general",
                "discord_channel_id": "767121150530027554",
                "youtube_url": "https://youtu.be/f31c1mXjbjY",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Intro",
                        "title": "Welcome",
                        "contributors": [
                            "Pascal Grosset"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T18:00:00Z",
                        "time_end": "2020-10-27T18:01:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Introduction to Cosmology",
                        "contributors": [
                            "Katrin Heitmann"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T18:01:00Z",
                        "time_end": "2020-10-27T18:19:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Re-ionization of the Universe",
                        "contributors": [
                            "Hannah Ross"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T18:20:00Z",
                        "time_end": "2020-10-27T18:32:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Intel Tools for Cosmology Visualization",
                        "contributors": [
                            "Aaron Knoll"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T18:32:00Z",
                        "time_end": "2020-10-27T18:43:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "NVidia Tools for Cosmology Visualization",
                        "contributors": [
                            "Peter Messmer"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T18:43:00Z",
                        "time_end": "2020-10-27T18:56:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Comparison of Eulerian and Lagrangian code",
                        "contributors": [
                            "Jesus Pulido"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T19:00:00Z",
                        "time_end": "2020-10-27T19:14:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Exascale Visualization, Techniques and Challenges",
                        "contributors": [
                            "Joseph Insley"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T19:14:00Z",
                        "time_end": "2020-10-27T19:24:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "OpenSpace",
                        "contributors": [
                            "Anders Ynnerman"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T19:25:00Z",
                        "time_end": "2020-10-27T19:42:00Z"
                    }
                ]
            }
        ]
    },
    "x-memorial": {
        "event": "VIS Memorials",
        "long_name": "VIS Memorials",
        "event_type": "Memorial",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Memorial Session",
                "session_id": "x-memorial",
                "chair": [
                    "Chris R. Johnson"
                ],
                "organizers": [
                    "Chris R. Johnson"
                ],
                "display_start": "2020-10-27T20:00:00Z",
                "time_start": "2020-10-27T20:00:00Z",
                "time_end": "2020-10-27T20:53:00Z",
                "discord_category": "vis memorials",
                "discord_channel": "memorial-session",
                "discord_channel_id": "767122255410757652",
                "youtube_url": "https://youtu.be/rsCKHe1v_So",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Introduction",
                        "contributors": [
                            "Chris R. Johnson"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T20:00:00Z",
                        "time_end": "2020-10-27T20:05:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Alfred Inselberg",
                        "contributors": [
                            "All"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T20:05:00Z",
                        "time_end": "2020-10-27T20:22:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Bill Lorensen",
                        "contributors": [
                            "All"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T20:22:00Z",
                        "time_end": "2020-10-27T20:38:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Lucy Nowell",
                        "contributors": [
                            "All"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T20:38:00Z",
                        "time_end": "2020-10-27T20:53:00Z"
                    }
                ]
            }
        ]
    },
    "a-visap": {
        "event": "VIS Arts Program",
        "long_name": "VIS Arts Program",
        "event_type": "VIS Arts Program",
        "event_description": "",
        "event_url": "https://visap.net/",
        "sessions": [
            {
                "title": "VISAP Opening Event",
                "session_id": "a-visap-opening",
                "chair": [
                    "Yoon Chung Han",
                    "Carmen Hull",
                    "Maria Lantin",
                    "Erik Brunvand"
                ],
                "organizers": [
                    "Erik Brunvand",
                    "Yoon Chung Han",
                    "Carmen Hull",
                    "Maria Lantin"
                ],
                "display_start": "2020-10-27T20:00:00Z",
                "time_start": "2020-10-27T20:00:00Z",
                "time_end": "2020-10-27T21:15:00Z",
                "discord_category": "vis arts program",
                "discord_channel": "visap-opening-event",
                "discord_channel_id": "767122262703734834",
                "youtube_url": "https://youtu.be/9-PVxbMBJ8Q",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Introduction",
                        "contributors": [
                            "Erik Brunvand",
                            "Yoon Chung Han",
                            "Carmen Hull",
                            "Maria Lantin"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T20:00:00Z",
                        "time_end": "2020-10-27T20:10:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "VISAP Fast Forward",
                        "contributors": [
                            "VISAP Artists"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T20:10:00Z",
                        "time_end": "2020-10-27T20:20:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Electo Electro 2020",
                        "contributors": [
                            "Mike Richison"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T20:20:00Z",
                        "time_end": "2020-10-27T20:28:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visum: Beyond the Physical Eye",
                        "contributors": [
                            "Lena Mathew"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T20:28:00Z",
                        "time_end": "2020-10-27T20:36:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Cangjie",
                        "contributors": [
                            "Weidi Zhang",
                            "Donghao Ren"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T20:36:00Z",
                        "time_end": "2020-10-27T20:45:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Q&A with Artists",
                        "contributors": [
                            "Mike Richison",
                            "Lena Mathew",
                            "Weidi Zhang"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T20:45:00Z",
                        "time_end": "2020-10-27T21:15:00Z"
                    }
                ]
            },
            {
                "title": "VISAP Session 1",
                "session_id": "a-visap-1",
                "chair": [
                    "Yoon Chung Han"
                ],
                "organizers": [
                    "Erik Brunvand",
                    "Yoon Chung Han",
                    "Carmen Hull",
                    "Maria Lantin"
                ],
                "display_start": "2020-10-28T14:00:00Z",
                "time_start": "2020-10-28T14:00:00Z",
                "time_end": "2020-10-28T15:25:00Z",
                "discord_category": "vis arts program",
                "discord_channel": "visap-session-1",
                "discord_channel_id": "767561686865412176",
                "youtube_url": "https://youtu.be/fKgBw987jTc",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrfNKPi1LOafSyNyBMTyE9Ya",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Keynote: Representation Matters: Mapping Gender and Sexuality on Twitter",
                        "contributors": [
                            "Sarah Sinwell"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T14:00:00Z",
                        "time_end": "2020-10-28T14:15:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "HeartBees: Visualizing Crowd Affects",
                        "contributors": [
                            "Chao Ying Qin",
                            "Marios Constantinides",
                            "Luca Maria Aiello",
                            "Daniele Quercia"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T14:15:00Z",
                        "time_end": "2020-10-28T14:30:00Z",
                        "uid": "a-arts-1022"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Tsuga Convictio: Visualizing for the ecological, feminine, and embodied",
                        "contributors": [
                            "Cathryn A Ploehn",
                            "Molly Wright",
                            "Daragh Byrne"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T14:30:00Z",
                        "time_end": "2020-10-28T14:45:00Z",
                        "uid": "a-arts-1043"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Q&A for Papers and Keynote",
                        "contributors": [
                            "Sarah Sinwell",
                            "Chao Ying Qin",
                            "Marios Constantinides",
                            "Luca Maria Aiello",
                            "Daniele Quercia",
                            "Cathryn A Ploehn",
                            "Molly Wright",
                            "Daragh Byrne"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T14:45:00Z",
                        "time_end": "2020-10-28T15:00:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Invited Artist Talk: Hannah Wolf",
                        "contributors": [
                            "Hannah E. Wolfe",
                            "\u015e\u00f6len K\u0131ratl\u0131",
                            "Alex Bundy"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T15:00:00Z",
                        "time_end": "2020-10-28T15:05:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Invited Artist Talk: Weidi Zhang",
                        "contributors": [
                            "Weidi Zhang"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T15:05:00Z",
                        "time_end": "2020-10-28T15:10:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Invited Artist Talk: Jiabao Li",
                        "contributors": [
                            "Jiabao Li"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T15:10:00Z",
                        "time_end": "2020-10-28T15:15:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Q&A for Artist Talks",
                        "contributors": [
                            "Hannah Wolf",
                            "Weidi Zhang",
                            "Jiabao Li"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T15:15:00Z",
                        "time_end": "2020-10-28T15:25:00Z"
                    }
                ]
            },
            {
                "title": "VISAP Session 2",
                "session_id": "a-visap-2",
                "chair": [
                    "Erik Brunvand"
                ],
                "organizers": [
                    "Erik Brunvand",
                    "Yoon Chung Han",
                    "Carmen Hull",
                    "Maria Lantin"
                ],
                "display_start": "2020-10-29T14:00:00Z",
                "time_start": "2020-10-29T14:00:00Z",
                "time_end": "2020-10-29T15:20:00Z",
                "discord_category": "vis arts program",
                "discord_channel": "visap-session-2",
                "discord_channel_id": "768679025216323618",
                "youtube_url": "https://youtu.be/4z3aj046X1s",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrfNKPi1LOafSyNyBMTyE9Ya",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Printmaking, Puzzles, and Studio Closets: Using Artistic Metaphors to Reimagine the User Interface for Designing Immersive Visualizations",
                        "contributors": [
                            "Bridger Herman"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T14:00:00Z",
                        "time_end": "2020-10-29T14:15:00Z",
                        "uid": "a-arts-1037"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Tied in Knots. A Case Study on Anthropographic Data Visualization About Sexual Harassment in the Academy",
                        "contributors": [
                            "Tommaso Elli"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T14:15:00Z",
                        "time_end": "2020-10-29T14:30:00Z",
                        "uid": "a-arts-1033"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Leander: Navigating musical possibility space through color data sonification H44",
                        "contributors": [
                            "Lawton Hall"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T14:30:00Z",
                        "time_end": "2020-10-29T14:45:00Z",
                        "uid": "a-arts-1047"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Q&A for Papers and Pictorials",
                        "contributors": [
                            "Bridger Herman",
                            "Tommaso Elli",
                            "Lawton Hall"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T14:45:00Z",
                        "time_end": "2020-10-29T15:00:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Invited Artist Talk: Rebecca Xu",
                        "contributors": [
                            "Rebecca Xu"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T15:00:00Z",
                        "time_end": "2020-10-29T15:05:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Invited Artist Talk: Ha Na Lee",
                        "contributors": [
                            "Ha Na Lee"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T15:05:00Z",
                        "time_end": "2020-10-29T15:10:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Q&A for Artist Talks",
                        "contributors": [
                            "Rebecca Xu",
                            "Ha Na Lee"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T15:10:00Z",
                        "time_end": "2020-10-29T15:20:00Z"
                    }
                ]
            }
        ]
    },
    "m-lies": {
        "event": "VisLies!",
        "long_name": "VisLies!",
        "event_type": "Meetup",
        "event_description": "VisLies! is a yearly event at IEEE VIS. This fun and engaging evening session showcases examples of egregious perceptual, cognitive, and conceptual errors in visualization, presented by members of the Vis community.  Examples from our own work, from published papers, and from the internet highlight the many ways the visual representation can misrepresent the underlying phenomena in the data. This is a great opportunity for amusement and for learning, and every year we walk away with a smile on our faces and insights that may one day save the world.",
        "event_url": "",
        "sessions": [
            {
                "title": "VisLies!",
                "session_id": "m-lies",
                "chair": [
                    "Bernice Rogowitz",
                    "Ken Moreland"
                ],
                "organizers": [
                    "Ken Moreland",
                    "Bernice Rogowitz"
                ],
                "display_start": "2020-10-27T19:40:00Z",
                "time_start": "2020-10-27T19:40:00Z",
                "time_end": "2020-10-27T20:40:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "https://us02web.zoom.us/j/84103208111?pwd=UE85UHhXWTV2NzEzYXJSdlNWS2ZPQT09",
                "zoom_password": "OyY3uEBT",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Zoom Only",
                        "title": "VisLies!",
                        "contributors": [
                            "Ken Moreland",
                            "Bernice Rogowitz"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T19:40:00Z",
                        "time_end": "2020-10-27T20:40:00Z"
                    }
                ]
            }
        ]
    },
    "vis-opening": {
        "event": "VIS",
        "long_name": "VIS Opening",
        "event_type": "VIS",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Opening",
                "session_id": "vis-opening-1",
                "chair": [
                    "Valerio Pascucci",
                    "Mike Kirby"
                ],
                "organizers": [
                    "Valerio Pascucci",
                    "Mike Kirby"
                ],
                "display_start": "2020-10-27T13:20:00Z",
                "time_start": "2020-10-27T13:30:00Z",
                "time_end": "2020-10-27T13:45:00Z",
                "discord_category": "vis",
                "discord_channel": "opening",
                "discord_channel_id": "767121153688600589",
                "youtube_url": "https://youtu.be/FBaioHLtHAE",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "VIS Welcome",
                        "contributors": [
                            "Valerio Pascucci",
                            "Mike Kirby"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T13:30:00Z",
                        "time_end": "2020-10-27T13:43:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "VEC Intro",
                        "contributors": [
                            "Lisa Avila"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T13:43:00Z",
                        "time_end": "2020-10-27T13:45:00Z"
                    }
                ]
            },
            {
                "title": "Opening: Awards",
                "session_id": "vis-opening-2",
                "chair": [
                    "Valerio Pascucci",
                    "Catherine Plaisant",
                    "Terry Yoo",
                    "Mike Kirby",
                    "Giuseppe Santucci"
                ],
                "organizers": [
                    "Valerio Pascucci",
                    "Mike Kirby"
                ],
                "display_start": "2020-10-27T13:45:00Z",
                "time_start": "2020-10-27T13:45:00Z",
                "time_end": "2020-10-27T15:05:00Z",
                "discord_category": "vis",
                "discord_channel": "opening",
                "discord_channel_id": "767121153688600589",
                "youtube_url": "https://youtu.be/FBaioHLtHAE",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "VGTC Overview",
                        "contributors": [
                            "Jim Ahrens"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T13:45:00Z",
                        "time_end": "2020-10-27T13:48:00Z"
                    },
                    {
                        "type": "Live Presentation, Slides by Tech",
                        "title": "VGTC Awards Overview and Process",
                        "contributors": [
                            "Chuck Hansen"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T13:48:00Z",
                        "time_end": "2020-10-27T14:05:00Z"
                    },
                    {
                        "type": "Live Intro + Recorded Talk",
                        "title": "VGTC Visualization Technical Achievement Award",
                        "contributors": [
                            "Jean-Daniel Fekete"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T14:05:00Z",
                        "time_end": "2020-10-27T14:10:00Z"
                    },
                    {
                        "type": "Live Intro + Recorded Talk",
                        "title": "VGTC Visualization Career Award",
                        "contributors": [
                            "Catherine Plaisant"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T14:10:00Z",
                        "time_end": "2020-10-27T14:15:00Z"
                    },
                    {
                        "type": "Live Presentation, Slides by Tech",
                        "title": "VGTC Academy",
                        "contributors": [
                            "Chuck Hansen"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T14:15:00Z",
                        "time_end": "2020-10-27T14:20:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Introduction to Test of Time Awards",
                        "contributors": [
                            "Stephen North"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T14:20:00Z",
                        "time_end": "2020-10-27T14:25:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "VAST 10 Year Test of Time Award: iVisClassifier Introduction",
                        "contributors": [
                            "Giuseppe Santucci"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T14:25:00Z",
                        "time_end": "2020-10-27T14:28:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "VAST 10 Year Test of Time Award: iVisClassifier: An interactive visual analytics system for classification based on supervised dimension reduction",
                        "contributors": [
                            "Jaegul Choo",
                            "Hanseung Lee",
                            "Jaeyeon Kihm",
                            "Haesun Park\u00a0"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T14:28:00Z",
                        "time_end": "2020-10-27T14:32:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "InfoVis 20 Year Test of Time Award Introduction",
                        "contributors": [
                            "Catherine Plaisant"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T14:32:00Z",
                        "time_end": "2020-10-27T14:34:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "InfoVis 20 Year Test of Time Award: Polaris: a system for query, analysis and visualization of multi-dimensional relational databases",
                        "contributors": [
                            "Chris Stolte",
                            "Diane Tang",
                            "Pat Hanrahan"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T14:34:00Z",
                        "time_end": "2020-10-27T14:41:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "InfoVis 10 Year Test of Time Award Introduction",
                        "contributors": [
                            "Catherine Plaisant"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T14:41:00Z",
                        "time_end": "2020-10-27T14:43:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "InfoVis 10 Year Test of Time Award: Narrative Visualization: Telling Stories with Data",
                        "contributors": [
                            "Edward Segel",
                            "Jeffrey Heer"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T14:43:00Z",
                        "time_end": "2020-10-27T14:49:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "SciVis 25 Year Test of Time Award Introduction",
                        "contributors": [
                            "Terry Yoo"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T14:49:00Z",
                        "time_end": "2020-10-27T14:52:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "SciVis 25 Year Test of Time Award: High Dimensional Brushing for Interactive Exploration of Multivariate Data",
                        "contributors": [
                            "Allen R. Martin",
                            "Matthew O. Ward"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T14:52:00Z",
                        "time_end": "2020-10-27T14:57:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "SciVis 15 Year Test of Time Award Introduction",
                        "contributors": [
                            "Terry Yoo"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T14:57:00Z",
                        "time_end": "2020-10-27T14:59:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "SciVis 15 Year Test of Time Award: The Value of Visualization",
                        "contributors": [
                            "Jarke J. van Wijk"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-27T14:59:00Z",
                        "time_end": "2020-10-27T15:05:00Z"
                    }
                ]
            },
            {
                "title": "Opening: Best Papers",
                "session_id": "vis-opening-3",
                "chair": [
                    "Valerio Pascucci",
                    "Mike Kirby"
                ],
                "organizers": [
                    "Valerio Pascucci",
                    "Mike Kirby"
                ],
                "display_start": "2020-10-27T15:06:00Z",
                "time_start": "2020-10-27T15:06:00Z",
                "time_end": "2020-10-27T16:00:00Z",
                "discord_category": "vis",
                "discord_channel": "opening",
                "discord_channel_id": "767121153688600589",
                "youtube_url": "https://youtu.be/FBaioHLtHAE",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection",
                        "contributors": [
                            "Liang Gou"
                        ],
                        "abstract": "Traffic light detection is crucial for environment perception and decision-making in autonomous driving. State-of-the-art detectors are built upon deep Convolutional Neural Networks (CNN) and have exhibited promising performance. However, one looming concern with CNN based detectors is how to thoroughly evaluate the performance of accuracy and robustness before they can be deployed to autonomous vehicles. In this work, we propose a visual analytics system, VATLD, equipped with a disentangled representation learning and semantic adversarial learning, to assess, understand, and improve the accuracy and robustness of traffic light detectors in autonomous driving applications. The disentangled representation learning extracts data semantics to augment human cognition with human-friendly visual summarization, and the semantic adversarial learning efficiently exposes the interpretable robustness risks and enables minimal human interaction for actionable insights. We also demonstrate the effectiveness of various performance improvement strategies derived from actionable insights with our visual analytics system, VATLD, and illustrate the practical implications for safety-critical applications in autonomous driving.",
                        "time_start": "2020-10-27T15:06:00Z",
                        "time_end": "2020-10-27T15:20:00Z",
                        "uid": "f-vast-1307"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visual Reasoning Strategies for Effect Size Judgments and Decisions",
                        "contributors": [
                            "Alex Kale"
                        ],
                        "abstract": "Uncertainty visualizations often emphasize point estimates to support magnitude estimates or decisions through visual comparison. However, when design choices emphasize means, users may overlook uncertainty information and misinterpret visual distance as a proxy for effect size. We present findings from a mixed design experiment on Mechanical Turk which tests eight uncertainty visualization designs: 95% containment intervals, hypothetical outcome plots, densities, and quantile dotplots, each with and without means added. We find that adding means to uncertainty visualizations has small biasing effects on both magnitude estimation and decision-making, consistent with discounting uncertainty. We also see that visualization designs that support the least biased effect size estimation do not support the best decision-making, suggesting that a chart user's sense of effect size may not necessarily be identical when they use the same information for different tasks. In a qualitative analysis of users' strategy descriptions, we find that many users switch strategies and do not employ an optimal strategy when one exists. Uncertainty visualizations which are optimally designed in theory may not be the most effective in practice because of the ways that users satisfice with heuristics, suggesting opportunities to better understand visualization effectiveness by modeling sets of potential strategies.",
                        "time_start": "2020-10-27T15:20:00Z",
                        "time_end": "2020-10-27T15:35:00Z",
                        "uid": "f-info-1187"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Objective Observer-Relative Flow Visualization in Curved Spaces for Unsteady 2D Geophysical Flows",
                        "contributors": [
                            "Peter Rautek"
                        ],
                        "abstract": "Computing and visualizing features in fluid flow often depends on the observer, or reference frame, relative to which the input velocity field is given. A desired property of feature detectors is therefore that they are objective, meaning independent of the input reference frame. However, the standard definition of objectivity is only given for Euclidean domains, and cannot be applied in curved spaces. We build on methods from mathematical physics and Riemannian geometry to generalize objectivity to curved spaces, using the powerful notion of symmetry groups as the basis for defining objectivity. From this, we develop a general mathematical framework for computing objective observer fields for curved surfaces, relative to which other computed measures become objective. An important property of our framework is that it works intrinsically in 2D, instead of in the 3D ambient space. This enables a direct generalization of the 2D computation of observer fields via optimization from flat domains to curved domains, without having to perform optimization in 3D. We specifically develop the case of unsteady 2D geophysical flows given on spheres, such as the Earth. Observer fields on curved surfaces enable objective feature computation as well as the visualization of the time evolution of scalar and vector fields, such that the automatically computed reference frames follow moving structures like vortices in a way that makes them appear to be steady.",
                        "time_start": "2020-10-27T15:35:00Z",
                        "time_end": "2020-10-27T15:50:00Z",
                        "uid": "f-scivis-1144"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "The Anatomical Edutainer",
                        "contributors": [
                            "Marwin Schindler"
                        ],
                        "abstract": "Physical visualizations (i.e., data representations by means of physical objects) have been used for many centuries in medical and anatomical education. Recently, 3D printing techniques started also to emerge. Still, other medical physicalizations that rely on affordable and easy-to-find materials are limited, while smart strategies that take advantage of the optical properties of our physical world have not been thoroughly investigated. We propose the Anatomical Edutainer, a workflow to guide the easy, accessible, and affordable generation of physicalizations for tangible, interactive anatomical edutainment. The Anatomical Edutainer supports 2D printable and 3D foldable physicalizations that change their visual properties (i.e., hues of the visible spectrum) under colored lenses or colored lights, to reveal distinct anatomical structures through user interaction",
                        "time_start": "2020-10-27T15:50:00Z",
                        "time_end": "2020-10-27T16:00:00Z",
                        "uid": "s-short-1109"
                    }
                ]
            }
        ]
    },
    "vis-keynote": {
        "event": "Keynote",
        "long_name": "VIS Keynote",
        "event_type": "Keynote",
        "event_description": "For the first time in the history of the VIS conference, we will have a Nobel prize awardee as the keynote speaker: Mario Capecchi co-winner of the 2007 Nobel Prize in Physiology or Medicine.\n\nMario Capecchi, a renowned geneticist at the University of Utah, is a Distinguished Professor of Human Genetics with a unique life story of overcoming incredible odds when he nearly died as a child in the streets of Italy after his mother was arrested and put in a concentration camp by the Nazis during World War II.\n\nWith an uplifting message that is mainly directed towards the junior members of our community, Mario Capecchi will stress the importance of collaboration in scientific investigation and how strong collaborative research helped him in his professional journey towards the Nobel prize.",
        "event_url": "",
        "sessions": [
            {
                "title": "Keynote by Mario Capecchi",
                "session_id": "vis-keynote",
                "chair": [
                    "Valerio Pascucci",
                    "Mike Kirby"
                ],
                "organizers": [
                    "Valerio Pascucci",
                    "Mike Kirby"
                ],
                "display_start": "2020-10-27T16:00:00Z",
                "time_start": "2020-10-27T16:00:00Z",
                "time_end": "2020-10-27T17:00:00Z",
                "discord_category": "keynote",
                "discord_channel": "general",
                "discord_channel_id": "767121153688600589",
                "youtube_url": "https://youtu.be/FBaioHLtHAE",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "The Making of a Scientist",
                        "contributors": [
                            "Mario Capecchi"
                        ],
                        "abstract": "For the first time in the history of the VIS conference, we will have a Nobel prize awardee as the keynote speaker: Mario Capecchi co-winner of the 2007 Nobel Prize in Physiology or Medicine.\n\nMario Capecchi, a renowned geneticist at the University of Utah, is a Distinguished Professor of Human Genetics with a unique life story of overcoming incredible odds when he nearly died as a child in the streets of Italy after his mother was arrested and put in a concentration camp by the Nazis during World War II.\n\nWith an uplifting message that is mainly directed towards the junior members of our community, Mario Capecchi will stress the importance of collaboration in scientific investigation and how strong collaborative research helped him in his professional journey towards the Nobel prize.",
                        "time_start": "2020-10-27T16:00:00Z",
                        "time_end": "2020-10-27T17:00:00Z"
                    }
                ]
            }
        ]
    },
    "a-vizsec": {
        "event": "VizSec",
        "long_name": "IEEE Symposium on Visualization for Cyber Security",
        "event_type": "Symposium",
        "event_description": "VizSec brings together researchers and practitioners in information visualization and security to address the specific needs of the cyber security community through new and insightful visualization techniques.",
        "event_url": "https://vizsec.org/vizsec2020/",
        "sessions": [
            {
                "title": "Opening, Keynote, Best Paper",
                "session_id": "a-vizsec-1",
                "chair": [
                    "J\u00f6rn Kohlhammer",
                    "Marco Angelini"
                ],
                "organizers": [
                    "Marco Angelini",
                    "Chris Bryan",
                    "J\u00f6rn Kohlhammer"
                ],
                "display_start": "2020-10-28T14:00:00Z",
                "time_start": "2020-10-28T14:00:00Z",
                "time_end": "2020-10-28T15:30:00Z",
                "discord_category": "vizsec",
                "discord_channel": "opening-keynote-best-paper",
                "discord_channel_id": "767561678773682238",
                "youtube_url": "https://youtu.be/sbJzQvOD62E",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrfolfRl38D7dlVqN2qieFUU",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Opening Presentation",
                        "contributors": [
                            "J\u00f6rn Kohlhammer"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T14:00:00Z",
                        "time_end": "2020-10-28T14:10:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Keynote by Joshua Saxe",
                        "contributors": [
                            "Joshua Saxe"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T14:10:00Z",
                        "time_end": "2020-10-28T15:00:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Best Paper Introduction",
                        "contributors": [
                            "Marco Angelini"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T15:00:00Z",
                        "time_end": "2020-10-28T15:05:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Visualization Interface to Improve the Transparency of Collected Personal Data on the Internet",
                        "contributors": [
                            "Marija Schufrin, Steven Lamarr Reynolds"
                        ],
                        "abstract": "Online services are used for all kinds of activities, like news, entertainment, publishing content or connecting with others. But information technology enables new threats to privacy by means of global mass surveillance, vast databases and fast distribution networks. Current news are full of misuses and data leakages. In most cases, users are powerless in such situations and develop an attitude of neglect for their online behaviour. On the other hand, the GDPR (General Data Protection Regulation) gives users the right to request a copy of all their personal data stored by a particular service, but the received data is hard to understand or analyze by the common internet user. This paper presents TransparencyVis - a web-based interface to support the visual and interactive exploration of data exports from different online services. With this approach, we aim at increasing the awareness of personal data stored by such online services and the effects of online behaviour. This design study provides an online accessible prototype and a best practice to unify data exports from different sources.",
                        "time_start": "2020-10-28T15:05:00Z",
                        "time_end": "2020-10-28T15:30:00Z",
                        "uid": "a-vizsec-1011"
                    }
                ]
            },
            {
                "title": "Network Analysis and Incident Response, Practitioner talks",
                "session_id": "a-vizsec-2",
                "chair": [
                    "Steven Gomez",
                    "Lane Harrison"
                ],
                "organizers": [
                    "Marco Angelini",
                    "Chris Bryan",
                    "J\u00f6rn Kohlhammer"
                ],
                "display_start": "2020-10-28T16:00:00Z",
                "time_start": "2020-10-28T16:00:00Z",
                "time_end": "2020-10-28T17:30:00Z",
                "discord_category": "vizsec",
                "discord_channel": "network-analysis-and-incident-response-practitioner-talks",
                "discord_channel_id": "767561736312061982",
                "youtube_url": "https://youtu.be/F3zXF36nclw",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrfolfRl38D7dlVqN2qieFUU",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Exploratory Analysis of File System Metadata for Rapid Investigation of Security Incidents",
                        "contributors": [
                            "Radek O\u0161lej\u0161ek"
                        ],
                        "abstract": "Investigating cybersecurity incidents requires in-depth knowledge from the analyst. Moreover, the whole process is demanding due to the vast data volumes that need to be analyzed. While various techniques exist nowadays to help with particular tasks of the analysis, the process as a whole still requires a lot of manual activities and expert skills. We propose an approach that allows the analysis of disk snapshots more efficiently and with lower demands on expert knowledge. Following a user-centered design methodology, we implemented an analytical tool to guide analysts during security incident investigations. The viability of the solution was validated by an evaluation conducted with members of different security teams.",
                        "time_start": "2020-10-28T16:00:00Z",
                        "time_end": "2020-10-28T16:20:00Z",
                        "uid": "a-vizsec-1016"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Improving Interpretability for Cyber Vulnerability Assessment Using Focus and Context Visualizations",
                        "contributors": [
                            "Kenneth B. Alperin"
                        ],
                        "abstract": "Risk scoring provides a simple and quantifiable metric for decision support in cyber security operations, including prioritizing how to address discovered software vulnerabilities. However, scoring systems are often opaque to operators, which makes scores difficult to interpret in the context of their own networks, each other, or in a broader threat landscape. This interpretability challenge is exacerbated by recent applications of artificial intelligence (AI) and machine learning (ML) for vulnerability assessment, where opaque machine reasoning can hinder domain experts' trust in the decision-support toolkit or the actionability of its outputs. In this paper, we address this challenge through a combination of visualizations and analytics that complement existing techniques for vulnerability assessment. We present a study toward designing more interpretable visual encodings for decision support for vulnerability assessment. In particular, we consider the problem of making datasets of known vulnerabilities more interpretable at multiple scales, inspired by focus and context principles from the information visualization design community. The first scale considers individually scored vulnerabilities by using an explainable AI (XAI) toolkit for an ML risk-scoring model and by developing new visualizations of CVSS score features. The second scale uses an embedding for vulnerability descriptions to cluster potentially similar vulnerabilities. We outline use cases for these tools and discuss opportunities for applying XAI concepts to cyber risk and vulnerability management.",
                        "time_start": "2020-10-28T16:20:00Z",
                        "time_end": "2020-10-28T16:40:00Z",
                        "uid": "a-vizsec-1009"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Practitioner Talk 1",
                        "contributors": [
                            "Leo Meyerovich (Graphistry)"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T16:40:00Z",
                        "time_end": "2020-10-28T16:50:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Practitioner Talk 2",
                        "contributors": [
                            "Robert Gove (Two Six Labs)"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T16:50:00Z",
                        "time_end": "2020-10-28T17:00:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Practitioner Talk 3",
                        "contributors": [
                            "Younghoo Lee (Sophos)"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T17:00:00Z",
                        "time_end": "2020-10-28T17:10:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Q&A for Practitioners",
                        "contributors": [
                            "Leo Meyerovich",
                            "Robert Gove",
                            "Younghoo Lee"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T17:10:00Z",
                        "time_end": "2020-10-28T17:30:00Z"
                    }
                ]
            },
            {
                "title": "Short Papers, Posters and Closing",
                "session_id": "a-vizsec-3",
                "chair": [
                    "Sean McKenna",
                    "J\u00f6rn Kohlhammer",
                    "Chris Bryan"
                ],
                "organizers": [
                    "Marco Angelini",
                    "Chris Bryan",
                    "J\u00f6rn Kohlhammer"
                ],
                "display_start": "2020-10-28T18:00:00Z",
                "time_start": "2020-10-28T18:00:00Z",
                "time_end": "2020-10-28T19:10:00Z",
                "discord_category": "vizsec",
                "discord_channel": "short-papers-posters-and-closing",
                "discord_channel_id": "767561773921468416",
                "youtube_url": "https://youtu.be/zTy8cwDjiJU",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrfolfRl38D7dlVqN2qieFUU",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Visualization for Spectators in Cybersecurity Competitions",
                        "contributors": [
                            "David Schwartz"
                        ],
                        "abstract": "The goal is to raise awareness and encourage learning cybersecurity principles by making competitions appealing to a wider audience. In an effort to make events compelling, attractive, and watchable, the researchers will develop systems to support visualizations and make the transactions between teams in different cybersecurity competitions easy to comprehend. In informing and educating the audience on the intricacies of the competition through engaging visualizations, cybersecurity competitions will be opened up to a world beyond just participants. In doing so, we can potentially attract new talent into the field. Our team seeks to make prototype visualizations for key actions in various student cybersecurity competitions and assess spectator understanding of key principles of the competition.",
                        "time_start": "2020-10-28T18:00:00Z",
                        "time_end": "2020-10-28T18:20:00Z",
                        "uid": "a-vizsec-1007"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Malware vs Anti-Malware Battle - Gotta Evade 'em All",
                        "contributors": [
                            "Emily J. Chaffey"
                        ],
                        "abstract": "The landscape of malware development is ever-changing, creating a constant catch-up contest between the defenders and the adversaries. One of the methodologies that has the potential to pose a significant threat to systems is malware evasion. This is where malware tries to determine whether it is run in a controlled environment, such as a sandbox. Similarly, a malware can also learn how an Anti-Malware System (AMS) decides whether an input program is a malware or in fact benign with the goal of bypassing it. On the other hand, the AMS tries to detect whether a malware sample is performing such evasive checks, e.g. by evaluating the results of Reverse-Turing Test (RTT). This learning process can be viewed as a \u2018battle\u2019 between the AMS and the malware, due to the malware attempting to defeat the AMS, where a successful win for the malware would be to evade detection by the AMS and, conversely, a win for the AMS would be to correctly detect the malware and its evasive actions. We propose a visualisation-based system, called Gotta Evade \u2018em All, that allows cyber-security analysts to clearly see the evasive and anti-evasive actions performed by the malware and the AMS during the battle.",
                        "time_start": "2020-10-28T18:20:00Z",
                        "time_end": "2020-10-28T18:35:00Z",
                        "uid": "a-vizsec-1010"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Interpretable Visualizations of Deep Neural Networks for Domain Generation Algorithm Detection",
                        "contributors": [
                            "Franziska Becker"
                        ],
                        "abstract": "Due to their success in many application areas, deep learning models have found wide adoption for many problems. However, their black-box nature makes it hard to trust their decisions and to evaluate their line of reasoning. In the field of cybersecurity, this lack of trust and understanding poses a significant challenge for the utilization of deep learning models. Thus, we present a visual analytics system that provides designers of deep learning models for the classification of domain generation algorithms with understandable interpretations of their model. We cluster the activations of the model's nodes and leverage decision trees to explain these clusters. In combination with a 2D projection, the user can explore how the model views the data at different layers. In a preliminary evaluation of our system, we show how it can be employed to better understand misclassifications, identify potential biases and reason about the role different layers in a model may play.",
                        "time_start": "2020-10-28T18:35:00Z",
                        "time_end": "2020-10-28T18:50:00Z",
                        "uid": "a-vizsec-1017"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Poster Booster Session",
                        "contributors": [
                            "Sean McKenna"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T18:50:00Z",
                        "time_end": "2020-10-28T19:00:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Closing",
                        "contributors": [
                            "J\u00f6rn Kohlhammer"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T19:00:00Z",
                        "time_end": "2020-10-28T19:10:00Z"
                    }
                ]
            }
        ]
    },
    "vis-supporters": {
        "event": "Industrial Keynotes",
        "long_name": "Industrial Keynotes",
        "event_type": "Industrial Keynotes",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Tableau",
                "session_id": "vis-supporters-1",
                "chair": [
                    "Valerio Pascucci",
                    "Mike Kirby"
                ],
                "organizers": [
                    "Brian Summa"
                ],
                "display_start": "2020-10-28T15:35:00Z",
                "time_start": "2020-10-28T15:45:00Z",
                "time_end": "2020-10-28T16:00:00Z",
                "discord_category": "industrial keynotes",
                "discord_channel": "tableau",
                "discord_channel_id": "767561694448320532",
                "youtube_url": "https://youtu.be/Mc6v7_--wao",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Tableau",
                        "contributors": [
                            "Maureen Stone"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T15:45:00Z",
                        "time_end": "2020-10-28T16:00:00Z"
                    }
                ]
            },
            {
                "title": "Intel",
                "session_id": "vis-supporters-2",
                "chair": [
                    "Valerio Pascucci",
                    "Mike Kirby"
                ],
                "organizers": [
                    "Brian Summa"
                ],
                "display_start": "2020-10-29T15:35:00Z",
                "time_start": "2020-10-29T15:45:00Z",
                "time_end": "2020-10-29T16:00:00Z",
                "discord_category": "industrial keynotes",
                "discord_channel": "intel",
                "discord_channel_id": "768679032153440277",
                "youtube_url": "https://youtu.be/KdFvnLW0je4",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Intel",
                        "contributors": [
                            "Jim Jeffers"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T15:45:00Z",
                        "time_end": "2020-10-29T16:00:00Z"
                    }
                ]
            }
        ]
    },
    "cga-papers": {
        "event": "CG&A Full Papers",
        "long_name": "CG&A Full Papers",
        "event_type": "Paper Presentations",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Vis for the Masses",
                "session_id": "cga-papers-vis-masses",
                "chair": [
                    "Ingrid Hotz"
                ],
                "organizers": [],
                "display_start": "2020-10-28T16:00:00Z",
                "time_start": "2020-10-28T16:00:00Z",
                "time_end": "2020-10-28T17:30:00Z",
                "discord_category": "cga full papers",
                "discord_channel": "vis-for-the-masses",
                "discord_channel_id": "767561728438566922",
                "youtube_url": "https://youtu.be/DyQHu05X4w8",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrclVUrIPaS045_I1rOkf1Y5",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Personalized Sketch-Based Brushing in Scatterplots",
                        "contributors": [
                            "Chaoran Fan"
                        ],
                        "abstract": "Brushing is at the heart of most modern visual analytics solutions and effective and efficient brushing is crucial for successful interactive data exploration and analysis. As the user plays a central role in brushing, several data-driven brushing tools have been designed that are based on predicting the user's brushing goal. All of these general brushing models learn the users' average brushing preference, which is not optimal for every single user. In this paper, we propose an innovative framework that offers the user opportunities to improve the brushing technique while using it. We realized this framework with a CNN-based brushing technique and the result shows that with additional data from a particular user, the model can be refined (better performance in terms of accuracy), eventually converging to a personalized model based on a moderate amount of retraining.",
                        "time_start": "2020-10-28T16:00:00Z",
                        "time_end": "2020-10-28T16:15:00Z",
                        "uid": "f-cga-1"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Aggregated Ensemble Views for Deep Water Asteroid Impact Simulations",
                        "contributors": [
                            "Karim Huesmann"
                        ],
                        "abstract": "Simulation ensembles such as the ones simulating deep water asteroid impacts have many facets. Their analysis in terms of detecting spatiotemporal patterns, comparing multiple runs, and analyzing the influence of simulation parameters requires aggregation at multiple levels. We propose respective visual encodings embedded in an interactive visual analysis tool.",
                        "time_start": "2020-10-28T16:15:00Z",
                        "time_end": "2020-10-28T16:30:00Z",
                        "uid": "f-cga-2"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Illustrating Changes in Time-Series Data With Data Video",
                        "contributors": [
                            "Junhua Lu"
                        ],
                        "abstract": "Understanding the changes of time-series is a common task in many application domains. Converting time-series data into videos helps an audience with little or no background knowledge gain insights and deep impressions. It essentially integrates data visualizations and animations to present the evolution of data expressively. However, it remains challenging to create this kind of data video. First, it is difficult to efficiently detect important changes and include them in the video sequence. Existing methods require much manual effort to explore the data and find changes. Second, how these changes are emphasized in the videos is also worth studying. A video without emphasis will hinder an audience from noticing those important changes. This article presents an approach that extracts and visualizes important changes of a time-series. Users can explore and modify these changes, and apply visual effects on them. Case studies and user feedback demonstrate the effectiveness and usability of our approach.",
                        "time_start": "2020-10-28T16:30:00Z",
                        "time_end": "2020-10-28T16:45:00Z",
                        "uid": "f-cga-3"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Nano for the Public: An Exploranation Perspective",
                        "contributors": [
                            "Gunnar H\u00f6st"
                        ],
                        "abstract": "Public understanding of contemporary scientific issues is critical for the future of society. Public spaces, such as science centers, can impact the communication of science by providing active knowledge-building experiences of scientific phenomena. In contributing to this vision, we have previously developed an interactive visualization as part of a public exhibition about nano. We reflect on how the immersive design and features of the exhibit contribute as a tool for science communication in light of the emerging paradigm of exploranation, and offer some forward-looking perspectives about what this notion has to offer the domain.",
                        "time_start": "2020-10-28T16:45:00Z",
                        "time_end": "2020-10-28T17:00:00Z",
                        "uid": "f-cga-4"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "PixelClipper: Supporting Public Engagement and Conversation About Visualizations",
                        "contributors": [
                            "Sarah Storteboom"
                        ],
                        "abstract": "In this article, we present PixelClipper, a tool built for facilitating data engagement events. PixelClipper supports conversations around visualizations in public settings through annotation and commenting capabilities. It is recognized that understanding data is important for an informed society. However, even when visualizations are available on the web, open data is not yet reaching all audiences. Public facilitated events centered around data visualizations may help bridge this gap. PixelClipper is designed to promote discussion and engagement with visualizations in public settings. It allows viewers to quickly and expressively extract visual clippings from visualizations and add comments to them. Ambient and facilitator displays attract attention by showing clippings. They function as entry points to the full visualizations while supporting deeper conversations about the visualizations and data. We describe the design goals of PixelClipper, share our experiences from deploying it, and discuss its future potential in supporting data visualization engagement events.",
                        "time_start": "2020-10-28T17:00:00Z",
                        "time_end": "2020-10-28T17:15:00Z",
                        "uid": "f-cga-5"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Data-Driven Introduction to Authors, Readings, and Techniques in Visualization for the Digital Humanities",
                        "contributors": [
                            "Alejandro Benito-Santos"
                        ],
                        "abstract": "The newly rediscovered frontier between data visualization and the digital humanities has proven to be an exciting field of experimentation for scholars from both disciplines. This fruitful collaboration is attracting researchers from other areas of science who may be willing to create visual analysis tools that promote humanities research in its many forms. However, as the collaboration grows in complexity, it may become intimidating for these scholars to get engaged in the discipline. To facilitate this task, we have built an introduction to visualization for the digital humanities that sits on a data-driven stance adopted by the authors. In order to construct a dataset representative of the discipline, we analyze citations from a core corpus on 300 publications in visualization for the humanities obtained from recent editions of the InfoVis Vis4DH workshop, the ADHO Digital Humanities Conference, and the specialized digital humanities journal Digital Humanities Quarterly. From here, we extract referenced works and analyze more than 1900 publications in search of citation patterns, prominent authors in the field, and other interesting insights. Finally, following the path set by other researchers in the visualization and Human-Computer Interaction (HCI) communities, we analyze paper keywords to identify significant themes and research opportunities in the field.",
                        "time_start": "2020-10-28T17:15:00Z",
                        "time_end": "2020-10-28T17:30:00Z",
                        "uid": "f-cga-6"
                    }
                ]
            },
            {
                "title": "Toward Better Visual Analysis",
                "session_id": "cga-papers-toward-better-va",
                "chair": [
                    "Melanie Tory"
                ],
                "organizers": [],
                "display_start": "2020-10-29T16:00:00Z",
                "time_start": "2020-10-29T16:00:00Z",
                "time_end": "2020-10-29T17:30:00Z",
                "discord_category": "cga full papers",
                "discord_channel": "toward-better-visual-analysis",
                "discord_channel_id": "768679079347748894",
                "youtube_url": "https://youtu.be/9QlEdL-jRxI",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "https://www.youtube.com/playlist?list=PLjHCTOW5ojrdirV9UQb46BRVONU2Fws26",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Evaluating the Readability of Force Directed Graph Layouts: A Deep Learning Approach",
                        "contributors": [
                            "Hammad Haleem",
                            "Abishek Puri"
                        ],
                        "abstract": "Existing graph layout algorithms are usually not able to optimize all the aesthetic properties desired in a graph layout. To evaluate how well the desired visual features are reflected in a graph layout, many readability metrics have been proposed in the past decades. However, the calculation of these readability metrics often requires access to the node and edge coordinates and is usually computationally inefficient, especially for dense graphs. Importantly, when the node and edge coordinates are not accessible, it becomes impossible to evaluate the graph layouts quantitatively. In this paper, we present a novel deep learning-based approach to evaluate the readability of graph layouts by directly using graph images. A convolutional neural network architecture is proposed and trained on a benchmark dataset of graph images, which is composed of synthetically generated graphs and graphs created by sampling from real large networks. Multiple representative readability metrics (including edge crossing, node spread, and group overlap) are considered in the proposed approach. We quantitatively compare our approach to traditional methods and qualitatively evaluate our approach by showing usage scenarios and visualizing convolutional layers. This paper is a first step towards using deep learning based methods to quantitatively evaluate images from the visualization field.",
                        "time_start": "2020-10-29T16:00:00Z",
                        "time_end": "2020-10-29T16:15:00Z",
                        "uid": "f-cga-7"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Flow Field Reduction Via Reconstructing Vector Data From 3-D Streamlines Using Deep Learning",
                        "contributors": [
                            "Jun Han"
                        ],
                        "abstract": "We present a new approach for streamline-based flow field representation and reduction. Our method can work in the in situ visualization setting by tracing streamlines from each time step of the simulation and storing compressed streamlines for post hoc analysis where users can afford longer reconstruction time for higher reconstruction quality using decompressed streamlines. At the heart of our approach is a deep learning method for vector field reconstruction that takes the streamlines traced from the original vector fields as input and applies a two-stage process to reconstruct high-quality vector fields. To demonstrate the effectiveness of our approach, we show qualitative and quantitative results with several data sets and compare our method against the de facto method of gradient vector flow in terms of speed and quality tradeoff.",
                        "time_start": "2020-10-29T16:15:00Z",
                        "time_end": "2020-10-29T16:30:00Z",
                        "uid": "f-cga-8"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Analytic Provenance in Practice: The Role of Provenance in Real-World Visualization and Data Analysis Environments",
                        "contributors": [
                            "Karthic Madanagopal"
                        ],
                        "abstract": "Practical data analysis scenarios involve more than just the interpretation of data through visual and algorithmic analysis. Many real-world analysis environments involve multiple types of experts and analysts working together to solve problems and make decisions, adding organizational and social requirements to the mix. We aim to provide new knowledge about the role of provenance for practical problems in a variety of analysis scenarios central to national security. We present the findings from interviews with data analysts from domains, such as intelligence analysis, cyber-security, and geospatial intelligence. In addition to covering multiple analysis domains, our study also considers practical workplace implications related to organizational roles and the level of analyst experience. The results demonstrate how different needs for provenance depend on different roles in the analysis effort (e.g., data analyst, task managers, data analyst trainers, and quality control analysts). By considering the core challenges reported along with an analysis of existing provenance-support techniques through existing research and systems, we contribute new insights about needs and opportunities for improvements to provenance-support methods.",
                        "time_start": "2020-10-29T16:30:00Z",
                        "time_end": "2020-10-29T16:45:00Z",
                        "uid": "f-cga-9"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "A Provenance Task Abstraction Framework",
                        "contributors": [
                            "Simon Attfield"
                        ],
                        "abstract": "Visual analytics tools integrate provenance recording to externalize analytic processes or user insights. Provenance can be captured on varying levels of detail, and in turn activities can be characterized from different granularities. However, current approaches do not support inferring activities that can only be characterized across multiple levels of provenance. We propose a task abstraction framework that consists of a three stage approach, composed of 1) initializing a provenance task hierarchy, 2) parsing the provenance hierarchy by using an abstraction mapping mechanism, and 3) leveraging the task hierarchy in an analytical tool. Furthermore, we identify implications to accommodate iterative refinement, context, variability, and uncertainty during all stages of the framework. We describe a use case which exemplifies our abstraction framework, demonstrating how context can influence the provenance hierarchy to support analysis. The article concludes with an agenda, raising and discussing challenges that need to be considered for successfully implementing such a framework.",
                        "time_start": "2020-10-29T16:45:00Z",
                        "time_end": "2020-10-29T17:00:00Z",
                        "uid": "f-cga-10"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Capturing and Visualizing Provenance From Data Wrangling",
                        "contributors": [
                            "Christian Bors"
                        ],
                        "abstract": "Data quality management and assessment play a vital role for ensuring the trust in the data and its fitness-of-use for subsequent analysis. The transformation history of a data wrangling system is often insufficient for determining the usability of a dataset, lacking information how changes affected the dataset. Capturing workflow provenance along the wrangling process and combining it with descriptive information as data provenance can enable users to comprehend how these changes affected the dataset, and if they benefited data quality. We present DQProv Explorer, a system that captures and visualizes provenance from data wrangling operations. It features three visualization components: allowing the user to explore the provenance graph of operations and the data stream, the development of quality over time for a sequence of wrangling operations applied to the dataset, and the distribution of issues across the entirety of the dataset to determine error patterns.",
                        "time_start": "2020-10-29T17:00:00Z",
                        "time_end": "2020-10-29T17:15:00Z",
                        "uid": "f-cga-11"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Towards Placental Surface Vasculature Exploration in Virtual Reality",
                        "contributors": [
                            "Johannes Novotny"
                        ],
                        "abstract": "We present a case study evaluating the potential for interactively identifying placental surface blood vessels using magnetic resonance imaging (MRI) scans in virtual reality (VR) environments. We visualized the MRI data using direct volume rendering in a high-fidelity CAVE-like VR system, allowing medical professionals to identify relevant placental vessels directly from volume visualizations in the VR system, without prior vessel segmentation. Participants were able to trace most of the observable vascular structure, and consistently identified blood vessels down to diameters of 1 mm, an important requirement in diagnosing vascular diseases. Qualitative feedback from our participants suggests that our VR visualization is easy to understand and allows intuitive data exploration, but complex user interactions remained a challenge. Using these observations, we discuss implications and requirements for spatial tracing user interaction methods in VR environments. We believe that VR MRI visualizations are the next step towards effective surgery planning for prenatal diseases.",
                        "time_start": "2020-10-29T17:15:00Z",
                        "time_end": "2020-10-29T17:30:00Z",
                        "uid": "f-cga-12"
                    }
                ]
            }
        ]
    },
    "vis-gala": {
        "event": "VIS",
        "long_name": "Utah Natural History Museum Tour and Reception",
        "event_type": "VIS",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Utah Natural History Museum Tour and Reception",
                "session_id": "vis-gala",
                "chair": [
                    "Valerio Pascucci",
                    "Mike Kirby"
                ],
                "organizers": [
                    "Valerio Pascucci",
                    "Mike Kirby"
                ],
                "display_start": "2020-10-28T17:30:00Z",
                "time_start": "2020-10-28T17:30:00Z",
                "time_end": "2020-10-28T18:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": "https://youtu.be/uAiiglVIJPM",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Utah Natural History Museum Tour and Reception",
                        "contributors": [
                            "Valerio Pascucci",
                            "Mike Kirby"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T17:30:00Z",
                        "time_end": "2020-10-28T18:00:00Z"
                    }
                ]
            }
        ]
    },
    "a-sciviscontest": {
        "event": "SciVis Contest",
        "long_name": "SciVis Contest",
        "event_type": "Associated Event",
        "event_description": "The 2020 IEEE SciVis Contest is dedicated to create novel approaches or state of the art visualizations to assist domain scientists to better understand the complex transport mechanisms of eddies in the Red Sea under uncertainty",
        "event_url": "https://kaust-vislab.github.io/SciVis2020/",
        "sessions": [
            {
                "title": "SciVis Contest",
                "session_id": "a-sciviscontest",
                "chair": [
                    "Thomas Theussl"
                ],
                "organizers": [
                    "Thomas Theussl",
                    "Madhu Srinivasan",
                    "Silvio Rizzi"
                ],
                "display_start": "2020-10-28T18:00:00Z",
                "time_start": "2020-10-28T18:00:00Z",
                "time_end": "2020-10-28T19:30:00Z",
                "discord_category": "scivis contest",
                "discord_channel": "general",
                "discord_channel_id": "767561646997635092",
                "youtube_url": "https://youtu.be/1o0nzZYfjn8",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Opening Presentation",
                        "contributors": [
                            "Thomas Theussl"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T18:00:00Z",
                        "time_end": "2020-10-28T18:02:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Red Sea Eddies and their Ensemble Representation",
                        "contributors": [
                            "Ibrahim Hoteit"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T18:02:00Z",
                        "time_end": "2020-10-28T18:15:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Lagrangian Q-criterion and Transport of Salt and Temperature",
                        "contributors": [
                            "Steve Wolligandt",
                            "Janos Zimmermann"
                        ],
                        "abstract": "We analyze the IEEE SciVis contest 2020 data set. To detect eddies in the Red Sea, we use a Lagrangian Q criterion. Based on this, we confirm two eddies in the Red Sea that are consistent on the first three ensemble members, and only one eddy consistent over the first six ensemble members. To analyze the transport of salt and temperature, we propose a two-step visualization. First, we color code temperature/salinity as 2D time-dependent scalar fields. Second, we integrate particles with color coding the change of temperature/salinity along their paths. Based on this we can distinguish regions dominated by transport from regions where other effects are relevant. ",
                        "time_start": "2020-10-28T18:15:00Z",
                        "time_end": "2020-10-28T18:25:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Interactive Visual Analysis of Oceanographic Simulation Ensemble Data",
                        "contributors": [
                            "Johannes Fincke"
                        ],
                        "abstract": "The analysis of multi-run oceanographic simulation data raises vari- ous issues ranging from visualizing multi-field spatio-temporal data to properly identifying and depicting vortices. In this paper, we present an interactive tool that enables us to overcome these chal- lenges by employing multiple coordinated views",
                        "time_start": "2020-10-28T18:25:00Z",
                        "time_end": "2020-10-28T18:35:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Red Sea Eddies Visualization for the Web",
                        "contributors": [
                            "Hiep Vo"
                        ],
                        "abstract": "This work presents our approach to data processing, detection, and visualization techniques to visualize eddies. We also provide a web application to visualize the detected eddies, and other aspects can relate to eddies understanding, such as sea surface level, salt, temperature, and their correlations.",
                        "time_start": "2020-10-28T18:35:00Z",
                        "time_end": "2020-10-28T18:45:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Eddy-based visual analysis system for transport in the Red Sea",
                        "contributors": [
                            "Xiaoyang Han"
                        ],
                        "abstract": "The eddy is an important research target in the ocean, and it has a sig- nificant influence on ocean temperature, salt, and particle transport. The Red Sea is an extraordinary ocean that has high salt. Under- standing the temperature and salt transport of the Red Sea and the formation and evolution of eddies is of great significance to ocean analysis and research of the Red Sea. We designed an interactive visual analysis system to solve these problems and analyzed the influence of eddy on temperature and salt transport for the Red Sea.",
                        "time_start": "2020-10-28T18:45:00Z",
                        "time_end": "2020-10-28T18:55:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "The Use of 3D Optical Flow, Feature-Tracking and Token-Tracking Petri Nets to Analyze and Visualize Multiple Scales of Ocean Eddies",
                        "contributors": [
                            "Karen Bemis"
                        ],
                        "abstract": "Understanding and predicting ocean dynamics correctly remains a challenge even with today's advancements in various computational fields. One persistent limitation concerns the availability of tech- niques and tools for viewing and analyzing ocean simulation data. Visualizing the coherent structures, such as mesoscale eddies, is non-trivial; comparing the spatial and temporal distributions of such features across locations, time, and simulation (or observational) datasets is even more challenging. This study addresses the chal- lenge of visualizing ocean dynamics for the Red Sea - Persian Gulf region using the multiple ensembles, that is multiple independent simulations, of the Sci-Vis 2020 contest dataset. In this collabora- tive work, we introduce tools and algorithms to extract mesoscale ocean eddies and analyse their behaviour. We demonstrate the use of various computer vision techniques, including optical flow, gradient direction analysis, feature tracking and activity detection to extract and visualize eddies as well as transport patterns from the raw sim- ulation data, to analyze changes over time, and to characterize the differences between ensembles. ",
                        "time_start": "2020-10-28T18:55:00Z",
                        "time_end": "2020-10-28T19:05:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Tracing Eddy Transport in the Red Sea and Gulf of Aden",
                        "contributors": [
                            "Anke Friederici",
                            "Wito Engelke"
                        ],
                        "abstract": "Mesoscale eddies play an important role in understanding oceanic transport, e.g., of heat, pollution and biomass, which is of special interest in the Red Sea. Thereby, the analysis of transport structures relies on large-scale ensemble simulations of ocean flows, leaving the domain scientist with a vast amount of data to be explored. Such an ensemble data set, together with four analysis tasks, is the basis for the SciVis Contest 2020. In this submission, we present our solutions to approach these tasks. When designing our methods, the main goal was to rely on a simple concept and parameters that can easily be communicated to the domain scientist. With this in mind, we decided to implement direct methods as well as an geometric approach searching for 'almost-closed streamlines' using a winding angle criterion. The resulting streamlines are then clustered over space and time, resulting in stacks of three-dimensional volumes representing the eddies. Due to the explicit geometric extraction of eddies, quantitative measures can be computed. These include a multitude of statistical measures, e.g. size, average temperature, and salinity. Using linked, coordinated views with statistical repre- sentations and three-dimensional visualizations our system supports spatio-temporal exploration and filtering of the data. Our method has been implemented in the open-source framework Inviwo [1]. Additionally, a video demonstrating the capabilities of the method is submitted as supplementary material.",
                        "time_start": "2020-10-28T19:05:00Z",
                        "time_end": "2020-10-28T19:15:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Statistical Rendering for Visualization of Red Sea Eddy Simulation Data",
                        "contributors": [
                            "Tushar M. Athawale"
                        ],
                        "abstract": "Analyzing the effects of ocean eddies is important in oceanology for gaining insights into the transport of energy and biogeochemical particles. We present an application of statistical visualization tech- niques for the analysis of the Red Sea eddy simulation ensemble. Specifically, we demonstrate the applications of statistical volume rendering and statistical summary maps to velocity magnitude fields derived from the ensemble for the study of eddy positions. In statis- tical volume rendering, we model per-voxel data uncertainty using noise densities and study the propagation of uncertainty into the volume rendering pipeline. In the statistical summary maps, we char- acterize the uncertainty of gradient flow destinations to understand the structural variations of Morse complexes across the ensemble. We demonstrate the utility of our statistical visualization techniques for the analysis of eddy positions and their spatial uncertainty. ",
                        "time_start": "2020-10-28T19:15:00Z",
                        "time_end": "2020-10-28T19:25:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "SciVis Contest 2021 Announcement",
                        "contributors": [
                            "Alexei Razoumov"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T19:25:00Z",
                        "time_end": "2020-10-28T19:28:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Closing Presentation",
                        "contributors": [
                            "Thomas Theussl"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T19:28:00Z",
                        "time_end": "2020-10-28T19:30:00Z"
                    }
                ]
            }
        ]
    },
    "x-posters": {
        "event": "VIS",
        "long_name": "Poster Session",
        "event_type": "VIS",
        "event_description": "",
        "event_url": "https://vis2020-ieee.ipostersessions.com/Default.aspx?s=vis2020_gallery",
        "sessions": [
            {
                "title": "Poster Session",
                "session_id": "x-posters",
                "chair": [
                    "Posters Chairs"
                ],
                "organizers": [
                    "Posters Chairs"
                ],
                "display_start": "2020-10-28T19:30:00Z",
                "time_start": "2020-10-28T19:30:00Z",
                "time_end": "2020-10-28T20:30:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Discord Only",
                        "title": "Poster Session",
                        "contributors": [
                            "Posters Chairs"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T19:30:00Z",
                        "time_end": "2020-10-28T20:30:00Z"
                    }
                ]
            }
        ]
    },
    "m-publish": {
        "event": "Publish or Publicize",
        "long_name": "Publish or Publicize",
        "event_type": "Meetup",
        "event_description": "Publishing your paper is all good and well, but how does the rest of the world learn about it? What ways do you have to get even academic researchers to learn about your research if they happened to miss your conference talk? There\u2019s a healthy (if small) visualization community on Twitter, but blogging hasn\u2019t been so hot lately, and we haven\u2019t really established much of a foothold on YouTube. Let\u2019s chat about what tools we have to publicize visualization research, from old-school blogging to social media, podcasts to vlogs, and beyond (anybody want to start a vis TikTok?).",
        "event_url": "",
        "sessions": [
            {
                "title": "Publish or Publicize",
                "session_id": "m-publish",
                "chair": [
                    "Robert Kosara"
                ],
                "organizers": [
                    "Robert Kosara"
                ],
                "display_start": "2020-10-28T19:40:00Z",
                "time_start": "2020-10-28T19:40:00Z",
                "time_end": "2020-10-28T20:40:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "https://us02web.zoom.us/j/84525407680?pwd=RzhIUUxuRUhxYnY1UmlvWVBCY3Yvdz09",
                "zoom_password": "6K3JLpKK",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Zoom Only",
                        "title": "Publish or Publicize",
                        "contributors": [
                            "Robert Kosara"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-28T19:40:00Z",
                        "time_end": "2020-10-28T20:40:00Z"
                    }
                ]
            }
        ]
    },
    "p-vishealth": {
        "event": "The Role of Vis and Data Researchers During a Public Health Crisis",
        "long_name": "The Role of Vis and Data Researchers During a Public Health Crisis",
        "event_type": "Panel",
        "event_description": "We in the field like to think of visualization and data analysis as an unconditional good: a tool that enables clarity, communicates truth, and facilitates understanding. It stands to reason, then, that in a public health crisis such as the one we are going through, visualization and data researchers should be a part of the solution. But can we, and should we? What are the risks? Does the self-image of the field hold up with the current existing sociopolitical context?\n\nIn this panel, we will bring top researchers in visualization and data management to discuss how\u2014and whether\u2014visualization and data researchers can build effective solutions for urgent public health problems.",
        "event_url": "",
        "sessions": [
            {
                "title": "The Role of Vis and Data Researchers During a Public Health Crisis",
                "session_id": "p-vishealth",
                "chair": [
                    "Carlos Scheidegger",
                    "Arvind Satyanarayan"
                ],
                "organizers": [
                    "Arvind Satyanarayan",
                    "Carlos Scheidegger",
                    "Josh Levine",
                    "Dan Archambault",
                    "Micha Behrisch"
                ],
                "display_start": "2020-10-29T18:00:00Z",
                "time_start": "2020-10-29T18:00:00Z",
                "time_end": "2020-10-29T19:30:00Z",
                "discord_category": "the role of vis and data researchers during a public health crisis",
                "discord_channel": "general",
                "discord_channel_id": "768679119709405214",
                "youtube_url": "https://youtu.be/Uva2Dg7imps",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Panel",
                        "title": "The Role of Vis and Data Researchers During a Public Health Crisis",
                        "contributors": [
                            "Michael Correll",
                            "Jessica Hullman",
                            "Nils Gehlenborg",
                            "Ana Crisan"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T18:00:00Z",
                        "time_end": "2020-10-29T19:30:00Z"
                    }
                ]
            }
        ]
    },
    "l-med": {
        "event": "Recent Challenges in Medical Visualization",
        "long_name": "Recent Challenges in Medical Visualization",
        "event_type": "Application Spotlight",
        "event_description": "Medical is a popular application in visualization. In this area, there are decades of development. In addition, the medical application highly contributes to the visualization research. Medicine itself is a developing area, where novel imaging, diagnosis, and treatment techniques are constantly developed and improved. Due to the increasing data complexity in this field, novel challenges arise when considering medical visualization. Still, by now fewer and fewer medical visualization contributions appear in proceedings of VIS related conferences. In this application spotlight, we aim to summarize and discuss recent challenges and proposed solutions in the area of medical visualization. These challenges include uncertainty visualization, multi-modal visualization, teaching of medical students, challenges in publishing medical visualization papers, surgery assistance, and legal regulations that need to be considered when proposing novel visualization techniques to the medical market.\n\nWe plan to structure our application spotlight as follows. First, we intend to invite selected speakers, that are working in the field of medical visualization to obtain their opinion on recent challenges in this field. Subsequently, we aim to feature a lively discussion with the audience of our spotlight to identify additional problems. Furthermore, we aim to highlight recent problems in publishing papers in the application area of medical. This discussion will be the main part of the application spotlight and will be carefully tracked. Based on the proposed event, we aim to provide a list of current challenges in medical visualization that will be publicly available. We hope that this list of challenges provides a deeper understanding to the VIS community as to why developments in medical visualizations are in high demand and how our community can benefit from and contribute to them.",
        "event_url": "https://www.computer.org/digital-library/magazines/cg/call-for-papers-special-issue-on-the-role-of-visualization-in-the-manufacturing-industry",
        "sessions": [
            {
                "title": "Recent Challenges in Medical Visualization",
                "session_id": "l-med",
                "chair": [
                    "Christina Gillmann",
                    "Thomas Wischgoll",
                    "Noeska Smit"
                ],
                "organizers": [
                    "Christina Gillmann",
                    "Noeska Smit",
                    "Thomas Wischgoll"
                ],
                "display_start": "2020-10-29T18:00:00Z",
                "time_start": "2020-10-29T18:00:00Z",
                "time_end": "2020-10-29T19:30:00Z",
                "discord_category": "recent challenges in medical visualization",
                "discord_channel": "general",
                "discord_channel_id": "768679125857992705",
                "youtube_url": "https://youtu.be/UCxsx6nGvMs",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Opening Presentation: Introduction to the Topic",
                        "contributors": [
                            "Christina Gillmann",
                            "Noeska Smit",
                            "Thomas Wischgoll"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T18:00:00Z",
                        "time_end": "2020-10-29T18:10:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Visualization for AI in Health",
                        "contributors": [
                            "Anna Vilanova"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T18:10:00Z",
                        "time_end": "2020-10-29T18:20:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Invited Talk: Recent Developments in VR",
                        "contributors": [
                            "Bernhard Preim"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T18:20:00Z",
                        "time_end": "2020-10-29T18:30:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Medicinae Notitia Visibilis Fac - Quo Vadis?",
                        "contributors": [
                            "Eduard Gr\u00f6ller"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T18:30:00Z",
                        "time_end": "2020-10-29T18:40:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Open Discussion",
                        "contributors": [
                            "All speakers"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T18:40:00Z",
                        "time_end": "2020-10-29T19:30:00Z"
                    }
                ]
            }
        ]
    },
    "m-comm": {
        "event": "VisComm Meetup",
        "long_name": "VisComm Meetup",
        "event_type": "Meetup",
        "event_description": "This informal gathering is an opportunity to network with others interested in using visualization for communication. Come to find a collaborator, discuss best practices, share resources, or just have some social time! Be ready to introduce yourself and say a sentence or two about what you hope to get out of meetup.",
        "event_url": "",
        "sessions": [
            {
                "title": "VisComm Meetup",
                "session_id": "m-comm",
                "chair": [
                    "Adriana Arcia"
                ],
                "organizers": [
                    "Alvitta Ottley",
                    "Adriana Arcia",
                    "Ben Watson",
                    "Robert Kosara"
                ],
                "display_start": "2020-10-29T19:40:00Z",
                "time_start": "2020-10-29T19:40:00Z",
                "time_end": "2020-10-29T20:40:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "https://us02web.zoom.us/j/86037501039?pwd=MWZXQWsrSEcyMTBRL05idExXaW1VZz09",
                "zoom_password": "IdfdP8IB",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Zoom Only",
                        "title": "VisComm Meetup",
                        "contributors": [
                            "Adriana Arcia"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T19:40:00Z",
                        "time_end": "2020-10-29T20:40:00Z"
                    }
                ]
            }
        ]
    },
    "m-jobfair": {
        "event": "VIS JobFair Meetup",
        "long_name": "VIS JobFair Meetup",
        "event_type": "Meetup",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "VIS JobFair Meetup",
                "session_id": "m-jobfair",
                "chair": [
                    "Jaegul Choo",
                    "Hsiang-Yun Wu",
                    "Karen Schloss"
                ],
                "organizers": [
                    "Hsiang-Yun Wu",
                    "Karen Schloss",
                    "Jaegul Choo"
                ],
                "display_start": "2020-10-29T19:40:00Z",
                "time_start": "2020-10-29T19:40:00Z",
                "time_end": "2020-10-29T20:40:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": null,
                "zoom_password": null,
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Discord Only",
                        "title": "VIS JobFair Meetup",
                        "contributors": [
                            "Hsiang-Yun Wu",
                            "Karen Schloss",
                            "Jaegul Choo"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T19:40:00Z",
                        "time_end": "2020-10-29T20:40:00Z"
                    }
                ]
            }
        ]
    },
    "x-revise": {
        "event": "Restructuring VIS for the Future",
        "long_name": "Restructuring VIS for the Future",
        "event_type": "VIS",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "reVISe",
                "session_id": "x-revise",
                "chair": [
                    "Anders Ynnerman",
                    "Alexander Lex",
                    "Petra Isenberg",
                    "Christoph Garth",
                    "Min Chen",
                    "Shixia Liu",
                    "Alex Endert"
                ],
                "organizers": [
                    "Christoph Garth"
                ],
                "display_start": "2020-10-29T20:00:00Z",
                "time_start": "2020-10-29T20:00:00Z",
                "time_end": "2020-10-29T21:00:00Z",
                "discord_category": "restructuring vis for the future",
                "discord_channel": "revise",
                "discord_channel_id": "768679133689413633",
                "youtube_url": "https://youtu.be/09eRbWAnyyY",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Introduction: Restructuring IEEE VIS for the Future",
                        "contributors": [
                            "Christoph Garth",
                            "Alex Endert",
                            "Alexander Lex",
                            "Anders Ynnerman",
                            "Petra Isenberg",
                            "Min Chen",
                            "Shixia Liu"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T20:00:00Z",
                        "time_end": "2020-10-29T20:20:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Discussion",
                        "contributors": [
                            "Christoph Garth",
                            "Alex Endert",
                            "Alexander Lex",
                            "Anders Ynnerman",
                            "Petra Isenberg",
                            "Min Chen",
                            "Shixia Liu"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T20:10:00Z",
                        "time_end": "2020-10-29T21:00:00Z"
                    }
                ]
            }
        ]
    },
    "x-viskickoff": {
        "event": "VIS 2021 Kick-off Meeting",
        "long_name": "VIS 2021 Kick-off Meeting",
        "event_type": "VIS",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "VIS 2021 Kick-off Meeting",
                "session_id": "x-viskickoff",
                "chair": [
                    "Brian Summa"
                ],
                "organizers": [
                    "Brian Summa"
                ],
                "display_start": "2020-10-29T21:15:00Z",
                "time_start": "2020-10-29T21:15:00Z",
                "time_end": "2020-10-29T22:15:00Z",
                "discord_category": "vis 2021 kick-off meeting",
                "discord_channel": "general",
                "discord_channel_id": "768679140865736736",
                "youtube_url": "https://youtu.be/V-jLISubQAM",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "VIS 2021 Kick-off Meeting Intro",
                        "contributors": [
                            "Brian Summa"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T21:15:00Z",
                        "time_end": "2020-10-29T21:20:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "VIS 2021 Kick-off Meeting",
                        "contributors": [
                            "Brian Summa"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-29T21:15:00Z",
                        "time_end": "2020-10-29T22:15:00Z"
                    }
                ]
            }
        ]
    },
    "m-velo": {
        "event": "Le tour de VIS 2020 - MeetUp in Watopia",
        "long_name": "Le tour de VIS 2020 - MeetUp in Watopia",
        "event_type": "Meetup",
        "event_description": "",
        "event_url": "https://www.gicentre.net/velo-club-de-vis",
        "sessions": [
            {
                "title": "Le tour de VIS 2020 - MeetUp in Watopia",
                "session_id": "m-velo",
                "chair": [
                    "Jason Dykes"
                ],
                "organizers": [
                    "Jason Dykes"
                ],
                "display_start": "2020-10-29T21:00:00Z",
                "time_start": "2020-10-29T21:00:00Z",
                "time_end": "2020-10-29T22:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "https://us02web.zoom.us/j/85748247904?pwd=akJzRjJYdXJSTC9rcmpvbENrczBxZz09",
                "zoom_password": 490255,
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Zoom Only",
                        "title": "Le tour de VIS 2020 - MeetUp in Watopia",
                        "contributors": [
                            "Jason Dykes"
                        ],
                        "abstract": "After an exciting test ride, Dan and Jason would like to invite you to join them in the wilds of Watopia for a Zwift group ride during IEEE VIS.\n\nYou'll need a trainer, Zwift, and a Zoom connection but we've set it up so that we all stay together during the ride whatever individual workout we decide to try. We recommend this light training ride - ready for longer ride at the weekend, but go with whatever you like - Zwift will keep us together!",
                        "time_start": "2020-10-29T21:00:00Z",
                        "time_end": "2020-10-29T22:00:00Z"
                    }
                ]
            },
            {
                "title": "Le tour de VIS 2020 - MeetUp in Watopia part 2",
                "session_id": "m-velo-2",
                "chair": [
                    "Jason Dykes"
                ],
                "organizers": [
                    "Jason Dykes"
                ],
                "display_start": "2020-10-30T20:00:00Z",
                "time_start": "2020-10-30T20:00:00Z",
                "time_end": "2020-10-30T21:00:00Z",
                "discord_category": "",
                "discord_channel": "",
                "discord_channel_id": 0,
                "youtube_url": null,
                "zoom_meeting": "https://us02web.zoom.us/j/81124985478?pwd=R3FkU2lqOG4vYUY5UlNXajh5QXYxdz09",
                "zoom_password": 652101,
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Zoom Only",
                        "title": "Le tour de VIS 2020 - MeetUp in Watopia part 2",
                        "contributors": [
                            "Jason Dykes"
                        ],
                        "abstract": "After an exciting test ride, Dan and Jason would like to invite you to join them in the wilds of Watopia for a Zwift group ride during IEEE VIS.\n\nYou'll need a trainer, Zwift, and a Zoom connection but we've set it up so that we all stay together during the ride whatever individual workout we decide to try. We recommend this light training ride - ready for longer ride at the weekend, but go with whatever you like - Zwift will keep us together!",
                        "time_start": "2020-10-30T20:00:00Z",
                        "time_end": "2020-10-30T21:00:00Z"
                    }
                ]
            }
        ]
    },
    "l-industrial": {
        "event": "The Role of Visualization in Industrial Production",
        "long_name": "The Role of Visualization in Industrial Production",
        "event_type": "Application Spotlight",
        "event_description": "Industrial production is a manifold and complex process holding several substeps from product concept over prototyping to manufacturing and quality management. Here, visualization approaches can be applied in nearly each step in order to increase the quality of decision making processes. In this context, each step of the process contributes unique requirements for visualizations to be applicable. This holds several problems: first, there is no standardization for visualizations in industrial production. Second, companies and visualization researchers can follow different aims which can make it hard to build a functional collaboration and third, there is no mechanism that keeps track of already invested or unsolved problems in visualization in industrial production. In this application spotlight we aim to shed a light on the application of industrial production when concerning visualization tasks. We aim to bring together different actors in visualization of industrial production, such as visualization researchers, research institutes and company fellows. We aim to structure our application spotlight along the industrial production pipeline and highlight the challenges when considering visualization approaches. Furthermore, the spotlight aims to discuss the open problems and solved tasked on the boundary between visualization research and industrial production. Here, the goal is to generate an overview over the role of visualization in industrial production, generate access for visualization researchers to this unique application and build a first step in creating a taxonomy of visualizations in industrial production.",
        "event_url": "",
        "sessions": [
            {
                "title": "The Role of Visualization in Industrial Production",
                "session_id": "l-industrial",
                "chair": [
                    "Gerik Scheuermann",
                    "Christina Gillmann",
                    "Petra Gospodnetic"
                ],
                "organizers": [],
                "display_start": "2020-10-30T14:00:00Z",
                "time_start": "2020-10-30T14:00:00Z",
                "time_end": "2020-10-30T15:30:00Z",
                "discord_category": "the role of visualization in industrial production",
                "discord_channel": "general",
                "discord_channel_id": "767753497243549768",
                "youtube_url": "https://youtu.be/ZN3Vjr_G8h8",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "Introduction to the Topic",
                        "contributors": [
                            "Christina Gillmann",
                            "Petra Gospodnetic",
                            "Gerik Scheuermann"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-30T14:00:00Z",
                        "time_end": "2020-10-30T14:12:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Make it Beautiful",
                        "contributors": [
                            "Sebastian Grottel"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-30T14:12:00Z",
                        "time_end": "2020-10-30T14:24:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Tensor-driven Visualization",
                        "contributors": [
                            "Vanessa Kretzschmar"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-30T14:24:00Z",
                        "time_end": "2020-10-30T14:36:00Z"
                    },
                    {
                        "type": "Live Presentation",
                        "title": "Emphasizing the importance of visual analytics in industry",
                        "contributors": [
                            "Johanna Schmidt"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-30T14:34:00Z",
                        "time_end": "2020-10-30T14:48:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Open Discussion",
                        "contributors": [
                            "Christina Gillmann",
                            "Petra Gospodnetic",
                            "Gerik Scheuermann"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-30T14:48:00Z",
                        "time_end": "2020-10-30T15:30:00Z"
                    }
                ]
            }
        ]
    },
    "p-socialgood": {
        "event": "Visualization for Social Good",
        "long_name": "Visualization for Social Good",
        "event_type": "Panel",
        "event_description": "After reflection on the Visualization for Social Good tutorial at IEEE VIS 2019, we noticed a perception that work using the power of visualization to further the public good was under-studied or otherwise not part of the main VIS conference. At the same time, there is a growing \u201ctechlash\u201d that highlights the potential of data science (and data visualization) as tools for reinforcing structures of inequality and injustice. We envision this panel to function as a way of airing these competing viewpoints and providing guidance on how the visualization community should use their power and responsibilities in the world. As such, we have centered our panelists\u2019 statements around response to a single question: What role (if any) should data visualization play in effecting positive change in the world?",
        "event_url": "https://vis4good.github.io/",
        "sessions": [
            {
                "title": "Visualization for Social Good",
                "session_id": "p-socialgood",
                "chair": [
                    "Lane Harrison ",
                    "Michelle Borkin",
                    "Leilani Battle",
                    "Michael Correll "
                ],
                "organizers": [
                    "Leilani Battle",
                    "Michelle Borkin",
                    "Michael Correll",
                    "Lane Harrison",
                    "Josh Levine",
                    "Dan Archambault",
                    "Micha Behrisch",
                    "Evan Peck"
                ],
                "display_start": "2020-10-30T16:00:00Z",
                "time_start": "2020-10-30T16:00:00Z",
                "time_end": "2020-10-30T17:30:00Z",
                "discord_category": "visualization for social good",
                "discord_channel": "general",
                "discord_channel_id": "767141460667138069",
                "youtube_url": "https://youtu.be/RkNFBxDH6L0",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "https://youtu.be/IZOGwFg6ch4",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": null,
                        "title": "Visualization for Social Good",
                        "contributors": [
                            "Catherine D'Ignazio",
                            "Ronald Metoyer",
                            "Michelle Borkin",
                            "Evan Peck"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-30T16:00:00Z",
                        "time_end": "2020-10-30T17:30:00Z"
                    }
                ]
            }
        ]
    },
    "l-bio": {
        "event": "Challenges in the Visualization of Bioelectric Fields for Cardiac and Neural Research",
        "long_name": "Challenges in the Visualization of Bioelectric Fields for Cardiac and Neural Research",
        "event_type": "Application Spotlight",
        "event_description": "The visualization of bioelectric fields has been an ongoing challenge to cardiac and neural researchers. Bioelectrical field data are heterogeneous and their interpretation often requires simultaneous interrogation of anatomical information with measured or simulated scalar, vector, and tensor fields. They may be volumetric or constrained to (obscuring) surfaces; and are usually time-dependent. To fully understand the spatiotemporal dynamics of the pathology under study, it is often necessary to integrate diverse formation across multiple scales, and to quantify and visualize the uncertainty associated with the data. Unfortunately, few visualization approaches are catered to the needs, conventions, and language of the applications, leading to a lack of widespread standards and use. Addressing these visualization challenges has potentially far-reaching benefits to both the understanding and diagnosis of a number of diseases.\n\nThe 90-minute session will be split between a single 30-minute plenary talk given by Dr. Rob S. MacLeod with experience in cardiac and neural analysis and visualization to outline the challenges facing the field. The plenary talk will be followed by three 15-minute talks from specialists from across the country who will outline their specific applications and visualization challenges.",
        "event_url": "http://www.sci.utah.edu/~beiwang/visspotlight2020/index.html",
        "sessions": [
            {
                "title": "Challenges in the Visualization of Bioelectric Fields for Cardiac and Neural Research",
                "session_id": "l-bio",
                "chair": [
                    "Wilson Good"
                ],
                "organizers": [
                    "Rob MacLeod",
                    "Bei Wang",
                    "Wilson Good"
                ],
                "display_start": "2020-10-30T16:00:00Z",
                "time_start": "2020-10-30T16:00:00Z",
                "time_end": "2020-10-30T17:30:00Z",
                "discord_category": "challenges in the visualization of bioelectric fields for cardiac and neural research",
                "discord_channel": "general",
                "discord_channel_id": "767141459765100544",
                "youtube_url": "https://youtu.be/9JdFmgg2t44",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Plenary talk",
                        "contributors": [
                            "Rob MacLeod"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-30T16:00:00Z",
                        "time_end": "2020-10-30T16:30:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Guiding Deep Brain Stimulation Research with Bioelectric Field Visualizations",
                        "contributors": [
                            "Andrew Janson"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-30T16:30:00Z",
                        "time_end": "2020-10-30T16:45:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "Visualization Challenges in Simulations of Noninvasic Brain Stimulation",
                        "contributors": [
                            "Sumientra Rampersad"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-30T16:45:00Z",
                        "time_end": "2020-10-30T17:00:00Z"
                    },
                    {
                        "type": "Live Panel",
                        "title": "Q&A Session",
                        "contributors": [
                            "Rob MacLeod",
                            "Andrew Janson",
                            "Sumientra Rampersad"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-30T17:00:00Z",
                        "time_end": "2020-10-30T17:30:00Z"
                    }
                ]
            }
        ]
    },
    "vis-capstone": {
        "event": "Capstone",
        "long_name": "VIS Capstone",
        "event_type": "Capstone",
        "event_description": "In this talk I touch on a few moments in my research history that have changed my way of thinking and gradually leading me to become passionate about \u2013 data visualization for empowerment and inclusion. I have chosen moments that are memorable to me because they have changed the way I think. I will step through my process of discovering the importance of thinking about how to make visualizations more empowering, more engaging, and more expressive, and how may lead to developing visualizations that work better for us as externalizations. I will relate these points to discussions and data feudalism and the importance of us staying connected to our data heritage.",
        "event_url": "",
        "sessions": [
            {
                "title": "Capstone by Sheelagh Carpendale",
                "session_id": "vis-capstone",
                "chair": [
                    "Valerio Pascucci",
                    "Mike Kirby"
                ],
                "organizers": [
                    "Valerio Pascucci",
                    "Mike Kirby"
                ],
                "display_start": "2020-10-30T18:00:00Z",
                "time_start": "2020-10-30T18:00:00Z",
                "time_end": "2020-10-30T19:00:00Z",
                "discord_category": "VIS",
                "discord_channel": "closing",
                "discord_channel_id": "767141539071131689",
                "youtube_url": "https://youtu.be/XQhBHnPIsRk",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Recorded Talk",
                        "title": "Data VIS for Empowerment and Inclusion",
                        "contributors": [
                            "Sheelagh Carpendale"
                        ],
                        "abstract": "In this talk I touch on a few moments in my research history that have changed my way of thinking and gradually leading me to become passionate about \u2013 data visualization for empowerment and inclusion. I have chosen moments that are memorable to me because they have changed the way I think. I will step through my process of discovering the importance of thinking about how to make visualizations more empowering, more engaging, and more expressive, and how may lead to developing visualizations that work better for us as externalizations. I will relate these points to discussions and data feudalism and the importance of us staying connected to our data heritage.",
                        "time_start": "2020-10-30T18:00:00Z",
                        "time_end": "2020-10-30T19:00:00Z"
                    }
                ]
            }
        ]
    },
    "vis-closing": {
        "event": "VIS",
        "long_name": "VIS Closing",
        "event_type": "VIS",
        "event_description": "",
        "event_url": "",
        "sessions": [
            {
                "title": "Closing",
                "session_id": "vis-closing-1",
                "chair": [
                    "Valerio Pascucci",
                    "Mike Kirby"
                ],
                "organizers": [
                    "Valerio Pascucci",
                    "Mike Kirby"
                ],
                "display_start": "2020-10-30T19:00:00Z",
                "time_start": "2020-10-30T19:00:00Z",
                "time_end": "2020-10-30T19:30:00Z",
                "discord_category": "vis",
                "discord_channel": "closing",
                "discord_channel_id": "767141539071131689",
                "youtube_url": "https://youtu.be/XQhBHnPIsRk",
                "zoom_meeting": "",
                "zoom_password": "",
                "ff_link": "",
                "ff_playlist": "",
                "time_slots": [
                    {
                        "type": "Live Presentation",
                        "title": "VIS 2020 Conference Update",
                        "contributors": [
                            "Valerio Pascucci",
                            "Mike Kirby"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-30T19:00:00Z",
                        "time_end": "2020-10-30T19:15:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "VEC Update",
                        "contributors": [
                            "Lisa Avila"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-30T19:15:00Z",
                        "time_end": "2020-10-30T19:20:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "EuroVis 2021 Welcome",
                        "contributors": [
                            "Renato Pajarola",
                            "Tobias G\u00fcnther"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-30T19:20:00Z",
                        "time_end": "2020-10-30T19:23:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "PacificVis 2021 Welcome",
                        "contributors": [
                            "Jiawan Zhang"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-30T19:22:00Z",
                        "time_end": "2020-10-30T19:25:00Z"
                    },
                    {
                        "type": "Recorded Talk",
                        "title": "VIS 2021 Announcement and Welcome",
                        "contributors": [
                            "Brian Summa"
                        ],
                        "abstract": null,
                        "time_start": "2020-10-30T19:25:00Z",
                        "time_end": "2020-10-30T19:30:00Z"
                    }
                ]
            }
        ]
    }
}
